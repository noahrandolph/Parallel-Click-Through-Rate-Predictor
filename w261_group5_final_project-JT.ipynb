{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "W261 Machine Learning at Scale<br>\n",
    "12 December 2018\n",
    "\n",
    "Wei Wang;\n",
    "Alice Lam;\n",
    "John Tabbone;\n",
    "Noah Randolph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUBRIC GUIDE ####\n",
    "Formulate the question \n",
    "\n",
    "limitation of the data and algorithm\n",
    "\n",
    "dataset contents and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click-Through-Rate (CTR), which defines as \"the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement\"$^1$, is a key metric that measure online advertisment performance. It demonstrates both 1) how effective the advertising platforms are, and 2) how effective the advertising campaigns are in targeting the right audience. Since the advertising income is highly correlated with CTR, platforms are motivated to improve their CTR to maximize their revenues. The goal of our analysis is to predict CTR, which can be useful to priortize which ad to show whom in order to maximize advertising revenue.\n",
    "\n",
    "Online platforms ranging from Google, Facebook, to casual game apps are essentially \"online real estate\" that draws \"traffic\", i.e. eyeballs on the screen. They can monetize these traffic by charging businesses advertising fee for putting up ads/links on the screen. Traditionally, the fee is charged per impression, hence termed \"CPI - Cost-per-impression\". Advertisers would have a campaign budget and a desired return on investment from the budget, e.g. bringing 1 million people to their website with a $\\$1,000,000$ budget. If the platform's CTR is 10%, the advertisers can only charge up to $\\$0.10$ per impression. If the platform's CTR is 100%, then the maximum CPI could reach $\\$1$. Online advertising model has gradually evolved to pay-for-performance, i.e. advertisers would only pay if the link is being clicked. Regardless of the advertising revneue model, platforms are highly incentivized to improve CTR.\n",
    "\n",
    "Based on the current work in the literature on modeling clicks and CTR, the first challenge is to understand user behaviors. Given the limited opportunity ads can be shown to a specific person at any given time, the platform should present the ads that a person is most likely to click. Understanding browsing and clicking behavior of each individual is thus essential in making CTR prediction for each user. Some of the features that are likely significant for such predictions are: time, day of week, location, gender, age, device they are using, sites they are visiting, sites they came from, topics of the ad, color of the ad, pixel location of the ad on the screen, etc. The data we analyze on was made available by Criteo and contains a portion of Criteo's traffic dataset for 7 days. It has both integer and categorical column of features. However, it is completely anonymiezd which limits us from conducting feature selection or engineering that is backed by contextual understanding.\n",
    "\n",
    "!JT -> Is this correct?  We're not trying to do this in realtime\n",
    "!JT -> We had a lesson in SparkStreaming\n",
    "\n",
    "The second challenge is to optimize algorithm speed, which means the prediction can be done in seconds. For example, given the fact that the person is in this location and launched this app at this time of the day, the algorithm should be able to predict the CTR in split second in order to decide which ad to push to the person. Any accurate prediction delivered too late is almost effectively useless. The algorithm speed is hindered by the great amount of traffic volume that comes into the site, as well as the massive amount of data that has numerous categorical variables with high cardinality. We thus leverage Spark to increase the scalability of our analysis. When selecting models, our priority would be speed over performance.\n",
    "\n",
    "Another approach to mitigate the speed challenge is to __not__ include information generated from the users from last few seconds/minutes/hours. This approach may be at the cost of accuracy as well because immediate information such as current location, last article the person look at, etc, can enhance accuracy significantly. This is a compromise the platforms need to evalaute given their specific business needs and infrastructure. We have no information on whether some of the features in the dataset is immediate features that's received a few seconds prior to the display of the ad. We choose to assume the features may contain such information.\n",
    "\n",
    "Throughout this project, we use logistic regression model because as the most prevalent algorithm for solving industry scale problems, it is designed to handle categorical dependent variable (Click vs. Non-Click). Logistics regression has its own drawbacks. As a generalized linear model, it requires transformation for non-linear features. This additional step can slow the process down when the feature space and data volume is too large. However, since the probability score outputs that logistics regression generates are straightforward for observations, and it is not particularly affected by mild cases of multi-collinearity, we decide to combine the power of logistics regression and Spark for our analysis.\n",
    "\n",
    "\n",
    "$^1$ https://en.wikipedia.org/wiki/Click-through_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Algorithm Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUBRIC GUIDE ####\n",
    "\n",
    "math explain clear, not overly techinical\n",
    "\n",
    "toy example is appropriate\n",
    "\n",
    "toy calculation is clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Algorithm Overview\n",
    "\n",
    "##### Motivation\n",
    "\n",
    "Logistic regression starts with a linear classifier $f(x) = w^Tx + b$ and applies a sigmoid activation function $\\sigma$ such that:\n",
    "\n",
    "$$\\sigma(f(x_{i})) =\\begin{cases}\n",
    "+1 & x_{i}\\ge .5\\\\\n",
    "-1 & x_{i}<.5\n",
    "\\end{cases} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\sigma(f(x))=\\frac{1}{1+e^{-f(x)}}\n",
    "$$\n",
    "\n",
    "This will 'binarize' the output appropriate to requirements.  \n",
    "\n",
    "##### Loss Function\n",
    "To create an accurate model that can estimate the probability of click event occuring $P(y=1|X)$ given training data X, we need to minimize cost function. \n",
    "\n",
    "$$\n",
    "P(y=1|x) = \\sigma(f(x)) = \\frac{1}{1+e^{-f(x)}}\\\\  \n",
    "P(y=-1|x) = 1 - \\sigma(f(x)) = \\frac{1}{1+e^{f(x)}}  \\\\   \n",
    "P(y_{i}|x_{i}) = \\frac{1}{1+e^{-y_{i}f(x_{i})}}    \\\\\n",
    "$$\n",
    "\n",
    "The likelihood is the combined product of all these probabilities\n",
    "\n",
    "$$\n",
    "\\prod_{i}^{n}\\frac{1}{1+e^{-y_{i}f(x_{i})}}\n",
    "$$\n",
    "\n",
    "We use the negative log liklehood as the loss function:\n",
    "\n",
    "$$\n",
    "\\sum_{i}^{n} \\log(1+e^{-y_{i}f(x_{i})})\\\\\n",
    "$$\n",
    "\n",
    "This is expressed in code in the logLoss() function\n",
    "\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "                        \n",
    "where $y_{i} = p[1]$  and $f(x) = np.dot(W, p[0])$  \n",
    "\n",
    "##### Gradient Descent\n",
    "We use gradient descent to find optimal parameters to minimize the loss function. We find the gradient of the log loss function as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla w = \\sum_{i}^{n} -y\\left(1- \\frac{1}{1+e^{-y_{i}f(x_{i})}}\\right)\\cdot x_{i}\n",
    "$$\n",
    "\n",
    "\n",
    "Again we can see this formula represented in the code of gdupdate() in the line:\n",
    "\n",
    "grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0]))\n",
    "\n",
    "where $y_{i} = p[1]$  and $f(x) = np.dot(W, p[0])$ \n",
    "\n",
    "##### Iterate\n",
    "The toy implementation then initializes the first gradient with a random guess.  It will iterate 5 times over the data, upddating the gradient and displaying the error.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Dataset Illustration\n",
    "\n",
    "To illustrate our process of implementing a logistics regression model, we create a toy dataset that contains 8 rows of randomly generated data, one categorical dependent variable and four features, which mimics the Criteo dataset. In our toy dataset, dependent values are either 1 or -1, which represents the situation of Click vs. Non-Click. Among the four features, we randomly generated integer values range from 0 to 10 for two numeric feature columns, and randomly picked either 0 or 1 for two categorical feature columns. \n",
    "\n",
    "While following the above steps, there are two additional modifications that we made in order to improve model accuracy. The first modification is normalizing feature values, which is not neccessary for the toy dataset but is essential for the entire dataset where high data variance occurs. The formula for normalization is:\n",
    "\\begin{equation}\\\n",
    "x_n= (x - \\mu)/\\sigma\n",
    "\\end{equation}\n",
    "where $x_n$ denotes the normalized $x$, $\\mu$ denotes the mean of $x$, $\\sigma$ denotes the variance of $x$.\n",
    "<br>\n",
    "\n",
    "The second modification is to add a bias term to the initial weight that we randomly generated, which eliminates the hassle of multiplying the data point by the weights and then adding the bias.\n",
    "\n",
    "One additional note is that when predicting on the entire dataset, we convert categorical data using one-hot encoding. This data manipulation process is not shown in this simplified toy dataset, but can be understood by the selection of 0 and 1 categorical terms.\n",
    "\n",
    "Our modeling design takes scalability into consideration by leveraging Spark dataframe and RDD to make the process scale to a larger dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Code Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting toyDataset.py\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Environment.\n",
    "\n",
    "# Execute code to generate and write the toy dataset\n",
    "%%writefile toyDataset.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Set the seed for random numbers\n",
    "SEED = 2615\n",
    "\n",
    "# Number of numeric columns\n",
    "NUMERIC_COLS = 2\n",
    "# Number of categorical columns (one hot encoded)\n",
    "ONE_HOT_COLS = 2\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# Creates a toy dataset in the form\n",
    "# LABEL,INT,INT,CAT,CAT\n",
    "# where CAT is categorical data that is represented as one hot encoded\n",
    "# (i.e. 0 or 1)\n",
    "#\n",
    "# w:  a list of weights \n",
    "# nrows:  The number of rows to produce\n",
    "def generateToyDataset(w=[8, -3, -1, 3, 8],nrows = 8):\n",
    "    '''generate toy logistic regression dataset with numerical and 1-hot encoded features'''\n",
    "        \n",
    "    # set random number generator\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    # These two x vectors represent integer data\n",
    "    x1 = np.random.randint(0, 10, nrows)\n",
    "    x2 = np.random.randint(0, 10, nrows)\n",
    "    \n",
    "    # These two represent categorical data that has been\n",
    "    # one hot encoded\n",
    "    x3 = np.random.randint(0, 2, nrows) \n",
    "    x4 = np.ones(nrows, np.int8) - x3 \n",
    "    \n",
    "    # Create an error term for linear function\n",
    "    noise = np.random.normal(5, 1, nrows)\n",
    "    \n",
    "    # Create linear function to determine labels\n",
    "    v = (w[0] + x1*w[1] + x2*w[2] + x3*w[3] + x4*w[4] + noise)\n",
    "    \n",
    "    # 'Activation function' v>0 to determine binary labels 1 and -1\n",
    "    y = (v>0) * 2 - 1 \n",
    "    \n",
    "    # Assemble vectors into single matrix structure\n",
    "    # NB:  This technique works to assemble the toy dataset but would \n",
    "    # be cost prohibitive to perform on a larger dataset\n",
    "    df = spark.createDataFrame(zip(y.tolist(), x1.tolist(), x2.tolist(), x3.tolist(), x4.tolist()))\n",
    " \n",
    "    # Rename columns from default\n",
    "    # c1,c2,c3 to human readable\n",
    "    # Label,I1,I2,C1,C2\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,NUMERIC_COLS)]+['C{}'.format(i) for i in range(0,ONE_HOT_COLS)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Utility function to change format of RDD\n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    # Create matrix structure of numeric and catageory features\n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERIC_COLS)] + [row['C{}'.format(i)] for i in range(0, ONE_HOT_COLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    # extract labels\n",
    "    y = row['Label']\n",
    "    \n",
    "    #return features_array (matrix) paired with label vector\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "# Given dataRDD, will return an rdd with standardized column values.\n",
    "#  This will transform each feature into a set of values whose mean\n",
    "# converges on 0 and who's standard deviation converges on 1\n",
    "def normalize(dataRDD):\n",
    "    # Take the mean of each column\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    # Take standard deviation of each column\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    # Standardize the features by calculating the difference between \n",
    "    # the actual and the mean divided by the standard deviation.  \n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStdev, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0])) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    new_model = W - learningRate * grad\n",
    "    return new_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# create a toy dataset that includes 1-hot columns for development\n",
    "df = generateToyDataset()   \n",
    "\n",
    "# Create training data set by converting dataframe to RDD.  \n",
    "# Seperates label column from feature matrix and returns at tupple\n",
    "# e.g. (features,labels)\n",
    "trainRDD = df.rdd.map(dfToRDD)\n",
    "\n",
    "# normalize RDD and cache\n",
    "normedRDDcached = normalize(trainRDD).cache()\n",
    "print(normedRDDcached.take(1))\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(normedRDDcached.take(1)[0][0])\n",
    "\n",
    "wInitial = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# 1 iteration of gradient descent with initial\n",
    "# random values\n",
    "w = GDUpdate(normedRDDcached, wInitial)\n",
    "\n",
    "# Iterate\n",
    "nSteps = 5\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    w = GDUpdate(normedRDDcached, w)\n",
    "    loss = logLoss(normedRDDcached, w)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(i,3) for i in w]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-10 08:21:13 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[(array([-1.63525964,  0.62123652,  1.        , -1.        ]), 1)]\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 7.7014879401802405\n",
      "Model: [1.101, -0.162, -1.391, -0.468, -0.797]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 6.1460241873746195\n",
      "Model: [0.865, -0.478, -1.282, -0.469, -0.795]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 5.006600355698076\n",
      "Model: [0.66, -0.743, -1.182, -0.491, -0.774]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 4.183106733189442\n",
      "Model: [0.485, -0.964, -1.092, -0.52, -0.744]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 3.591671886745936\n",
      "Model: [0.337, -1.15, -1.016, -0.55, -0.714]\n"
     ]
    }
   ],
   "source": [
    "!python toyDataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUBRIC GUIDE ####\n",
    "\n",
    "EDA well choosen and well explained\n",
    "\n",
    "Code is scalable and well commented\n",
    "\n",
    "Written discussion connects the EDA to the algorithm/potential challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of our EDA is to exaimne all features so that only variables that display linear relationship with the outcome variable and that are not highly correlated with each other are choosen. In addition to this, we will also leverage the power of large data sets before making conclusions on final feature selection. In order to achieve the above purpose, we designed three steps for EDA proecss. \n",
    "\n",
    "Step 1: Get basic statistics of each feature. \n",
    "\n",
    "Step 2: Check distributions of features.\n",
    "\n",
    "Step 3: Examine inter-correlations between numeric features and correlation between numeric features vs. dependent variable.\n",
    "\n",
    "The results illustrate that among 13 numeric features, some have larger scale and more variance than other variables. For example, variable I4 has much larger mean and max value. This finding confirms the need of using normalization across the variables. In the mean time, out of 26 categorical variables, some have a great amount of distinct values, which post a challenge for us to incorporate a huge amount of features after one hot encoding transformation. Besides, since the data quality isn't the most ideal and many values are missing, we will also need to consider missing value imputation in the feature engineering step. \n",
    "\n",
    "HIgh inter-correlation also exists among some numeric features. For instance, variable I3 is highly correlated with I12, variable I6 is highly correlated with I10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing loadAndEDA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loadAndEDA.py\n",
    "#!/usr/bin/env python\n",
    "import subprocess\n",
    "\n",
    "subprocess.call([\"pip\",\"install\",\"seaborn\"])\n",
    "\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import udf, col, countDistinct, isnan, when, count, desc\n",
    "import pandas as pd\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "MINICLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "MINILOCALPATH = 'data/train_005.txt'\n",
    "\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; MINICLOUDPATH; MINILOCALPATH\n",
    "    df = spark.read.csv(path=MINILOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,13)]+['C{}'.format(i) for i in range(0,26)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''returns approximate median values of the columns given, with null values ignored'''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "def getDescribe(df, cols):\n",
    "    return df.select(cols).describe().show()\n",
    "\n",
    "def getDistinctCount(df, cols):\n",
    "    return df.agg(*(countDistinct(col(c)).alias(c) for c in cols)).show()\n",
    "\n",
    "def checkNA(df, cols):\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()\n",
    "\n",
    "def getTopCountsValues(df, n, cols):\n",
    "    topCounts_dict= {key: value for (key, value) in zip(cols, \n",
    "                                        [[x[1] for x in df.groupBy(c).count().sort(desc(\"count\")).head(n)] \\\n",
    "                                         for c in cols])}\n",
    "    return topCounts_dict\n",
    "\n",
    "def plotHist(df):\n",
    "    '''plot histogram of numeric features'''\n",
    "    df.hist(figsize=(15,15), bins=15)\n",
    "    return plt.show()\n",
    "\n",
    "def CorrMatrix(df):\n",
    "    '''get correlation matrix of numeric features'''\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(11, 9))\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=.5)\n",
    "    plt.title(\"Correlations between numerical features.\")\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "df = loadData().cache()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "print(\"\\nTEST DATASET ROW COUNTS: \", testDf.count())\n",
    "print(\"\\nTRAIN DATASET ROW COUNTS: \", trainDf.count())\n",
    "print(\"\\nCOLUMN TYPES\\n\", df.dtypes)\n",
    "print(\"\\nMEDIAN OF NUMERIC COLUMNS\\n\", getMedians(trainDf, trainDf.columns[1:14]))\n",
    "\n",
    "print(\"\\nDESCRIPTIONS OF NUMERICAL COLUMNS\")\n",
    "getDescribe(trainDf, trainDf.columns[1:8])\n",
    "getDescribe(trainDf, trainDf.columns[8:14])\n",
    "\n",
    "print(\"\\nCOUNTS OF NAs\")\n",
    "checkNA(trainDf, trainDf.columns[:20])\n",
    "checkNA(trainDf, trainDf.columns[20:])\n",
    "\n",
    "print(\"\\nCOUNTS OF DISTINCT VALUE FOR CATEGORICAL VARIABLE COLUMNS\")\n",
    "getDistinctCount(trainDf, trainDf.columns[15:])\n",
    "\n",
    "print(\"\\nOCCURENCE COUNT OF TOP 3 MOST FREQUENT VALUES FOR EACH VARIABLE\")\n",
    "count_n = 3 # Max can only be 3 because one column (c8) has only 3 categorical values\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[1:12])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[12:23])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[23:34])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[34:])))\n",
    "\n",
    "pandaTrain =trainDf.toPandas()\n",
    "print(\"\\nHistograms for Numeric Values\")\n",
    "plotHist(pandaTrain)\n",
    "print(\"\\nCorrelation Matrix between Numeric Values\")\n",
    "CorrMatrix(pandaTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/anaconda/lib/python3.6/site-packages (0.9.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda/lib/python3.6/site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.3 in /opt/anaconda/lib/python3.6/site-packages (from seaborn) (1.15.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /opt/anaconda/lib/python3.6/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /opt/anaconda/lib/python3.6/site-packages (from seaborn) (0.23.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.7.3)\n",
      "Requirement already satisfied: pytz in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2018.5)\n",
      "Requirement already satisfied: six>=1.10 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (39.2.0)\n",
      "\u001b[31mdistributed 1.22.0 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "2018-12-10 08:18:53 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-10 08:19:05 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "                                                                                \n",
      "TEST DATASET ROW COUNTS:  4578\n",
      "\n",
      "TRAIN DATASET ROW COUNTS:  18379\n",
      "\n",
      "COLUMN TYPES\n",
      " [('Label', 'int'), ('I0', 'int'), ('I1', 'int'), ('I2', 'int'), ('I3', 'int'), ('I4', 'int'), ('I5', 'int'), ('I6', 'int'), ('I7', 'int'), ('I8', 'int'), ('I9', 'int'), ('I10', 'int'), ('I11', 'int'), ('I12', 'int'), ('C0', 'string'), ('C1', 'string'), ('C2', 'string'), ('C3', 'string'), ('C4', 'string'), ('C5', 'string'), ('C6', 'string'), ('C7', 'string'), ('C8', 'string'), ('C9', 'string'), ('C10', 'string'), ('C11', 'string'), ('C12', 'string'), ('C13', 'string'), ('C14', 'string'), ('C15', 'string'), ('C16', 'string'), ('C17', 'string'), ('C18', 'string'), ('C19', 'string'), ('C20', 'string'), ('C21', 'string'), ('C22', 'string'), ('C23', 'string'), ('C24', 'string'), ('C25', 'string')]\n",
      "\n",
      "MEDIAN OF NUMERIC COLUMNS\n",
      " [[1.0], [2.0], [6.0], [4.0], [2715.0], [31.0], [3.0], [7.0], [36.0], [1.0], [1.0], [0.0], [4.0]]\n",
      "\n",
      "DESCRIPTIONS OF NUMERICAL COLUMNS\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|                I0|                I1|                I2|               I3|               I4|                I5|               I6|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|             10109|             18379|             14386|            14314|            17905|             14191|            17600|\n",
      "|   mean|3.5502027895934316|101.80967408455302| 25.63763381064924| 7.30068464440408|18436.65942474169|111.70875907265167|17.12034090909091|\n",
      "| stddev| 9.293289659497264|374.02991892400274|331.46919953261744|8.624079890068804|68931.50353591116|332.34996893862655|66.91093732423182|\n",
      "|    min|                 0|                -2|                 0|                0|                0|                 0|                0|\n",
      "|    max|               292|              6901|             31814|              186|          1585026|             21658|             2802|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|                I7|                I8|                I9|              I10|               I11|               I12|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             18369|             17600|             10109|            17600|              4282|             14314|\n",
      "|   mean|12.419293374707387|106.89028409090909|0.6220199821940845|2.790738636363636|0.8895375992526856| 8.168366634064553|\n",
      "| stddev|14.088934412061754|227.30410625706682|0.6948805353765156|5.310155836571858|3.8061148775636693|11.428526114043635|\n",
      "|    min|                 0|                 0|                 0|                0|                 0|                 0|\n",
      "|    max|               626|              7501|                 7|               91|                86|               226|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "\n",
      "\n",
      "COUNTS OF NAs\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "|Label|  I0| I1|  I2|  I3| I4|  I5| I6| I7| I8|  I9|I10|  I11| I12| C0| C1| C2| C3| C4|  C5|\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "|    0|8270|  0|3993|4065|474|4188|779| 10|779|8270|779|14097|4065|  0|  0|653|653|  0|2260|\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "| C6| C7| C8| C9|C10|C11|C12|C13|C14|C15|C16|C17| C18| C19|C20|  C21|C22|C23| C24| C25|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "|  0|  0|  0|  0|  0|653|  0|  0|  0|653|  0|  0|8082|8082|653|13890|  0|653|8082|8082|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "\n",
      "\n",
      "COUNTS OF DISTINCT VALUE FOR CATEGORICAL VARIABLE COLUMNS\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "| C1|   C2|  C3| C4| C5|  C6| C7| C8|  C9| C10| C11| C12|C13| C14| C15|C16| C17|C18|C19| C20|C21|C22| C23|C24| C25|\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "|435|10328|7028| 78|  9|4533|131|  3|4812|2676|9670|2172| 25|2850|8588|  9|1564|697|  3|9201|  9| 14|4099| 44|3140|\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "\n",
      "\n",
      "OCCURENCE COUNT OF TOP 3 MOST FREQUENT VALUES FOR EACH VARIABLE\n",
      "     I0    I1    I2    I3   I4    I5    I6    I7   I8    I9   I10\n",
      "0  8270  3175  3993  4065  474  4188  4016  2165  779  8270  6062\n",
      "1  4288  2832  2342  2298  430   965  2363  1412  657  4824  4165\n",
      "2  1805  1926  1564  1885  337   493  1578  1340  587  4478  2571\n",
      "\n",
      "\n",
      "     I11   I12    C0    C1   C2   C3     C4    C5   C6     C7     C8\n",
      "0  14097  4065  9262  2175  653  653  12304  7250  403  10909  16559\n",
      "1   3216  2267  3033   756  424  621   2934  4029  236   3071   1815\n",
      "2    618  1804  1575   712  205  430   1179  3391  167   1401      5\n",
      "\n",
      "\n",
      "     C9  C10  C11  C12   C13  C14  C15   C16  C17   C18   C19\n",
      "0  4134  609  653  609  6392  292  653  8386  598  8082  8082\n",
      "1   270  390  430  447  6373  188  430  2448  532  6355  3496\n",
      "2   129  274  424  291  2806  173  424  2103  495   352  3478\n",
      "\n",
      "\n",
      "   C20    C21   C22  C23   C24   C25\n",
      "0  653  13890  8102  923  8082  8082\n",
      "1  430   2596  3607  858  2554   744\n",
      "2  424   1557  2248  813  2038   334\n",
      "                                                                                \n",
      "Histograms for Numeric Values\n",
      "Figure(1500x1500)\n",
      "\n",
      "Correlation Matrix between Numeric Values\n",
      "Figure(1100x900)\n"
     ]
    }
   ],
   "source": [
    "!python loadAndEDA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'submit_job_to_cluster.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=testcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=row_counts.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from running EDA code above:\n",
    "Main dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 9164811)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 36675806)<br>\n",
    "\n",
    "Toy dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 4578)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 18379)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting algorithmImplementation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile algorithmImplementation.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, desc, isnan, when\n",
    "import numpy as np\n",
    "from operator import add\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "MINICLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "MINILOCALPATH = 'data/train_005.txt'\n",
    "NUMERICCOLS = 13\n",
    "CATEGORICALCOLS = 26\n",
    "NUMERICCOLNAMES = ['I{}'.format(i) for i in range(0,NUMERICCOLS)]\n",
    "CATCOLNAMES = ['C{}'.format(i) for i in range(0,CATEGORICALCOLS)]\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"algorithmImplementation\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; TOYCLOUDPATH; TOYLOCALPATH\n",
    "    df = spark.read.csv(path=MINILOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label'] + NUMERICCOLNAMES + CATCOLNAMES\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''\n",
    "    returns approximate median values of the columns given, with null values ignored\n",
    "    '''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "\n",
    "def getMostFrequentCats(df, cols, n):\n",
    "    '''\n",
    "    returns a dict where the key is the column and value is an ordered list\n",
    "    of the top n categories in that column in descending order\n",
    "    '''\n",
    "    freqCatDict = {col: None for col in df.columns[cols:]}\n",
    "    for col in df.columns[cols:]:\n",
    "        listOfRows = df.groupBy(col).count().sort('count', ascending=False).take(n)\n",
    "        topCats = [row[col] for row in listOfRows]\n",
    "        freqCatDict[col] = topCats[:n]\n",
    "    return freqCatDict\n",
    "    \n",
    "\n",
    "def rareReplacer(df, dictOfMostFreqSets):\n",
    "    '''\n",
    "    Iterates through columns and replaces non-Frequent categories with 'rare' string.\n",
    "    '''\n",
    "    for colName in df.columns[NUMERICCOLS+1:]:\n",
    "        bagOfCats = dictOfMostFreqSets[colName]\n",
    "        df = df.withColumn(colName, \n",
    "                           udf(lambda x: 'rare' if x not in bagOfCats else x, StringType())(df[colName])).cache()\n",
    "    return df\n",
    "\n",
    "    \n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + \\\n",
    "                        [row['C{}'.format(i)] for i in range(0, CATEGORICALCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def emitColumnAndCat(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits a record for each categorical column value along with a zero for one-hot\n",
    "    encoding. The emitted values will become a reference dictionary for one-hot encoding in later steps.\n",
    "        Input: (array([features], dtype='<U21'), 0) or (features, label)\n",
    "        Output: ((categorical column, category), 0) or (complex key, value)\n",
    "    The last zero in the output is for initializing one-hot encoding.\n",
    "    \"\"\"\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        yield ((catColName, element), 0)\n",
    "\n",
    "\n",
    "def oneHotEncoder(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits row where categorical columns are replaced with 1-hot encoded columns.\n",
    "        Input: (numerical and categorical features, label)\n",
    "        Output: (numerical and one-hot encoded categorical features, label)\n",
    "    \"\"\"\n",
    "    oneHotDict = copy.deepcopy(oneHotReference)\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        oneHotDict[(catColName, element)] = 1\n",
    "    numericElements = list(line[0][:NUMERICCOLS])\n",
    "    features = np.array(numericElements + [value for key, value in oneHotDict.items()], dtype=np.float)\n",
    "    return (features, line[1])\n",
    "\n",
    "\n",
    "def getMeanAndVar(trainRDD):\n",
    "    \"\"\"\n",
    "    Returns the mean and variance of the training dataset for use in normalizing future records\n",
    "    (e.g. the test set) to be run on model.\n",
    "    \"\"\"\n",
    "    featureMeans = trainRDD.map(lambda x: x[0]).mean()\n",
    "    featureStDevs = np.sqrt(trainRDD.map(lambda x: x[0]).variance())\n",
    "    return featureMeans, featureStDevs\n",
    "    \n",
    "\n",
    "def normalize(dataRDD, featureMeans, featureStDevs):\n",
    "    \"\"\"\n",
    "    Scale and center data around the mean of each feature.\n",
    "    \"\"\"\n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStDevs, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "def dataAugmenter(line):\n",
    "        \"\"\"\n",
    "        Adds a 1 value to the array of feature values for the bias term\n",
    "        \"\"\"\n",
    "        return (np.append([1.0], line[0]), line[1])\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute log loss.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(dataAugmenter)\n",
    "    \n",
    "    # broadcast the weights\n",
    "    bW = sc.broadcast(W)\n",
    "    \n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(bW.value, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def GDUpdateWithReg(dataRDD, W, learningRate = 0.1, regType = None, regParam = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update with ridge or lasso regularization.\n",
    "    Args:\n",
    "        dataRDD - tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        learningRate - (float) defaults to 0.1\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient\n",
    "    Returns:\n",
    "        model   - (array) updated coefficients, bias still at index 0\n",
    "    \"\"\"\n",
    "    # augmented data\n",
    "    augmentedData = dataRDD.map(dataAugmenter)\n",
    "    \n",
    "    # broadcast the weights\n",
    "    bW = sc.broadcast(W)\n",
    "    \n",
    "    # this gets parallelized\n",
    "    def partialGrad(line):\n",
    "        return (-line[1] * (1 - (1 / (1 + np.exp(-line[1] * np.dot(bW.value, line[0]))))) * line[0])\n",
    "    \n",
    "    # reduce to bring it all back together to compute the gradient\n",
    "    grad = augmentedData.map(partialGrad) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    \n",
    "    if regType == 'ridge':\n",
    "        reg = 2*regParam * sum(W[1:])\n",
    "    elif regType == 'lasso':\n",
    "        reg = regParam * sum(W[1:]/np.sign(W[1:]))   \n",
    "    else:\n",
    "        reg = 0\n",
    "    grad = grad + reg\n",
    "    \n",
    "    new_model = W - (grad * learningRate)    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "def GradientDescentWithReg(trainRDD, testRDD, wInit, nSteps = 20, learningRate = 0.1,\n",
    "                         regType = None, regParam = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of regularized gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    trainHistory, testHistory, modelHistory = [], [], []\n",
    "    \n",
    "    model = wInit\n",
    "    for idx in range(nSteps):  \n",
    "        # update the model\n",
    "        model = GDUpdateWithReg(trainRDD, model, learningRate, regType, regParam)\n",
    "        trainingLoss = logLoss(trainRDD, model) \n",
    "        testLoss = logLoss(testRDD, model) \n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        trainHistory.append(trainingLoss)\n",
    "        testHistory.append(testLoss)\n",
    "        modelHistory.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {trainingLoss}\")\n",
    "            print(f\"test loss: {testLoss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return trainHistory, testHistory, modelHistory\n",
    "\n",
    "\n",
    "# get accuracy of model on test data\n",
    "def predictionChecker(line):\n",
    "    \"\"\"\n",
    "    Takes final model from gradient descent iterations and makes a prediction on the row of\n",
    "    test dataset values.\n",
    "    Returns 1 if prediction matches label and 0 otherwise.\n",
    "    \"\"\"\n",
    "    predictionProbability = 1/(1 + np.exp(-1 * np.dot(bModel.value, line[0])))\n",
    "    if predictionProbability > 0.5:\n",
    "        prediction = 1\n",
    "    else:\n",
    "        prediction = 0\n",
    "    if prediction == line[1]:\n",
    "        ans = 1\n",
    "    else:\n",
    "        ans = 0\n",
    "    return ans\n",
    "\n",
    "\n",
    "\n",
    "# load data\n",
    "df = loadData()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "\n",
    "# get top n most frequent categories for each column (in training set only)\n",
    "n = 50\n",
    "mostFreqCatDict = getMostFrequentCats(trainDf, NUMERICCOLS+1, n)\n",
    "\n",
    "# get dict of sets of most frequent categories in each column for fast lookups during filtering (in later code)\n",
    "setsMostFreqCatDict = {key: set(value) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get the top category from each column for imputation of missing values (in training set only)\n",
    "fillNADictCat = {key: (value[0] if value[0] is not None else value[1]) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get dict of median numeric values for imputation of missing values (in training set only)\n",
    "fillNADictNum = {key: value for (key, value) in zip(trainDf.columns[1:NUMERICCOLS+1], \n",
    "                                                    [x[0] for x in getMedians(trainDf,\n",
    "                                                                              trainDf.columns[1:NUMERICCOLS+1])])}\n",
    "\n",
    "# impute missing values in training and test set\n",
    "trainDf = trainDf.na.fill(fillNADictNum) \\\n",
    "                 .na.fill(fillNADictCat)\n",
    "testDf = testDf.na.fill(fillNADictNum) \\\n",
    "               .na.fill(fillNADictCat)\n",
    "\n",
    "# replace low-frequency categories with 'rare' string in training and test set\n",
    "trainDf = rareReplacer(trainDf, setsMostFreqCatDict) # df gets cached in function\n",
    "testDf = rareReplacer(testDf, setsMostFreqCatDict) # df gets cached in function\n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = trainDf.rdd.map(dfToRDD).cache()\n",
    "testRDD = testDf.rdd.map(dfToRDD).cache()\n",
    "        \n",
    "# create and broadcast reference dictionary to be used in constructing 1 hot encoded RDD\n",
    "oneHotReference = trainRDD.flatMap(emitColumnAndCat) \\\n",
    "                          .reduceByKeyLocally(add) # note: only the zero values are being added here (main goal is to output a dictionary)\n",
    "sc.broadcast(oneHotReference)\n",
    "\n",
    "# replace rows with new rows having categorical columns 1-hot encoded\n",
    "trainRDD = trainRDD.map(oneHotEncoder).cache()\n",
    "testRDD = testRDD.map(oneHotEncoder).cache()\n",
    "\n",
    "# normalize RDD\n",
    "featureMeans, featureStDevs = getMeanAndVar(trainRDD)\n",
    "trainRDD = normalize(trainRDD, featureMeans, featureStDevs).cache()\n",
    "testRDD = normalize(testRDD, featureMeans, featureStDevs).cache() # use the mean and st. dev. from trainRDD\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(trainRDD.take(1)[0][0])\n",
    "wInit = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# run training iterations\n",
    "start = time.time()\n",
    "logLossTrain, logLossTest, models = GradientDescentWithReg(trainRDD, testRDD, wInit, nSteps=100, \n",
    "                                                           learningRate = 0.1,\n",
    "                                                           regType=\"ridge\", regParam=0.1)\n",
    "\n",
    "# get model accuracy\n",
    "bModel = sc.broadcast(models[-1])\n",
    "predictionResults = testRDD.map(dataAugmenter) \\\n",
    "                           .map(predictionChecker) \\\n",
    "                           .map(lambda line: (line, 1)) \\\n",
    "                           .reduce(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "accuracy = predictionResults[0]/predictionResults[1]\n",
    "\n",
    "\n",
    "print(\"LOG LOSSES OVER TRAINING SET:\")\n",
    "print(logLossTrain)\n",
    "print(\"LOG LOSSES OVER TEST SET:\")\n",
    "print(logLossTest)\n",
    "print(\"FINAL MODEL:\")\n",
    "print(models[-1])\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")\n",
    "print(\"TEST SET ACCURACY:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-11 06:31:09 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-11 06:31:22 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "LOG LOSSES OVER TRAINING SET:\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "LOG LOSSES OVER TEST SET:\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "FINAL MODEL:\n",
      "[3.13094807e+128 3.13094807e+128 3.13094807e+128 ... 3.13094807e+128\n",
      " 3.13094807e+128 3.13094807e+128]\n",
      "\n",
      "... trained 100 iterations in 176.85037899017334 seconds\n",
      "TEST SET ACCURACY:\n",
      "0.5441240716470074\n"
     ]
    }
   ],
   "source": [
    "!python algorithmImplementation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ImportError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "Uploading pyspark file to GCS\n",
      "finalprojectcluster - RUNNING\n",
      "Submitted job ID b0a8c3e6-5ef7-49cf-b90d-4a083c4d7141\n",
      "Waiting for job to finish...\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=finalprojectcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=algorithmImplementation.py --instance_type=n1-standard-16 --worker_nodes=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
