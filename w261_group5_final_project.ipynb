{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "W261 Machine Learning at Scale<br>\n",
    "12 December 2018\n",
    "\n",
    "Wei Wang;\n",
    "Alice Lam;\n",
    "John Tabbone;\n",
    "Noah Randolph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Formulation\n",
    "\n",
    "Alice Lam: \"enhancing CTR means improved monetization of your current traffic (eyeballs/views). The algorithm to predict CTR accurately is useful for the platform to show specific ads to specific people who would have the highest CTR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing toyDataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile toyDataset.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SEED = 2615\n",
    "NUMERICCOLS = 2\n",
    "ONEHOTCOLS = 2\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def generateToyDataset(w=[8, -3, -1, 3, 8]):\n",
    "    '''generate toy logistic regression dataset with numerical and 1-hot encoded features'''\n",
    "    nrows=8\n",
    "    np.random.seed(SEED)\n",
    "    x1 = np.random.randint(0, 10, nrows)\n",
    "    x2 = np.random.randint(0, 10, nrows)\n",
    "    x3 = np.random.randint(0, 2, nrows) # simulate 1-hot\n",
    "    x4 = np.ones(nrows, np.int8) - x3   # with x3 and x4\n",
    "    noise = np.random.normal(5, 1, nrows)\n",
    "    v = (w[0] + x1*w[1] + x2*w[2] + x3*w[3] + x4*w[4] + noise)\n",
    "    y = (v>0) * 2 - 1 # y = 1 or -1\n",
    "    df = spark.createDataFrame(zip(y.tolist(), x1.tolist(), x2.tolist(), x3.tolist(), x4.tolist()))\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,2)]+['C{}'.format(i) for i in range(0,2)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    return df\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    ################## YOUR CODE HERE ##################\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    ################## (END) YOUR CODE ##################\n",
    "    return loss\n",
    "\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    ################## YOUR CODE HERE ################# \n",
    "    grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0])) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    new_model = W - learningRate * grad\n",
    "    ################## (END) YOUR CODE ################# \n",
    "    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + [row['C{}'.format(i)] for i in range(0, ONEHOTCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data around the mean of each feature.\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStdev, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "# create a toy dataset that includes 1-hot columns for development\n",
    "df = generateToyDataset()   \n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = df.rdd.map(dfToRDD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# normalize RDD\n",
    "normedRDDcached = normalize(trainRDD).cache()\n",
    "print(normedRDDcached.take(1))\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(normedRDDcached.take(1)[0][0])\n",
    "wInitial = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# 1 iteration of gradient descent\n",
    "w = GDUpdate(normedRDDcached, wInitial)\n",
    "\n",
    "nSteps = 5\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    w = GDUpdate(normedRDDcached, w)\n",
    "    loss = logLoss(normedRDDcached, w)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(i,3) for i in w]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:08:34 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[(array([-1.63525964,  0.62123652, -1.63525964,  0.62123652]), 1)]\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 5.3395528396468634\n",
      "Model: [1.159, -0.032, -1.371, -1.253, -0.443]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 4.105591078941364\n",
      "Model: [0.951, -0.233, -1.283, -1.453, -0.355]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 3.342186060749107\n",
      "Model: [0.774, -0.381, -1.215, -1.601, -0.287]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 2.8463811954082905\n",
      "Model: [0.621, -0.494, -1.163, -1.715, -0.235]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 2.5082379162198345\n",
      "Model: [0.488, -0.585, -1.125, -1.805, -0.196]\n"
     ]
    }
   ],
   "source": [
    "!python toyDataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA & Discussion of Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing loadAndEDA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loadAndEDA.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "MINICLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "MINILOCALPATH = 'data/train_005.txt'\n",
    "\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; MINICLOUDPATH; MINILOCALPATH\n",
    "    df = spark.read.csv(path=MINILOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,13)]+['C{}'.format(i) for i in range(0,26)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def displayHead(df, n=5):\n",
    "    '''returns head of the training dataset'''\n",
    "    return df.head(n)\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''returns approximate median values of the columns given, with null values ignored'''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "\n",
    "df = loadData().cache()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "print(\"\\nTEST DATASET ROW COUNTS: \", testDf.count())\n",
    "print(\"\\nTRAIN DATASET ROW COUNTS: \", trainDf.count())\n",
    "# print(\"HEAD\\n\", displayHead(trainDf))\n",
    "print(\"\\nCOLUMN TYPES\\n\", df.dtypes)\n",
    "print(\"\\nMEDIAN OF NUMERIC COLUMNS\\n\", getMedians(trainDf, trainDf.columns[1:14]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-01 19:13:43 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-01 19:13:45 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-12-01 19:13:56 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "                                                                                \n",
      "TEST DATASET ROW COUNTS:  4578\n",
      "\n",
      "TRAIN DATASET ROW COUNTS:  18379\n",
      "\n",
      "COLUMN TYPES\n",
      " [('Label', 'int'), ('I0', 'int'), ('I1', 'int'), ('I2', 'int'), ('I3', 'int'), ('I4', 'int'), ('I5', 'int'), ('I6', 'int'), ('I7', 'int'), ('I8', 'int'), ('I9', 'int'), ('I10', 'int'), ('I11', 'int'), ('I12', 'int'), ('C0', 'string'), ('C1', 'string'), ('C2', 'string'), ('C3', 'string'), ('C4', 'string'), ('C5', 'string'), ('C6', 'string'), ('C7', 'string'), ('C8', 'string'), ('C9', 'string'), ('C10', 'string'), ('C11', 'string'), ('C12', 'string'), ('C13', 'string'), ('C14', 'string'), ('C15', 'string'), ('C16', 'string'), ('C17', 'string'), ('C18', 'string'), ('C19', 'string'), ('C20', 'string'), ('C21', 'string'), ('C22', 'string'), ('C23', 'string'), ('C24', 'string'), ('C25', 'string')]\n",
      "\n",
      "MEDIAN OF NUMERIC COLUMNS\n",
      " [[1.0], [2.0], [6.0], [4.0], [2715.0], [31.0], [3.0], [7.0], [36.0], [1.0], [1.0], [0.0], [4.0]]\n"
     ]
    }
   ],
   "source": [
    "!python loadAndEDA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'submit_job_to_cluster.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=testcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=row_counts.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from running EDA code above:\n",
    "Main dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 9164811)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 36675806)<br>\n",
    "\n",
    "Toy dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 4578)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 18379)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting featureEngineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile featureEngineering.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, desc, isnan, when\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "TOYCLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "TOYLOCALPATH = 'data/train_005.txt'\n",
    "NUMERICCOLS = 13\n",
    "CATEGORICALCOLS = 26\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"featureEngineering\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; TOYCLOUDPATH; TOYLOCALPATH\n",
    "    df = spark.read.csv(path=TOYCLOUDPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,NUMERICCOLS)]+['C{}'.format(i) for i in range(0,CATEGORICALCOLS)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''\n",
    "    returns approximate median values of the columns given, with null values ignored\n",
    "    '''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "\n",
    "def getMostFrequentCats(df, cols, n):\n",
    "    '''\n",
    "    returns a dict where the key is the column and value is an ordered list\n",
    "    of the top n categories in that column in descending order\n",
    "    '''\n",
    "    freqCatDict = {col: None for col in df.columns[cols:]}\n",
    "    for col in df.columns[cols:]:\n",
    "        listOfRows = df.groupBy(col).count().sort('count', ascending=False).take(n)\n",
    "        topCats = [row[col] for row in listOfRows]\n",
    "        freqCatDict[col] = topCats[:n]\n",
    "    return freqCatDict\n",
    "    \n",
    "\n",
    "def rareReplacer(df, dictOfMostFreqSets):\n",
    "    '''\n",
    "    Iterates through columns and replaces non-Frequent categories with 'rare' string.\n",
    "    '''\n",
    "    for colName in df.columns[NUMERICCOLS+1:]:\n",
    "        bagOfCats = dictOfMostFreqSets[colName]\n",
    "        df = df.withColumn(colName, udf(lambda x: 'rare' if x not in bagOfCats else x, StringType())(df[colName])).cache()\n",
    "    return df\n",
    "\n",
    "    \n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + [row['C{}'.format(i)] for i in range(0, CATEGORICALCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "# load data\n",
    "df = loadData()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "testDf.cache()\n",
    "trainDf.cache()\n",
    "\n",
    "# get top n most frequent categories for each column (in training set only)\n",
    "n = 100\n",
    "mostFreqCatDict = getMostFrequentCats(trainDf, NUMERICCOLS+1, n)\n",
    "\n",
    "# get dict of sets of most frequent categories in each column for fast lookups during filtering (in later code)\n",
    "setsMostFreqCatDict = {key: set(value) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get the top category from each column for imputation of missing values (in training set only)\n",
    "fillNADictCat = {key: (value[0] if value[0] is not None else value[1]) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get dict of median numeric values for imputation of missing values (in training set only)\n",
    "fillNADictNum = {key: value for (key, value) in zip(trainDf.columns[1:NUMERICCOLS+1], \n",
    "                                                    [x[0] for x in getMedians(trainDf,\n",
    "                                                                              trainDf.columns[1:NUMERICCOLS+1])])}\n",
    "\n",
    "# impute missing values in training and test set\n",
    "trainDf = trainDf.na.fill(fillNADictNum) \\\n",
    "                 .na.fill(fillNADictCat).cache()\n",
    "testDf = testDf.na.fill(fillNADictNum) \\\n",
    "               .na.fill(fillNADictCat).cache()\n",
    "\n",
    "# replace low-frequency categories with 'rare' string in training and test set\n",
    "trainDf = rareReplacer(trainDf, setsMostFreqCatDict)\n",
    "testDf = rareReplacer(testDf, setsMostFreqCatDict)\n",
    "\n",
    "\n",
    "print(trainDf.take(3))\n",
    "print(testDf.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 20:47:49 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-05 20:48:00 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "[Row(Label=0, I0=1, I1=-2, I2=1, I3=0, I4=14, I5=31, I6=0, I7=16, I8=17, I9=1, I10=0, I11=0, I12=0, C0='3c9d8785', C1='207b2d81', C2='rare', C3='rare', C4='30903e74', C5='fe6b92e5', C6='rare', C7='0b153874', C8='a73ee510', C9='3b08e48b', C10='rare', C11='rare', C12='rare', C13='b28479f6', C14='rare', C15='rare', C16='1e88c74f', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='c9d4222a', C22='be7c41b4', C23='rare', C24='001f3601', C25='rare'), Row(Label=0, I0=1, I1=-2, I2=1, I3=2, I4=17560, I5=55, I6=22, I7=2, I8=105, I9=1, I10=3, I11=0, I12=2, C0='05db9164', C1='rare', C2='d032c263', C3='c18be181', C4='4cf72387', C5='7e0ccccf', C6='rare', C7='0b153874', C8='a73ee510', C9='rare', C10='rare', C11='6aaba33c', C12='rare', C13='b28479f6', C14='rare', C15='b041b04a', C16='07c540c4', C17='rare', C18='21ddcdc9', C19='5840adea', C20='723b4dfd', C21='ad3062eb', C22='32c7478e', C23='3fdb382b', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-2, I2=2, I3=4, I4=33245, I5=26, I6=0, I7=0, I8=19, I9=1, I10=0, I11=0, I12=4, C0='05db9164', C1='58e67aaf', C2='rare', C3='rare', C4='384874ce', C5='7e0ccccf', C6='rare', C7='0b153874', C8='a73ee510', C9='rare', C10='rare', C11='rare', C12='rare', C13='07d13a8f', C14='10935a85', C15='rare', C16='d4bb7bd8', C17='c21c3e4c', C18='1d1eb838', C19='a458ea53', C20='rare', C21='ad3062eb', C22='55dd3565', C23='rare', C24='9b3e8820', C25='rare')]\n",
      "[Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=7, I8=36, I9=1, I10=1, I11=0, I12=4, C0='5a9ed9b0', C1='38a947a1', C2='d032c263', C3='c18be181', C4='384874ce', C5='7e0ccccf', C6='88002ee1', C7='1f89b562', C8='7cc72ec2', C9='3b08e48b', C10='f1b78ab4', C11='6aaba33c', C12='6e5da64f', C13='64c94865', C14='rare', C15='b041b04a', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='723b4dfd', C21='ad3062eb', C22='55dd3565', C23='3fdb382b', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=7, I8=36, I9=1, I10=1, I11=0, I12=4, C0='5a9ed9b0', C1='38a947a1', C2='rare', C3='rare', C4='25c83c98', C5='7e0ccccf', C6='rare', C7='0b153874', C8='7cc72ec2', C9='3b08e48b', C10='rare', C11='rare', C12='rare', C13='07d13a8f', C14='rare', C15='rare', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='ad3062eb', C22='be7c41b4', C23='rare', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=0, I8=36, I9=1, I10=1, I11=0, I12=4, C0='05db9164', C1='38a947a1', C2='rare', C3='rare', C4='25c83c98', C5='fbad5c96', C6='970f01b2', C7='0b153874', C8='7cc72ec2', C9='3b08e48b', C10='36bccca0', C11='rare', C12='80467802', C13='07d13a8f', C14='rare', C15='rare', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='ad3062eb', C22='423fab69', C23='rare', C24='001f3601', C25='49d68486')]\n"
     ]
    }
   ],
   "source": [
    "!python featureEngineering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ImportError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "Uploading pyspark file to GCS\n",
      "featengcluster - RUNNING\n",
      "Submitted job ID 313fe53a-83bd-43af-a06e-8fb37052ba94\n",
      "Waiting for job to finish...\n",
      "Job finished.\n",
      "Downloading output file\n",
      "Received job output b\"Ivy Default Cache set to: /root/.ivy2/cache\\nThe jars for the packages stored in: /root/.ivy2/jars\\n:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\\ncom.databricks#spark-xml_2.11 added as a dependency\\ngraphframes#graphframes added as a dependency\\ncom.databricks#spark-avro_2.11 added as a dependency\\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\\n\\tconfs: [default]\\n\\tfound com.databricks#spark-xml_2.11;0.4.1 in central\\n\\tfound graphframes#graphframes;0.5.0-spark2.1-s_2.11 in spark-packages\\n\\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\\n\\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\\n\\tfound org.scala-lang#scala-reflect;2.11.0 in central\\n\\tfound org.slf4j#slf4j-api;1.7.7 in central\\n\\tfound com.databricks#spark-avro_2.11;4.0.0 in central\\n\\tfound org.apache.avro#avro;1.7.6 in central\\n\\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\\n\\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\\n\\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\\n\\tfound org.xerial.snappy#snappy-java;1.0.5 in central\\n\\tfound org.apache.commons#commons-compress;1.4.1 in central\\n\\tfound org.tukaani#xz;1.0 in central\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.4.1/spark-xml_2.11-0.4.1.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-xml_2.11;0.4.1!spark-xml_2.11.jar (46ms)\\ndownloading http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar ...\\n\\t[SUCCESSFUL ] graphframes#graphframes;0.5.0-spark2.1-s_2.11!graphframes.jar (152ms)\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-avro_2.11;4.0.0!spark-avro_2.11.jar (24ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2!scala-logging-api_2.11.jar (13ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2!scala-logging-slf4j_2.11.jar (14ms)\\ndownloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.0/scala-reflect-2.11.0.jar ...\\n\\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.0!scala-reflect.jar (287ms)\\ndownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar ...\\n\\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.7!slf4j-api.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.6/avro-1.7.6.jar ...\\n\\t[SUCCESSFUL ] org.apache.avro#avro;1.7.6!avro.jar(bundle) (25ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (18ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (36ms)\\ndownloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\\n\\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (13ms)\\ndownloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar ...\\n\\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.0.5!snappy-java.jar(bundle) (46ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\\n\\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (15ms)\\ndownloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\\n\\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (14ms)\\n:: resolution report :: resolve 3288ms :: artifacts dl 724ms\\n\\t:: modules in use:\\n\\tcom.databricks#spark-avro_2.11;4.0.0 from central in [default]\\n\\tcom.databricks#spark-xml_2.11;0.4.1 from central in [default]\\n\\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\\n\\tgraphframes#graphframes;0.5.0-spark2.1-s_2.11 from spark-packages in [default]\\n\\torg.apache.avro#avro;1.7.6 from central in [default]\\n\\torg.apache.commons#commons-compress;1.4.1 from central in [default]\\n\\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\\n\\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\\n\\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\\n\\torg.slf4j#slf4j-api;1.7.7 from central in [default]\\n\\torg.tukaani#xz;1.0 from central in [default]\\n\\torg.xerial.snappy#snappy-java;1.0.5 from central in [default]\\n\\t:: evicted modules:\\n\\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\torg.slf4j#slf4j-api;1.6.4 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\t---------------------------------------------------------------------\\n\\t|                  |            modules            ||   artifacts   |\\n\\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\\n\\t---------------------------------------------------------------------\\n\\t|      default     |   16  |   14  |   14  |   2   ||   14  |   14  |\\n\\t---------------------------------------------------------------------\\n:: retrieving :: org.apache.spark#spark-submit-parent\\n\\tconfs: [default]\\n\\t14 artifacts copied, 0 already retrieved (7998kB/49ms)\\n18/12/05 21:35:22 INFO org.spark_project.jetty.util.log: Logging initialized @7414ms\\n18/12/05 21:35:22 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT\\n18/12/05 21:35:22 INFO org.spark_project.jetty.server.Server: Started @7503ms\\n18/12/05 21:35:22 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@3841473b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n18/12/05 21:35:23 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.10-hadoop2\\n18/12/05 21:35:24 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at featengcluster-m/10.128.0.2:8032\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.databricks_spark-xml_2.11-0.4.1.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.tukaani_xz-1.0.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1544045561257_0001\\n18/12/05 21:35:46 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\\n\\r[Stage 1:>                                                          (0 + 2) / 2]\\r[Stage 1:=============================>                             (1 + 1) / 2]\\r[Stage 2:=========>                                              (35 + 7) / 200]\\r[Stage 2:===================>                                    (69 + 6) / 200]\\r[Stage 2:============================>                          (105 + 6) / 200]\\r[Stage 2:================================>                      (118 + 7) / 200]\\r[Stage 2:=================================================>     (181 + 6) / 200]\\r[Stage 2:======================================================>(198 + 2) / 200]\\r                                                                                \\r\\r[Stage 4:============================>                          (103 + 6) / 200]\\r[Stage 4:========================================>              (148 + 6) / 200]\\r[Stage 4:===================================================>   (189 + 6) / 200]\\r[Stage 4:======================================================>(198 + 2) / 200]\\r[Stage 4:======================================================>(199 + 1) / 200]\\r                                                                                \\r\\r[Stage 6:====================================================>  (190 + 7) / 200]\\r                                                                                \\r[Row(Label=0, I0=1, I1=-2, I2=1, I3=0, I4=14, I5=31, I6=0, I7=16, I8=17, I9=1, I10=0, I11=0, I12=0, C0='3c9d8785', C1='207b2d81', C2='rare', C3='rare', C4='30903e74', C5='fe6b92e5', C6='rare', C7='0b153874', C8='a73ee510', C9='3b08e48b', C10='rare', C11='rare', C12='rare', C13='b28479f6', C14='rare', C15='rare', C16='1e88c74f', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='c9d4222a', C22='be7c41b4', C23='rare', C24='001f3601', C25='rare'), Row(Label=0, I0=1, I1=-2, I2=1, I3=2, I4=17560, I5=55, I6=22, I7=2, I8=105, I9=1, I10=3, I11=0, I12=2, C0='05db9164', C1='rare', C2='d032c263', C3='c18be181', C4='4cf72387', C5='7e0ccccf', C6='rare', C7='0b153874', C8='a73ee510', C9='rare', C10='rare', C11='6aaba33c', C12='rare', C13='b28479f6', C14='rare', C15='b041b04a', C16='07c540c4', C17='rare', C18='21ddcdc9', C19='5840adea', C20='723b4dfd', C21='ad3062eb', C22='32c7478e', C23='3fdb382b', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-2, I2=2, I3=4, I4=33245, I5=26, I6=0, I7=0, I8=19, I9=1, I10=0, I11=0, I12=4, C0='05db9164', C1='58e67aaf', C2='rare', C3='rare', C4='384874ce', C5='7e0ccccf', C6='rare', C7='0b153874', C8='a73ee510', C9='rare', C10='rare', C11='rare', C12='rare', C13='07d13a8f', C14='10935a85', C15='rare', C16='d4bb7bd8', C17='c21c3e4c', C18='1d1eb838', C19='a458ea53', C20='rare', C21='ad3062eb', C22='55dd3565', C23='rare', C24='9b3e8820', C25='rare')]\\n[Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=7, I8=36, I9=1, I10=1, I11=0, I12=4, C0='5a9ed9b0', C1='38a947a1', C2='d032c263', C3='c18be181', C4='384874ce', C5='7e0ccccf', C6='88002ee1', C7='1f89b562', C8='7cc72ec2', C9='3b08e48b', C10='f1b78ab4', C11='6aaba33c', C12='6e5da64f', C13='64c94865', C14='rare', C15='b041b04a', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='723b4dfd', C21='ad3062eb', C22='55dd3565', C23='3fdb382b', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=7, I8=36, I9=1, I10=1, I11=0, I12=4, C0='5a9ed9b0', C1='38a947a1', C2='rare', C3='rare', C4='25c83c98', C5='7e0ccccf', C6='rare', C7='0b153874', C8='7cc72ec2', C9='3b08e48b', C10='rare', C11='rare', C12='rare', C13='07d13a8f', C14='rare', C15='rare', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='ad3062eb', C22='be7c41b4', C23='rare', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=0, I8=36, I9=1, I10=1, I11=0, I12=4, C0='05db9164', C1='38a947a1', C2='rare', C3='rare', C4='25c83c98', C5='fbad5c96', C6='970f01b2', C7='0b153874', C8='7cc72ec2', C9='3b08e48b', C10='36bccca0', C11='rare', C12='80467802', C13='07d13a8f', C14='rare', C15='rare', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='ad3062eb', C22='423fab69', C23='rare', C24='001f3601', C25='49d68486')]\\n18/12/05 21:36:46 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@3841473b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n\"\n",
      "Tearing down cluster\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=featengcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=featureEngineering.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
