{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "W261 Machine Learning at Scale<br>\n",
    "12 December 2018\n",
    "\n",
    "Wei Wang;\n",
    "Alice Lam;\n",
    "John Tabbone;\n",
    "Noah Randolph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Formulation\n",
    "\n",
    "Click-Through-Rate (CTR) is a key metric that measure online advertisment performance. It demonstrates both 1) how effective the advertising platforms are, and 2) how effective the advertising campaigns are in targeting the right audience. The advertising rate a platform can charge is highly correlated with CTR. Thus platforms are motivated to improve their CTR to maximize their revenues. The goal of our analysis is to predict CTR, which can be useful to priortize which ad to show whom in order to maximize advertising revenue.\n",
    "\n",
    "Online platforms ranging from Google, Facebook, to casual game apps are essentially \"online real estate\" that draws \"traffic\", i.e. eyeballs on the screen. They can monetize these traffic by charging businesses advertising fee for putting up ads/links on the screen. Traditionally, the fee is charged per impression, hence termed \"CPI - Cost-per-impression\". Advertisers would have a campaign budget and a desired return on investment from the budget, e.g. bringing 1 million people to their website with a $\\$1,000,000$ budget. If the platform's CTR is 10%, the advertisers can only up to $\\$0.10$ per impression. If the platform's CTR is 100%, then the maximum CPI could reach $\\$1$. Online advertising model has gradually evolved to pay-for-performance, i.e. advertisers would only pay if the link is being clicked. Regardless of the advertising revneue model, platforms are highly incentivized to improve CTR.\n",
    "\n",
    "To improve CTR, a platform should present the ad X to the group of people who are most likely to click the ad. On the other hand, given the limited opportunity ads can be shown to a specific person at any given time, the platform should also present the ads that the person is most likely to click. Understanding browsing and clicking behavior of each individual is thus essential in making CTR prediction for each user. Some of the features that are likely significant for such predictions are: time, day of week, location, gender, age, device they are using, sites they are visiting, sites they came from, topics of the ad, color of the ad, pixel location of the ad on the screen, etc. Unfortunately, the data we have is completely anonymiezd which prevents us from conducting feature selection or engineering that is backed by contextual understanding.\n",
    "\n",
    "Digital ad market has soared to $\\$88b^1$ and online advertising accounts for $40\\%^2$ of total advertising budget in 2018. The huge market has attracted significant work in the literature on modeling clicks and CTR. The challenge of building a CTR prediction model is to handle massive amount of data that has numerous categorical variables with high cardinality. In this use case, it is also critical that the prediction can be done in seconds. For example, given the fact that the person is in this location and launched this app at this time of the day, the algorithm should be able to predict the CTR in split second in order to decide which ad to push to the person. If it takes more than five seconds, the opportunity to show the ads would be lost already. Any accurate prediction delivered too late is almost effectively useless. Another approach to mitigate this issue is to __not__ include information generated from the users from last few seconds/minutes/hours, enabling us to have generated a prediction set ahead of time. This approach may be at the cost of accuracy as well because immediate information such as current location, last article the person look at, etc, can enhance accuracy significantly. This is a compromise the platforms need to evalaute given their specific business needs and infrastructure. \n",
    "\n",
    "We have no information on whether some of the features in the dataset is immediate features that's received a few seconds prior to the display of the ad. We choose to assume the features may contain such information. Thus we do not attempt to build a very sophisticated model far superior than existing models at the cost of speed. Our priority would be speed over performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$^1$ https://adexchanger.com/online-advertising/digital-ad-market-soars-to-88-billion-facebook-and-google-contribute-90-of-growth/ <br>\n",
    "$^2$ https://www.marketing-interactive.com/online-advertising-to-account-for-44-6-of-global-ad-spend/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting toyDataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile toyDataset.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SEED = 2615\n",
    "NUMERICCOLS = 2\n",
    "ONEHOTCOLS = 2\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def generateToyDataset(w=[8, -3, -1, 3, 8]):\n",
    "    '''generate toy logistic regression dataset with numerical and 1-hot encoded features'''\n",
    "    nrows=8\n",
    "    np.random.seed(SEED)\n",
    "    x1 = np.random.randint(0, 10, nrows)\n",
    "    x2 = np.random.randint(0, 10, nrows)\n",
    "    x3 = np.random.randint(0, 2, nrows) # simulate 1-hot\n",
    "    x4 = np.ones(nrows, np.int8) - x3   # with x3 and x4\n",
    "    noise = np.random.normal(5, 1, nrows)\n",
    "    v = (w[0] + x1*w[1] + x2*w[2] + x3*w[3] + x4*w[4] + noise)\n",
    "    y = (v>0) * 2 - 1 # y = 1 or -1\n",
    "    df = spark.createDataFrame(zip(y.tolist(), x1.tolist(), x2.tolist(), x3.tolist(), x4.tolist()))\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,2)]+['C{}'.format(i) for i in range(0,2)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    return df\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0])) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    new_model = W - learningRate * grad \n",
    "    return new_model\n",
    "\n",
    "\n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + [row['C{}'.format(i)] for i in range(0, ONEHOTCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data around the mean of each feature.\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStdev, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "# create a toy dataset that includes 1-hot columns for development\n",
    "df = generateToyDataset()   \n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = df.rdd.map(dfToRDD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# normalize RDD\n",
    "normedRDDcached = normalize(trainRDD).cache()\n",
    "print(normedRDDcached.take(1))\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(normedRDDcached.take(1)[0][0])\n",
    "wInitial = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# 1 iteration of gradient descent\n",
    "w = GDUpdate(normedRDDcached, wInitial)\n",
    "\n",
    "nSteps = 5\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    w = GDUpdate(normedRDDcached, w)\n",
    "    loss = logLoss(normedRDDcached, w)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(i,3) for i in w]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 22:39:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-05 22:39:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-12-05 22:39:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "[(array([-1.63525964,  0.62123652,  1.        , -1.        ]), 1)]\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 7.7014879401802405\n",
      "Model: [1.101, -0.162, -1.391, -0.468, -0.797]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 6.1460241873746195\n",
      "Model: [0.865, -0.478, -1.282, -0.469, -0.795]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 5.006600355698076\n",
      "Model: [0.66, -0.743, -1.182, -0.491, -0.774]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 4.183106733189442\n",
      "Model: [0.485, -0.964, -1.092, -0.52, -0.744]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 3.591671886745936\n",
      "Model: [0.337, -1.15, -1.016, -0.55, -0.714]\n"
     ]
    }
   ],
   "source": [
    "!python toyDataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA & Discussion of Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting loadAndEDA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loadAndEDA.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import udf, col, countDistinct, isnan, when, count, desc\n",
    "import pandas as pd\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "MINICLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "MINILOCALPATH = 'data/train_005.txt'\n",
    "\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; MINICLOUDPATH; MINILOCALPATH\n",
    "    df = spark.read.csv(path=MINILOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,13)]+['C{}'.format(i) for i in range(0,26)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def displayHead(df, n=5):\n",
    "    '''returns head of the training dataset'''\n",
    "    return df.head(n)\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''returns approximate median values of the columns given, with null values ignored'''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "def getDescribe(df, cols):\n",
    "    return df.select(cols).describe().show()\n",
    "\n",
    "def getDistinctCount(df, cols):\n",
    "    return df.agg(*(countDistinct(col(c)).alias(c) for c in cols)).show()\n",
    "\n",
    "def checkNA(df, cols):\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()\n",
    "\n",
    "def getCorrMatrix(df, cols):\n",
    "    df = df.select(cols)\n",
    "    col_names = df.columns\n",
    "    features = df.rdd.map(lambda row: row[0:])\n",
    "    corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "    corr_df = pd.DataFrame(corr_mat)\n",
    "    corr_df.index, corr_df.columns = col_names, col_names\n",
    "    return corr_df\n",
    "\n",
    "def getTopCountsValues(df, n, cols):\n",
    "    topCounts_dict= {key: value for (key, value) in zip(cols, \n",
    "                                        [[x[1] for x in df.groupBy(c).count().sort(desc(\"count\")).head(n)] \\\n",
    "                                         for c in cols])}\n",
    "    return topCounts_dict\n",
    "\n",
    "df = loadData().cache()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "print(\"\\nTEST DATASET ROW COUNTS: \", testDf.count())\n",
    "print(\"\\nTRAIN DATASET ROW COUNTS: \", trainDf.count())\n",
    "# print(\"HEAD\\n\", displayHead(trainDf))\n",
    "print(\"\\nCOLUMN TYPES\\n\", df.dtypes)\n",
    "print(\"\\nMEDIAN OF NUMERIC COLUMNS\\n\", getMedians(trainDf, trainDf.columns[1:14]))\n",
    "\n",
    "print(\"\\nDESCRIPTIONS OF NUMERICAL COLUMNS\")\n",
    "getDescribe(trainDf, trainDf.columns[1:8])\n",
    "getDescribe(trainDf, trainDf.columns[8:14])\n",
    "\n",
    "print(\"\\nCOUNTS OF NAs\")\n",
    "checkNA(trainDf, trainDf.columns[:20])\n",
    "checkNA(trainDf, trainDf.columns[20:])\n",
    "\n",
    "#print(\"\\nCORRELATION MATRIX\")\n",
    "#getCorrMatrix(trainDf, trainDf.columns[1:14]) # This doesn't work if there's NA in there\n",
    "\n",
    "print(\"\\nCOUNTS OF DISTINCT VALUE FOR CATEGORICAL VARIABLE COLUMNS\")\n",
    "getDistinctCount(trainDf, trainDf.columns[15:])\n",
    "\n",
    "print(\"\\nOCCURENCE COUNT OF TOP 3 MOST FREQUENT VALUES FOR EACH VARIABLE\")\n",
    "count_n = 3 # Max can only be 3 because one column (c8) has only 3 categorical values\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[1:12])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[12:23])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[23:34])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[34:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07 01:11:44 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-07 01:11:46 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-12-07 01:11:46 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2018-12-07 01:12:01 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "                                                                                \n",
      "TEST DATASET ROW COUNTS:  4578\n",
      "\n",
      "TRAIN DATASET ROW COUNTS:  18379\n",
      "\n",
      "COLUMN TYPES\n",
      " [('Label', 'int'), ('I0', 'int'), ('I1', 'int'), ('I2', 'int'), ('I3', 'int'), ('I4', 'int'), ('I5', 'int'), ('I6', 'int'), ('I7', 'int'), ('I8', 'int'), ('I9', 'int'), ('I10', 'int'), ('I11', 'int'), ('I12', 'int'), ('C0', 'string'), ('C1', 'string'), ('C2', 'string'), ('C3', 'string'), ('C4', 'string'), ('C5', 'string'), ('C6', 'string'), ('C7', 'string'), ('C8', 'string'), ('C9', 'string'), ('C10', 'string'), ('C11', 'string'), ('C12', 'string'), ('C13', 'string'), ('C14', 'string'), ('C15', 'string'), ('C16', 'string'), ('C17', 'string'), ('C18', 'string'), ('C19', 'string'), ('C20', 'string'), ('C21', 'string'), ('C22', 'string'), ('C23', 'string'), ('C24', 'string'), ('C25', 'string')]\n",
      "\n",
      "MEDIAN OF NUMERIC COLUMNS\n",
      " [[1.0], [2.0], [6.0], [4.0], [2715.0], [31.0], [3.0], [7.0], [36.0], [1.0], [1.0], [0.0], [4.0]]\n",
      "\n",
      "DESCRIPTIONS OF NUMERICAL COLUMNS\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|                I0|                I1|                I2|               I3|               I4|                I5|               I6|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|             10109|             18379|             14386|            14314|            17905|             14191|            17600|\n",
      "|   mean|3.5502027895934316|101.80967408455302| 25.63763381064924| 7.30068464440408|18436.65942474169|111.70875907265167|17.12034090909091|\n",
      "| stddev| 9.293289659497264|374.02991892400274|331.46919953261744|8.624079890068804|68931.50353591116|332.34996893862655|66.91093732423182|\n",
      "|    min|                 0|                -2|                 0|                0|                0|                 0|                0|\n",
      "|    max|               292|              6901|             31814|              186|          1585026|             21658|             2802|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|                I7|                I8|                I9|              I10|               I11|               I12|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             18369|             17600|             10109|            17600|              4282|             14314|\n",
      "|   mean|12.419293374707387|106.89028409090909|0.6220199821940845|2.790738636363636|0.8895375992526856| 8.168366634064553|\n",
      "| stddev|14.088934412061754|227.30410625706682|0.6948805353765156|5.310155836571858|3.8061148775636693|11.428526114043635|\n",
      "|    min|                 0|                 0|                 0|                0|                 0|                 0|\n",
      "|    max|               626|              7501|                 7|               91|                86|               226|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "\n",
      "\n",
      "COUNTS OF NAs\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "|Label|  I0| I1|  I2|  I3| I4|  I5| I6| I7| I8|  I9|I10|  I11| I12| C0| C1| C2| C3| C4|  C5|\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "|    0|8270|  0|3993|4065|474|4188|779| 10|779|8270|779|14097|4065|  0|  0|653|653|  0|2260|\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "| C6| C7| C8| C9|C10|C11|C12|C13|C14|C15|C16|C17| C18| C19|C20|  C21|C22|C23| C24| C25|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "|  0|  0|  0|  0|  0|653|  0|  0|  0|653|  0|  0|8082|8082|653|13890|  0|653|8082|8082|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "\n",
      "\n",
      "COUNTS OF DISTINCT VALUE FOR CATEGORICAL VARIABLE COLUMNS\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "| C1|   C2|  C3| C4| C5|  C6| C7| C8|  C9| C10| C11| C12|C13| C14| C15|C16| C17|C18|C19| C20|C21|C22| C23|C24| C25|\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "|435|10328|7028| 78|  9|4533|131|  3|4812|2676|9670|2172| 25|2850|8588|  9|1564|697|  3|9201|  9| 14|4099| 44|3140|\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "\n",
      "\n",
      "OCCURENCE COUNT OF TOP 5 MOST FREQUENT VALUES FOR EACH VARIABLE\n",
      "     I0    I1    I2    I3   I4    I5    I6    I7   I8    I9   I10\n",
      "0  8270  3175  3993  4065  474  4188  4016  2165  779  8270  6062\n",
      "1  4288  2832  2342  2298  430   965  2363  1412  657  4824  4165\n",
      "2  1805  1926  1564  1885  337   493  1578  1340  587  4478  2571\n",
      "\n",
      "\n",
      "     I11   I12    C0    C1   C2   C3     C4    C5   C6     C7     C8\n",
      "0  14097  4065  9262  2175  653  653  12304  7250  403  10909  16559\n",
      "1   3216  2267  3033   756  424  621   2934  4029  236   3071   1815\n",
      "2    618  1804  1575   712  205  430   1179  3391  167   1401      5\n",
      "\n",
      "\n",
      "     C9  C10  C11  C12   C13  C14  C15   C16  C17   C18   C19\n",
      "0  4134  609  653  609  6392  292  653  8386  598  8082  8082\n",
      "1   270  390  430  447  6373  188  430  2448  532  6355  3496\n",
      "2   129  274  424  291  2806  173  424  2103  495   352  3478\n",
      "\n",
      "\n",
      "   C20    C21   C22  C23   C24   C25\n",
      "0  653  13890  8102  923  8082  8082\n",
      "1  430   2596  3607  858  2554   744\n",
      "2  424   1557  2248  813  2038   334\n"
     ]
    }
   ],
   "source": [
    "!python loadAndEDA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'submit_job_to_cluster.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=testcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=row_counts.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from running EDA code above:\n",
    "Main dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 9164811)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 36675806)<br>\n",
    "\n",
    "Toy dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 4578)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 18379)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting featureEngineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile featureEngineering.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, desc, isnan, when\n",
    "import numpy as np\n",
    "from operator import add\n",
    "import copy\n",
    "\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "TOYCLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "TOYLOCALPATH = 'data/train_005.txt'\n",
    "NUMERICCOLS = 13\n",
    "CATEGORICALCOLS = 26\n",
    "NUMERICCOLNAMES = ['I{}'.format(i) for i in range(0,NUMERICCOLS)]\n",
    "CATCOLNAMES = ['C{}'.format(i) for i in range(0,CATEGORICALCOLS)]\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"featureEngineering\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; TOYCLOUDPATH; TOYLOCALPATH\n",
    "    df = spark.read.csv(path=TOYLOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label'] + NUMERICCOLNAMES + CATCOLNAMES\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''\n",
    "    returns approximate median values of the columns given, with null values ignored\n",
    "    '''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "\n",
    "def getMostFrequentCats(df, cols, n):\n",
    "    '''\n",
    "    returns a dict where the key is the column and value is an ordered list\n",
    "    of the top n categories in that column in descending order\n",
    "    '''\n",
    "    freqCatDict = {col: None for col in df.columns[cols:]}\n",
    "    for col in df.columns[cols:]:\n",
    "        listOfRows = df.groupBy(col).count().sort('count', ascending=False).take(n)\n",
    "        topCats = [row[col] for row in listOfRows]\n",
    "        freqCatDict[col] = topCats[:n]\n",
    "    return freqCatDict\n",
    "    \n",
    "\n",
    "def rareReplacer(df, dictOfMostFreqSets):\n",
    "    '''\n",
    "    Iterates through columns and replaces non-Frequent categories with 'rare' string.\n",
    "    '''\n",
    "    for colName in df.columns[NUMERICCOLS+1:]:\n",
    "        bagOfCats = dictOfMostFreqSets[colName]\n",
    "        df = df.withColumn(colName, udf(lambda x: 'rare' if x not in bagOfCats else x, StringType())(df[colName])).cache()\n",
    "    return df\n",
    "\n",
    "    \n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + [row['C{}'.format(i)] for i in range(0, CATEGORICALCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def emitColumnAndCat(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits a record for each categorical column value along with a zero for one-hot encoding.\n",
    "    The emitted values will become a reference dictionary for one-hot encoding in later steps.\n",
    "        Input: (array([features], dtype='<U21'), 0) or (features, label)\n",
    "        Output: ((categorical column, category), 0) or (complex key, value)\n",
    "    The last zero in the output is for initializing one-hot encoding.\n",
    "    \"\"\"\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        yield ((catColName, element), 0)\n",
    "\n",
    "\n",
    "def oneHotEncoder(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits row where categorical columns are replaced with 1-hot encoded columns.\n",
    "        Input: (numerical and categorical features, label)\n",
    "        Output: (numerical and one-hot encoded categorical features, label)\n",
    "    \"\"\"\n",
    "    oneHotDict = copy.deepcopy(oneHotReference)\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        oneHotDict[(catColName, element)] = 1\n",
    "    numericElements = list(line[0][:NUMERICCOLS])\n",
    "    return (numericElements + [value for key, value in oneHotDict.items()], line[1])\n",
    "\n",
    "\n",
    "# load data\n",
    "df = loadData()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "testDf.cache()\n",
    "trainDf.cache()\n",
    "\n",
    "# get top n most frequent categories for each column (in training set only)\n",
    "n = 10\n",
    "mostFreqCatDict = getMostFrequentCats(trainDf, NUMERICCOLS+1, n)\n",
    "\n",
    "# get dict of sets of most frequent categories in each column for fast lookups during filtering (in later code)\n",
    "setsMostFreqCatDict = {key: set(value) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get the top category from each column for imputation of missing values (in training set only)\n",
    "fillNADictCat = {key: (value[0] if value[0] is not None else value[1]) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get dict of median numeric values for imputation of missing values (in training set only)\n",
    "fillNADictNum = {key: value for (key, value) in zip(trainDf.columns[1:NUMERICCOLS+1], \n",
    "                                                    [x[0] for x in getMedians(trainDf,\n",
    "                                                                              trainDf.columns[1:NUMERICCOLS+1])])}\n",
    "\n",
    "# impute missing values in training and test set\n",
    "trainDf = trainDf.na.fill(fillNADictNum) \\\n",
    "                 .na.fill(fillNADictCat).cache()\n",
    "testDf = testDf.na.fill(fillNADictNum) \\\n",
    "               .na.fill(fillNADictCat).cache()\n",
    "\n",
    "# replace low-frequency categories with 'rare' string in training and test set\n",
    "trainDf = rareReplacer(trainDf, setsMostFreqCatDict) # df gets cached in function\n",
    "testDf = rareReplacer(testDf, setsMostFreqCatDict) # df gets cached in function\n",
    "\n",
    "# # numerically index categorical columns for one-hot encoder and to combine rare categories into one\n",
    "# for catColumn in trainDf.columns[NUMERICCOLS+1:]:\n",
    "#     catIndexer = StringIndexer(inputCol=catColumn, outputCol=catColumn+'Index', handleInvalid='error') # forces you to have different in & out col names\n",
    "#     stringIndexerModel = catIndexer.fit(trainDf)\n",
    "#     trainDf = stringIndexerModel.transform(trainDf).drop(catColumn).cache() # original string columns are kept in dataframe so should be deleted\n",
    "#     testDf = stringIndexerModel.transform(testDf).drop(catColumn).cache()\n",
    "    \n",
    "# # convert categorical columns to 1 hot encoded columns\n",
    "# indexColumnNames = trainDf.columns[NUMERICCOLS+1:]\n",
    "# oneHotColumnNames = [column.replace('Index', '') for column in trainDf.columns[NUMERICCOLS+1:]]\n",
    "# oneHotEncoder = OneHotEncoderEstimator(inputCols=indexColumnNames, outputCols=oneHotColumnNames) # forces you to have different in & out column names\n",
    "# oneHotModel = oneHotEncoder.fit(trainDf)                                                                        \n",
    "# trainDf = oneHotModel.transform(trainDf).cache()\n",
    "# testDf = oneHotModel.transform(testDf).cache()\n",
    "\n",
    "# # drop the index columns (original string columns are kept in dataframe so should be deleted)\n",
    "# for column in indexColumnNames:\n",
    "#     trainDf = trainDf.drop(column) \n",
    "#     testDf = testDf.drop(column)\n",
    "\n",
    "# # convert SparseVectors to 1D arrays in order to convert dataframe to RDD    \n",
    "# for column in trainDf.columns[NUMERICCOLS+1:]:\n",
    "#     trainDf = trainDf.withColumn(column, udf(lambda x: list(OrderedDict((y, None) for y in x)))(trainDf[column])).cache()\n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = trainDf.rdd.map(dfToRDD).cache()\n",
    "testRDD = testDf.rdd.map(dfToRDD).cache()\n",
    "        \n",
    "# create and broadcast reference dictionary to be used in constructing 1 hot encoded RDD\n",
    "oneHotReference = trainRDD.flatMap(emitColumnAndCat) \\\n",
    "                          .reduceByKeyLocally(add) # note: only the zero values are being added here (main goal is to output a dictionary)\n",
    "sc.broadcast(oneHotReference)\n",
    "\n",
    "# replace rows with new rows having categorical columns 1-hot encoded\n",
    "trainRDD1Hot = trainRDD.map(oneHotEncoder)\n",
    "\n",
    "\n",
    "print(trainRDD1Hot.takeSample(False, 5, SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-08 01:31:09 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-08 01:31:20 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "[(['1', '31', '17', '12', '2826', '351', '30', '13', '138', '1', '4', '0', '12', 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '751', '4', '4', '7788', '45', '3', '2', '75', '1', '3', '0', '4', 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['0', '1', '1', '22', '1910', '200', '5', '25', '149', '0', '1', '0', '29', 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '2', '29', '12', '17435', '1365', '1', '14', '191', '1', '1', '0', '14', 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '184', '4', '4', '2715', '31', '0', '4', '4', '1', '0', '0', '4', 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0)]\n"
     ]
    }
   ],
   "source": [
    "!python featureEngineering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ImportError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "Uploading pyspark file to GCS\n",
      "featengcluster - RUNNING\n",
      "Submitted job ID 313fe53a-83bd-43af-a06e-8fb37052ba94\n",
      "Waiting for job to finish...\n",
      "Job finished.\n",
      "Downloading output file\n",
      "Received job output b\"Ivy Default Cache set to: /root/.ivy2/cache\\nThe jars for the packages stored in: /root/.ivy2/jars\\n:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\\ncom.databricks#spark-xml_2.11 added as a dependency\\ngraphframes#graphframes added as a dependency\\ncom.databricks#spark-avro_2.11 added as a dependency\\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\\n\\tconfs: [default]\\n\\tfound com.databricks#spark-xml_2.11;0.4.1 in central\\n\\tfound graphframes#graphframes;0.5.0-spark2.1-s_2.11 in spark-packages\\n\\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\\n\\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\\n\\tfound org.scala-lang#scala-reflect;2.11.0 in central\\n\\tfound org.slf4j#slf4j-api;1.7.7 in central\\n\\tfound com.databricks#spark-avro_2.11;4.0.0 in central\\n\\tfound org.apache.avro#avro;1.7.6 in central\\n\\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\\n\\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\\n\\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\\n\\tfound org.xerial.snappy#snappy-java;1.0.5 in central\\n\\tfound org.apache.commons#commons-compress;1.4.1 in central\\n\\tfound org.tukaani#xz;1.0 in central\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.4.1/spark-xml_2.11-0.4.1.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-xml_2.11;0.4.1!spark-xml_2.11.jar (46ms)\\ndownloading http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar ...\\n\\t[SUCCESSFUL ] graphframes#graphframes;0.5.0-spark2.1-s_2.11!graphframes.jar (152ms)\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-avro_2.11;4.0.0!spark-avro_2.11.jar (24ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2!scala-logging-api_2.11.jar (13ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2!scala-logging-slf4j_2.11.jar (14ms)\\ndownloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.0/scala-reflect-2.11.0.jar ...\\n\\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.0!scala-reflect.jar (287ms)\\ndownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar ...\\n\\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.7!slf4j-api.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.6/avro-1.7.6.jar ...\\n\\t[SUCCESSFUL ] org.apache.avro#avro;1.7.6!avro.jar(bundle) (25ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (18ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (36ms)\\ndownloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\\n\\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (13ms)\\ndownloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar ...\\n\\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.0.5!snappy-java.jar(bundle) (46ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\\n\\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (15ms)\\ndownloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\\n\\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (14ms)\\n:: resolution report :: resolve 3288ms :: artifacts dl 724ms\\n\\t:: modules in use:\\n\\tcom.databricks#spark-avro_2.11;4.0.0 from central in [default]\\n\\tcom.databricks#spark-xml_2.11;0.4.1 from central in [default]\\n\\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\\n\\tgraphframes#graphframes;0.5.0-spark2.1-s_2.11 from spark-packages in [default]\\n\\torg.apache.avro#avro;1.7.6 from central in [default]\\n\\torg.apache.commons#commons-compress;1.4.1 from central in [default]\\n\\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\\n\\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\\n\\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\\n\\torg.slf4j#slf4j-api;1.7.7 from central in [default]\\n\\torg.tukaani#xz;1.0 from central in [default]\\n\\torg.xerial.snappy#snappy-java;1.0.5 from central in [default]\\n\\t:: evicted modules:\\n\\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\torg.slf4j#slf4j-api;1.6.4 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\t---------------------------------------------------------------------\\n\\t|                  |            modules            ||   artifacts   |\\n\\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\\n\\t---------------------------------------------------------------------\\n\\t|      default     |   16  |   14  |   14  |   2   ||   14  |   14  |\\n\\t---------------------------------------------------------------------\\n:: retrieving :: org.apache.spark#spark-submit-parent\\n\\tconfs: [default]\\n\\t14 artifacts copied, 0 already retrieved (7998kB/49ms)\\n18/12/05 21:35:22 INFO org.spark_project.jetty.util.log: Logging initialized @7414ms\\n18/12/05 21:35:22 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT\\n18/12/05 21:35:22 INFO org.spark_project.jetty.server.Server: Started @7503ms\\n18/12/05 21:35:22 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@3841473b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n18/12/05 21:35:23 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.10-hadoop2\\n18/12/05 21:35:24 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at featengcluster-m/10.128.0.2:8032\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.databricks_spark-xml_2.11-0.4.1.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.tukaani_xz-1.0.jar added multiple times to distributed cache.\\n18/12/05 21:35:27 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1544045561257_0001\\n18/12/05 21:35:46 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\\n\\r[Stage 1:>                                                          (0 + 2) / 2]\\r[Stage 1:=============================>                             (1 + 1) / 2]\\r[Stage 2:=========>                                              (35 + 7) / 200]\\r[Stage 2:===================>                                    (69 + 6) / 200]\\r[Stage 2:============================>                          (105 + 6) / 200]\\r[Stage 2:================================>                      (118 + 7) / 200]\\r[Stage 2:=================================================>     (181 + 6) / 200]\\r[Stage 2:======================================================>(198 + 2) / 200]\\r                                                                                \\r\\r[Stage 4:============================>                          (103 + 6) / 200]\\r[Stage 4:========================================>              (148 + 6) / 200]\\r[Stage 4:===================================================>   (189 + 6) / 200]\\r[Stage 4:======================================================>(198 + 2) / 200]\\r[Stage 4:======================================================>(199 + 1) / 200]\\r                                                                                \\r\\r[Stage 6:====================================================>  (190 + 7) / 200]\\r                                                                                \\r[Row(Label=0, I0=1, I1=-2, I2=1, I3=0, I4=14, I5=31, I6=0, I7=16, I8=17, I9=1, I10=0, I11=0, I12=0, C0='3c9d8785', C1='207b2d81', C2='rare', C3='rare', C4='30903e74', C5='fe6b92e5', C6='rare', C7='0b153874', C8='a73ee510', C9='3b08e48b', C10='rare', C11='rare', C12='rare', C13='b28479f6', C14='rare', C15='rare', C16='1e88c74f', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='c9d4222a', C22='be7c41b4', C23='rare', C24='001f3601', C25='rare'), Row(Label=0, I0=1, I1=-2, I2=1, I3=2, I4=17560, I5=55, I6=22, I7=2, I8=105, I9=1, I10=3, I11=0, I12=2, C0='05db9164', C1='rare', C2='d032c263', C3='c18be181', C4='4cf72387', C5='7e0ccccf', C6='rare', C7='0b153874', C8='a73ee510', C9='rare', C10='rare', C11='6aaba33c', C12='rare', C13='b28479f6', C14='rare', C15='b041b04a', C16='07c540c4', C17='rare', C18='21ddcdc9', C19='5840adea', C20='723b4dfd', C21='ad3062eb', C22='32c7478e', C23='3fdb382b', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-2, I2=2, I3=4, I4=33245, I5=26, I6=0, I7=0, I8=19, I9=1, I10=0, I11=0, I12=4, C0='05db9164', C1='58e67aaf', C2='rare', C3='rare', C4='384874ce', C5='7e0ccccf', C6='rare', C7='0b153874', C8='a73ee510', C9='rare', C10='rare', C11='rare', C12='rare', C13='07d13a8f', C14='10935a85', C15='rare', C16='d4bb7bd8', C17='c21c3e4c', C18='1d1eb838', C19='a458ea53', C20='rare', C21='ad3062eb', C22='55dd3565', C23='rare', C24='9b3e8820', C25='rare')]\\n[Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=7, I8=36, I9=1, I10=1, I11=0, I12=4, C0='5a9ed9b0', C1='38a947a1', C2='d032c263', C3='c18be181', C4='384874ce', C5='7e0ccccf', C6='88002ee1', C7='1f89b562', C8='7cc72ec2', C9='3b08e48b', C10='f1b78ab4', C11='6aaba33c', C12='6e5da64f', C13='64c94865', C14='rare', C15='b041b04a', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='723b4dfd', C21='ad3062eb', C22='55dd3565', C23='3fdb382b', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=7, I8=36, I9=1, I10=1, I11=0, I12=4, C0='5a9ed9b0', C1='38a947a1', C2='rare', C3='rare', C4='25c83c98', C5='7e0ccccf', C6='rare', C7='0b153874', C8='7cc72ec2', C9='3b08e48b', C10='rare', C11='rare', C12='rare', C13='07d13a8f', C14='rare', C15='rare', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='ad3062eb', C22='be7c41b4', C23='rare', C24='001f3601', C25='49d68486'), Row(Label=0, I0=1, I1=-1, I2=6, I3=4, I4=2715, I5=31, I6=3, I7=0, I8=36, I9=1, I10=1, I11=0, I12=4, C0='05db9164', C1='38a947a1', C2='rare', C3='rare', C4='25c83c98', C5='fbad5c96', C6='970f01b2', C7='0b153874', C8='7cc72ec2', C9='3b08e48b', C10='36bccca0', C11='rare', C12='80467802', C13='07d13a8f', C14='rare', C15='rare', C16='2005abd1', C17='rare', C18='21ddcdc9', C19='5840adea', C20='rare', C21='ad3062eb', C22='423fab69', C23='rare', C24='001f3601', C25='49d68486')]\\n18/12/05 21:36:46 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@3841473b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n\"\n",
      "Tearing down cluster\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=featengcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=featureEngineering.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
