{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "W261 Machine Learning at Scale<br>\n",
    "12 December 2018\n",
    "\n",
    "### Team 5\n",
    "Wei Wang\n",
    "Alice Lam\n",
    "John Tabbone\n",
    "Noah Randolph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUBRIC GUIDE ####\n",
    "Formulate the question \n",
    "\n",
    "limitation of the data and algorithm\n",
    "\n",
    "dataset contents and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click-Through-Rate (CTR), which defines as \"the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement\"$^1$, is a key metric that measure online advertisment performance. It demonstrates both 1) how effective the advertising platforms are, and 2) how effective the advertising campaigns are in targeting the right audience. Since the advertising income is highly correlated with CTR, platforms are motivated to improve their CTR to maximize their revenues. The goal of our analysis is to predict CTR, which can be useful to priortize which ad to show whom in order to maximize advertising revenue.\n",
    "\n",
    "Online platforms ranging from Google, Facebook, to casual game apps are essentially \"online real estate\" that draws \"traffic\", i.e. eyeballs on the screen. They can monetize these traffic by charging businesses advertising fee for putting up ads/links on the screen. Traditionally, the fee is charged per impression, hence termed \"CPI - Cost-per-impression\". Advertisers would have a campaign budget and a desired return on investment from the budget, e.g. bringing 1 million people to their website with a $\\$1,000,000$ budget. If the platform's CTR is 10%, the advertisers can only charge up to $\\$0.10$ per impression. If the platform's CTR is 100%, then the maximum CPI could reach $\\$1$. Online advertising model has gradually evolved to pay-for-performance, i.e. advertisers would only pay if the link is being clicked. Regardless of the advertising revneue model, platforms are highly incentivized to improve CTR.\n",
    "\n",
    "Based on the current work in the literature on modeling clicks and CTR, the first challenge is to understand user behaviors. Given the limited opportunity ads can be shown to a specific person at any given time, the platform should present the ads that a person is most likely to click. Understanding browsing and clicking behavior of each individual is thus essential in making CTR prediction for each user. Some of the features that are likely significant for such predictions are: time, day of week, location, gender, age, device they are using, sites they are visiting, sites they came from, topics of the ad, color of the ad, pixel location of the ad on the screen, etc. The data we analyze on was made available by Criteo and contains a portion of Criteo's traffic dataset for 7 days. It has both integer and categorical column of features. However, it is completely anonymiezd which limits us from conducting feature selection or engineering that is backed by contextual understanding.\n",
    "\n",
    "The second challenge is to optimize algorithm speed, which means the prediction can be done in seconds. For example, given the fact that the person is in this location and launched this app at this time of the day, the algorithm should be able to predict the CTR in split second in order to decide which ad to push to the person. Any accurate prediction delivered too late is almost effectively useless. The algorithm speed is hindered by the great amount of traffic volume that comes into the site, as well as the massive amount of data that has numerous categorical variables with high cardinality. We thus leverage Spark to increase the scalability of our analysis. When selecting models, our priority would be speed over performance.\n",
    "\n",
    "Another approach to mitigate the speed challenge is to __not__ include information generated from the users from last few seconds/minutes/hours. This approach may be at the cost of accuracy as well because immediate information such as current location, last article the person look at, etc, can enhance accuracy significantly. This is a compromise the platforms need to evalaute given their specific business needs and infrastructure. We have no information on whether some of the features in the dataset is immediate features that's received a few seconds prior to the display of the ad. We choose to assume the features may contain such information.\n",
    "\n",
    "Throughout this project, we use logistics regression model because as the most prevalent algorithm for solving industry scale problems, it is designed to handle categorical dependent variable (Click vs. Non-Click). Logistics regression has its own drawbacks. As a generalized linear model, it requires transformation for non-linear features. This additional step can slow the process down when the feature space and data volume is too large. However, since the probability score outputs that logistics regression generates are straightforward for observations, and it is not particularly affected by mild cases of multi-collinearity, we decide to combine the power of logistics regression and Spark for our analysis.\n",
    "\n",
    "\n",
    "$^1$ https://en.wikipedia.org/wiki/Click-through_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Algorithm Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUBRIC GUIDE ####\n",
    "\n",
    "math explain clear, not overly techinical\n",
    "\n",
    "toy example is appropriate\n",
    "\n",
    "toy calculation is clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Algorithm Overview\n",
    "\n",
    "##### Motivation\n",
    "\n",
    "Logistic regression starts with a linear classifier $f(x) = w^Tx + b$ and applies a sigmoid activation function $\\sigma$ such that:\n",
    "\n",
    "$$\\sigma(f(x_{i})) =\\begin{cases}\n",
    "+1 & x_{i}\\ge .5\\\\\n",
    "-1 & x_{i}<.5\n",
    "\\end{cases} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\sigma(f(x))=\\frac{1}{1+e^{-f(x)}}\n",
    "$$\n",
    "\n",
    "This will 'binarize' the output appropriate to requirements.  \n",
    "\n",
    "##### Loss Function\n",
    "To create an accurate model that can estimate the probability of click event occuring $P(y=1|X)$ given training data X, we need to minimize cost function. \n",
    "\n",
    "$$\n",
    "P(y=1|x) = \\sigma(f(x)) = \\frac{1}{1+e^{-f(x)}}\\\\  \n",
    "P(y=-1|x) = 1 - \\sigma(f(x)) = \\frac{1}{1+e^{f(x)}}  \\\\   \n",
    "P(y_{i}|x_{i}) = \\frac{1}{1+e^{-y_{i}f(x_{i})}}    \\\\\n",
    "$$\n",
    "\n",
    "The likelihood is the combined product of all these probabilities\n",
    "\n",
    "$$\n",
    "\\prod_{i}^{n}\\frac{1}{1+e^{-y_{i}f(x_{i})}}\n",
    "$$\n",
    "\n",
    "We use the negative log liklehood as the loss function:\n",
    "\n",
    "$$\n",
    "\\sum_{i}^{n} \\log(1+e^{-y_{i}f(x_{i})})\\\\\n",
    "$$\n",
    "\n",
    "This is expressed in code in the logLoss() function\n",
    "\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "                        \n",
    "where<br> \n",
    "$y_{i} = p[1]$, and <br>\n",
    "$f(x) = np.dot(W, p[0])$  \n",
    "\n",
    "##### Gradient Descent\n",
    "We use gradient descent to find optimal parameters to minimize the loss function. We find the gradient of the log loss function as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla w = \\sum_{i}^{n} -y\\left(1- \\frac{1}{1+e^{-y_{i}f(x_{i})}}\\right)\\cdot x_{i}\n",
    "$$\n",
    "\n",
    "\n",
    "Again we can see this formula represented in the code of gdupdate() in the line:\n",
    "\n",
    "grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0]))\n",
    "\n",
    "where<br> \n",
    "$y_{i} = p[1]$,<br>\n",
    "$f(x) = np.dot(W, p[0])$, and<br> \n",
    "$x_{i} = p[0]$\n",
    "\n",
    "##### Iterate\n",
    "Below, the toy implementation then initializes the first gradient with a random guess.  It will iterate 5 times over the data, upddating the gradient and displaying the error.\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Dataset Illustration\n",
    "\n",
    "To illustrate our process of implementing a logistic regression model, we create a toy dataset that contains 8 rows of randomly generated data, one categorical dependent variable and four features, to mimic the Criteo dataset. In our toy dataset, dependent values are either 1 or -1, which represent the situation of click vs. non-click. Among the four features, we randomly generate integer values range from 0 to 10 for two numeric feature columns, and randomly picked either 0 or 1 for two categorical feature columns. \n",
    "\n",
    "While following the above steps, there are two additional modifications that we make in order to improve model accuracy. The first modification is normalizing feature values, which is not neccessary for the toy dataset but is essential for the entire dataset where high data variance occurs. The formula for normalization is:\n",
    "\\begin{equation}\\\n",
    "x_n= (x - \\mu)/\\sigma\n",
    "\\end{equation}\n",
    "where $x_n$ denotes the normalized $x$, $\\mu$ denotes the mean of $x$, $\\sigma$ denotes the standard deviation of $x$.\n",
    "<br>\n",
    "\n",
    "The second modification is to add a bias term to the initial weights that we randomly generated, which eliminates the hassle of multiplying the data point by the weights and then adding the bias.\n",
    "\n",
    "One additional note is that when predicting on the entire dataset, we convert categorical data using one-hot encoding. This data manipulation process is not shown in this simplified toy dataset illustration.\n",
    "\n",
    "Our modeling design takes scalability into consideration. We leverage Spark dataframes and RDDs to make the process scale to a larger dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting toyDataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile toyDataset.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Set the seed for random numbers\n",
    "SEED = 2615\n",
    "\n",
    "# Number of numeric columns\n",
    "NUMERIC_COLS = 2\n",
    "# Number of categorical columns (one hot encoded)\n",
    "ONE_HOT_COLS = 2\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# Creates a toy dataset in the form\n",
    "# LABEL,INT,INT,CAT,CAT\n",
    "# where CAT is categorical data that is represented as one hot encoded\n",
    "# (i.e. 0 or 1)\n",
    "#\n",
    "# w:  a list of weights \n",
    "# nrows:  The number of rows to produce\n",
    "def generateToyDataset(w=[8, -3, -1, 3, 8],nrows = 8):\n",
    "    '''generate toy logistic regression dataset with numerical and 1-hot encoded features'''\n",
    "        \n",
    "    # set random number generator\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    # These two x vectors represent numeric data\n",
    "    x1 = np.random.randint(0, 10, nrows)\n",
    "    x2 = np.random.randint(0, 10, nrows)\n",
    "    \n",
    "    # These two represent categorical data that has been\n",
    "    # one hot encoded\n",
    "    x3 = np.random.randint(0, 2, nrows) \n",
    "    x4 = np.ones(nrows, np.int8) - x3 \n",
    "    \n",
    "    # Create an error term for linear function\n",
    "    noise = np.random.normal(5, 1, nrows)\n",
    "    \n",
    "    # Create linear function to determine labels\n",
    "    v = (w[0] + x1*w[1] + x2*w[2] + x3*w[3] + x4*w[4] + noise)\n",
    "    \n",
    "    # Activation function v>0 to determine binary labels 1 and -1\n",
    "    y = (v>0) * 2 - 1 \n",
    "    \n",
    "    # Assemble vectors into single matrix structure\n",
    "    # NB:  This technique works to assemble the toy dataset but would \n",
    "    # be cost prohibitive to perform on a larger dataset\n",
    "    df = spark.createDataFrame(zip(y.tolist(), x1.tolist(), x2.tolist(), x3.tolist(), x4.tolist()))\n",
    " \n",
    "    # Rename columns from default\n",
    "    # c1,c2,c3 to human readable\n",
    "    # Label,I1,I2,C1,C2\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,NUMERIC_COLS)]+['C{}'.format(i) for i in range(0,ONE_HOT_COLS)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    return df\n",
    "\n",
    "# Utility function to change format of RDD\n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    # Create matrix structure of numeric and catageory features\n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERIC_COLS)] + [row['C{}'.format(i)] for i in range(0, ONE_HOT_COLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    # extract labels\n",
    "    y = row['Label']\n",
    "    \n",
    "    #return features_array (matrix) paired with label vector\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "# Given dataRDD, will return an rdd with standardized column values.\n",
    "#  This will transform each feature into a set of values whose mean\n",
    "# converges on 0 and who's standard deviation converges on 1\n",
    "def normalize(dataRDD):\n",
    "    # Take the mean of each column\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    # Take standard deviation of each column\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    # Standardize the features by calculating the difference between \n",
    "    # the actual and the mean divided by the standard deviation.  \n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStdev, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0])) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    new_model = W - learningRate * grad\n",
    "    return new_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# create a toy dataset that includes 1-hot columns for development\n",
    "df = generateToyDataset()   \n",
    "\n",
    "# Create training data set by converting dataframe to RDD.  \n",
    "# Seperates label column from feature matrix and returns at tupple\n",
    "# e.g. (features,labels)\n",
    "trainRDD = df.rdd.map(dfToRDD)\n",
    "\n",
    "# normalize RDD and cache\n",
    "normedRDDcached = normalize(trainRDD).cache()\n",
    "print(normedRDDcached.take(1))\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(normedRDDcached.take(1)[0][0])\n",
    "\n",
    "wInitial = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# 1 iteration of gradient descent with initial\n",
    "# random values\n",
    "w = GDUpdate(normedRDDcached, wInitial)\n",
    "\n",
    "# Iterate\n",
    "nSteps = 5\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    w = GDUpdate(normedRDDcached, w)\n",
    "    loss = logLoss(normedRDDcached, w)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(i,3) for i in w]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-10 08:21:13 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[(array([-1.63525964,  0.62123652,  1.        , -1.        ]), 1)]\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 7.7014879401802405\n",
      "Model: [1.101, -0.162, -1.391, -0.468, -0.797]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 6.1460241873746195\n",
      "Model: [0.865, -0.478, -1.282, -0.469, -0.795]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 5.006600355698076\n",
      "Model: [0.66, -0.743, -1.182, -0.491, -0.774]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 4.183106733189442\n",
      "Model: [0.485, -0.964, -1.092, -0.52, -0.744]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 3.591671886745936\n",
      "Model: [0.337, -1.15, -1.016, -0.55, -0.714]\n"
     ]
    }
   ],
   "source": [
    "!python toyDataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUBRIC GUIDE ####\n",
    "\n",
    "EDA well choosen and well explained\n",
    "\n",
    "Code is scalable and well commented\n",
    "\n",
    "Written discussion connects the EDA to the algorithm/potential challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of our EDA is to exaimne all features so that only variables that display linear relationship with the outcome variable and that are not highly correlated with each other are choosen. In addition to this, we will also leverage the power of large data sets before making conclusions on final feature selection. In order to achieve the above purpose, we designed three steps for EDA proecss. \n",
    "\n",
    "Step 1: Get basic statistics of each feature. \n",
    "\n",
    "Step 2: Check distributions of features.\n",
    "\n",
    "Step 3: Examine inter-correlations between numeric features and correlation between numeric features vs. dependent variable.\n",
    "\n",
    "The results illustrate that among 13 numeric features, some have larger scale and more variance than other variables. For example, variable I4 has much larger mean and max value. This finding confirms the need of using normalization across the variables. In the mean time, out of 26 categorical variables, some have a great amount of distinct values, which post a challenge for us to incorporate a huge amount of features after one hot encoding transformation. Besides, since the data quality isn't the most ideal and many values are missing, we will also need to consider missing value imputation in the feature engineering step. \n",
    "\n",
    "HIgh inter-correlation also exists among some numeric features. For instance, variable I3 is highly correlated with I12, variable I6 is highly correlated with I10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing loadAndEDA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loadAndEDA.py\n",
    "#!/usr/bin/env python\n",
    "import subprocess\n",
    "\n",
    "subprocess.call([\"pip\",\"install\",\"seaborn\"])\n",
    "\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import udf, col, countDistinct, isnan, when, count, desc\n",
    "import pandas as pd\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "MINICLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "MINILOCALPATH = 'data/train_005.txt'\n",
    "\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; MINICLOUDPATH; MINILOCALPATH\n",
    "    df = spark.read.csv(path=MINILOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,13)]+['C{}'.format(i) for i in range(0,26)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''returns approximate median values of the columns given, with null values ignored'''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "def getDescribe(df, cols):\n",
    "    return df.select(cols).describe().show()\n",
    "\n",
    "def getDistinctCount(df, cols):\n",
    "    return df.agg(*(countDistinct(col(c)).alias(c) for c in cols)).show()\n",
    "\n",
    "def checkNA(df, cols):\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()\n",
    "\n",
    "def getTopCountsValues(df, n, cols):\n",
    "    topCounts_dict= {key: value for (key, value) in zip(cols, \n",
    "                                        [[x[1] for x in df.groupBy(c).count().sort(desc(\"count\")).head(n)] \\\n",
    "                                         for c in cols])}\n",
    "    return topCounts_dict\n",
    "\n",
    "def plotHist(df):\n",
    "    '''plot histogram of numeric features'''\n",
    "    df.hist(figsize=(15,15), bins=15)\n",
    "    return plt.show()\n",
    "\n",
    "def CorrMatrix(df):\n",
    "    '''get correlation matrix of numeric features'''\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(11, 9))\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=.5)\n",
    "    plt.title(\"Correlations between numerical features.\")\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "df = loadData().cache()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "print(\"\\nTEST DATASET ROW COUNTS: \", testDf.count())\n",
    "print(\"\\nTRAIN DATASET ROW COUNTS: \", trainDf.count())\n",
    "print(\"\\nCOLUMN TYPES\\n\", df.dtypes)\n",
    "print(\"\\nMEDIAN OF NUMERIC COLUMNS\\n\", getMedians(trainDf, trainDf.columns[1:14]))\n",
    "\n",
    "print(\"\\nDESCRIPTIONS OF NUMERICAL COLUMNS\")\n",
    "getDescribe(trainDf, trainDf.columns[1:8])\n",
    "getDescribe(trainDf, trainDf.columns[8:14])\n",
    "\n",
    "print(\"\\nCOUNTS OF NAs\")\n",
    "checkNA(trainDf, trainDf.columns[:20])\n",
    "checkNA(trainDf, trainDf.columns[20:])\n",
    "\n",
    "print(\"\\nCOUNTS OF DISTINCT VALUE FOR CATEGORICAL VARIABLE COLUMNS\")\n",
    "getDistinctCount(trainDf, trainDf.columns[15:])\n",
    "\n",
    "print(\"\\nOCCURENCE COUNT OF TOP 3 MOST FREQUENT VALUES FOR EACH VARIABLE\")\n",
    "count_n = 3 # Max can only be 3 because one column (c8) has only 3 categorical values\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[1:12])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[12:23])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[23:34])))\n",
    "print(\"\\n\")\n",
    "print (pd.DataFrame(getTopCountsValues(trainDf, count_n, trainDf.columns[34:])))\n",
    "\n",
    "pandaTrain =trainDf.toPandas()\n",
    "print(\"\\nHistograms for Numeric Values\")\n",
    "plotHist(pandaTrain)\n",
    "print(\"\\nCorrelation Matrix between Numeric Values\")\n",
    "CorrMatrix(pandaTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/anaconda/lib/python3.6/site-packages (0.9.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/anaconda/lib/python3.6/site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.3 in /opt/anaconda/lib/python3.6/site-packages (from seaborn) (1.15.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in /opt/anaconda/lib/python3.6/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /opt/anaconda/lib/python3.6/site-packages (from seaborn) (0.23.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.7.3)\n",
      "Requirement already satisfied: pytz in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2018.5)\n",
      "Requirement already satisfied: six>=1.10 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (39.2.0)\n",
      "\u001b[31mdistributed 1.22.0 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "2018-12-10 08:18:53 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-10 08:19:05 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "                                                                                \n",
      "TEST DATASET ROW COUNTS:  4578\n",
      "\n",
      "TRAIN DATASET ROW COUNTS:  18379\n",
      "\n",
      "COLUMN TYPES\n",
      " [('Label', 'int'), ('I0', 'int'), ('I1', 'int'), ('I2', 'int'), ('I3', 'int'), ('I4', 'int'), ('I5', 'int'), ('I6', 'int'), ('I7', 'int'), ('I8', 'int'), ('I9', 'int'), ('I10', 'int'), ('I11', 'int'), ('I12', 'int'), ('C0', 'string'), ('C1', 'string'), ('C2', 'string'), ('C3', 'string'), ('C4', 'string'), ('C5', 'string'), ('C6', 'string'), ('C7', 'string'), ('C8', 'string'), ('C9', 'string'), ('C10', 'string'), ('C11', 'string'), ('C12', 'string'), ('C13', 'string'), ('C14', 'string'), ('C15', 'string'), ('C16', 'string'), ('C17', 'string'), ('C18', 'string'), ('C19', 'string'), ('C20', 'string'), ('C21', 'string'), ('C22', 'string'), ('C23', 'string'), ('C24', 'string'), ('C25', 'string')]\n",
      "\n",
      "MEDIAN OF NUMERIC COLUMNS\n",
      " [[1.0], [2.0], [6.0], [4.0], [2715.0], [31.0], [3.0], [7.0], [36.0], [1.0], [1.0], [0.0], [4.0]]\n",
      "\n",
      "DESCRIPTIONS OF NUMERICAL COLUMNS\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|                I0|                I1|                I2|               I3|               I4|                I5|               I6|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|             10109|             18379|             14386|            14314|            17905|             14191|            17600|\n",
      "|   mean|3.5502027895934316|101.80967408455302| 25.63763381064924| 7.30068464440408|18436.65942474169|111.70875907265167|17.12034090909091|\n",
      "| stddev| 9.293289659497264|374.02991892400274|331.46919953261744|8.624079890068804|68931.50353591116|332.34996893862655|66.91093732423182|\n",
      "|    min|                 0|                -2|                 0|                0|                0|                 0|                0|\n",
      "|    max|               292|              6901|             31814|              186|          1585026|             21658|             2802|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|                I7|                I8|                I9|              I10|               I11|               I12|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             18369|             17600|             10109|            17600|              4282|             14314|\n",
      "|   mean|12.419293374707387|106.89028409090909|0.6220199821940845|2.790738636363636|0.8895375992526856| 8.168366634064553|\n",
      "| stddev|14.088934412061754|227.30410625706682|0.6948805353765156|5.310155836571858|3.8061148775636693|11.428526114043635|\n",
      "|    min|                 0|                 0|                 0|                0|                 0|                 0|\n",
      "|    max|               626|              7501|                 7|               91|                86|               226|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "\n",
      "\n",
      "COUNTS OF NAs\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "|Label|  I0| I1|  I2|  I3| I4|  I5| I6| I7| I8|  I9|I10|  I11| I12| C0| C1| C2| C3| C4|  C5|\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "|    0|8270|  0|3993|4065|474|4188|779| 10|779|8270|779|14097|4065|  0|  0|653|653|  0|2260|\n",
      "+-----+----+---+----+----+---+----+---+---+---+----+---+-----+----+---+---+---+---+---+----+\n",
      "\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "| C6| C7| C8| C9|C10|C11|C12|C13|C14|C15|C16|C17| C18| C19|C20|  C21|C22|C23| C24| C25|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "|  0|  0|  0|  0|  0|653|  0|  0|  0|653|  0|  0|8082|8082|653|13890|  0|653|8082|8082|\n",
      "+---+---+---+---+---+---+---+---+---+---+---+---+----+----+---+-----+---+---+----+----+\n",
      "\n",
      "\n",
      "COUNTS OF DISTINCT VALUE FOR CATEGORICAL VARIABLE COLUMNS\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "| C1|   C2|  C3| C4| C5|  C6| C7| C8|  C9| C10| C11| C12|C13| C14| C15|C16| C17|C18|C19| C20|C21|C22| C23|C24| C25|\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "|435|10328|7028| 78|  9|4533|131|  3|4812|2676|9670|2172| 25|2850|8588|  9|1564|697|  3|9201|  9| 14|4099| 44|3140|\n",
      "+---+-----+----+---+---+----+---+---+----+----+----+----+---+----+----+---+----+---+---+----+---+---+----+---+----+\n",
      "\n",
      "\n",
      "OCCURENCE COUNT OF TOP 3 MOST FREQUENT VALUES FOR EACH VARIABLE\n",
      "     I0    I1    I2    I3   I4    I5    I6    I7   I8    I9   I10\n",
      "0  8270  3175  3993  4065  474  4188  4016  2165  779  8270  6062\n",
      "1  4288  2832  2342  2298  430   965  2363  1412  657  4824  4165\n",
      "2  1805  1926  1564  1885  337   493  1578  1340  587  4478  2571\n",
      "\n",
      "\n",
      "     I11   I12    C0    C1   C2   C3     C4    C5   C6     C7     C8\n",
      "0  14097  4065  9262  2175  653  653  12304  7250  403  10909  16559\n",
      "1   3216  2267  3033   756  424  621   2934  4029  236   3071   1815\n",
      "2    618  1804  1575   712  205  430   1179  3391  167   1401      5\n",
      "\n",
      "\n",
      "     C9  C10  C11  C12   C13  C14  C15   C16  C17   C18   C19\n",
      "0  4134  609  653  609  6392  292  653  8386  598  8082  8082\n",
      "1   270  390  430  447  6373  188  430  2448  532  6355  3496\n",
      "2   129  274  424  291  2806  173  424  2103  495   352  3478\n",
      "\n",
      "\n",
      "   C20    C21   C22  C23   C24   C25\n",
      "0  653  13890  8102  923  8082  8082\n",
      "1  430   2596  3607  858  2554   744\n",
      "2  424   1557  2248  813  2038   334\n",
      "                                                                                \n",
      "Histograms for Numeric Values\n",
      "Figure(1500x1500)\n",
      "\n",
      "Correlation Matrix between Numeric Values\n",
      "Figure(1100x900)\n"
     ]
    }
   ],
   "source": [
    "!python loadAndEDA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'submit_job_to_cluster.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=testcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=row_counts.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from running EDA code above:\n",
    "Main dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 9164811)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 36675806)<br>\n",
    "\n",
    "Toy dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 4578)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 18379)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting algorithmImplementation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile algorithmImplementation.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, desc, isnan, when\n",
    "import numpy as np\n",
    "from operator import add\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "MINICLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "MINILOCALPATH = 'data/train_005.txt'\n",
    "NUMERICCOLS = 13\n",
    "CATEGORICALCOLS = 26\n",
    "NUMERICCOLNAMES = ['I{}'.format(i) for i in range(0,NUMERICCOLS)]\n",
    "CATCOLNAMES = ['C{}'.format(i) for i in range(0,CATEGORICALCOLS)]\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"algorithmImplementation\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; TOYCLOUDPATH; TOYLOCALPATH\n",
    "    df = spark.read.csv(path=MAINCLOUDPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label'] + NUMERICCOLNAMES + CATCOLNAMES\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''\n",
    "    returns approximate median values of the columns given, with null values ignored\n",
    "    '''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "\n",
    "def getMostFrequentCats(df, cols, n):\n",
    "    '''\n",
    "    returns a dict where the key is the column and value is an ordered list\n",
    "    of the top n categories in that column in descending order\n",
    "    '''\n",
    "    freqCatDict = {col: None for col in df.columns[cols:]}\n",
    "    for col in df.columns[cols:]:\n",
    "        listOfRows = df.groupBy(col).count().sort('count', ascending=False).take(n)\n",
    "        topCats = [row[col] for row in listOfRows]\n",
    "        freqCatDict[col] = topCats[:n]\n",
    "    return freqCatDict\n",
    "    \n",
    "\n",
    "def rareReplacer(df, dictOfMostFreqSets):\n",
    "    '''\n",
    "    Iterates through columns and replaces non-Frequent categories with 'rare' string.\n",
    "    '''\n",
    "    for colName in df.columns[NUMERICCOLS+1:]:\n",
    "        bagOfCats = dictOfMostFreqSets[colName]\n",
    "        df = df.withColumn(colName, \n",
    "                           udf(lambda x: 'rare' if x not in bagOfCats else x, \n",
    "                               StringType())(df[colName])).cache()\n",
    "    return df\n",
    "\n",
    "    \n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + \\\n",
    "                        [row['C{}'.format(i)] for i in range(0, CATEGORICALCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def emitColumnAndCat(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits a record for each categorical column value \n",
    "    along with a zero for one-hot encoding. The emitted values will become a \n",
    "    reference dictionary for one-hot encoding in later steps.\n",
    "        Input: (array([features], dtype='<U21'), 0) or (features, label)\n",
    "        Output: ((categorical column, category), 0) or (complex key, value)\n",
    "    The last zero in the output is for initializing one-hot encoding.\n",
    "    \"\"\"\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        yield ((catColName, element), 0)\n",
    "\n",
    "\n",
    "def oneHotEncoder(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits row where categorical columns are replaced\n",
    "    with 1-hot encoded columns.\n",
    "        Input: (numerical and categorical features, label)\n",
    "        Output: (numerical and one-hot encoded categorical features, label)\n",
    "    \"\"\"\n",
    "    oneHotDict = copy.deepcopy(bOneHotReference.value)\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        oneHotDict[(catColName, element)] = 1\n",
    "    numericElements = list(line[0][:NUMERICCOLS])\n",
    "    features = np.array(numericElements + [value for key, value in oneHotDict.items()],\n",
    "                        dtype=np.float)\n",
    "    return (features, line[1])\n",
    "\n",
    "\n",
    "def getMeanAndVar(trainRDD):\n",
    "    \"\"\"\n",
    "    Returns the mean and variance of the training dataset for use in normalizing\n",
    "    future records (e.g. the test set) to be run on model.\n",
    "    \"\"\"\n",
    "    featureMeans = trainRDD.map(lambda x: x[0]).mean()\n",
    "    featureStDevs = np.sqrt(trainRDD.map(lambda x: x[0]).variance())\n",
    "    return featureMeans, featureStDevs\n",
    "    \n",
    "\n",
    "def normalize(dataRDD, featureMeans, featureStDevs):\n",
    "    \"\"\"\n",
    "    Scale and center data around the mean of each feature.\n",
    "    \"\"\"\n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStDevs, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "def dataAugmenter(line):\n",
    "        \"\"\"\n",
    "        Adds a 1 value to the array of feature values for the bias term\n",
    "        \"\"\"\n",
    "        return (np.append([1.0], line[0]), line[1])\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute log loss.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(dataAugmenter)\n",
    "    \n",
    "    # broadcast the weights\n",
    "    bW = sc.broadcast(W)\n",
    "    \n",
    "    def loss(line):\n",
    "        loss = np.log(1 + np.exp(-line[1] * np.dot(bW.value, line[0])))\n",
    "        return loss\n",
    "    \n",
    "    losses = augmentedData.map(lambda line: (loss(line), 1)) \\\n",
    "                          .reduce(lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "    cost = losses[0] / losses[1]\n",
    "    return cost\n",
    "\n",
    "\n",
    "def GDUpdateWithReg(dataRDD, W, learningRate = 0.1, regType = None, regParam = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update with ridge or lasso regularization.\n",
    "    Args:\n",
    "        dataRDD - tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        learningRate - (float) defaults to 0.1\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient\n",
    "    Returns:\n",
    "        model   - (array) updated coefficients, bias still at index 0\n",
    "    \"\"\"\n",
    "    # augmented data\n",
    "    augmentedData = dataRDD.map(dataAugmenter)\n",
    "    \n",
    "    # broadcast the weights\n",
    "    bW = sc.broadcast(W)\n",
    "    \n",
    "    # this gets parallelized\n",
    "    def partialGrad(line):\n",
    "        return (((1 / (1 + np.exp(-1 * np.dot(bW.value, line[0])))) - line[1]) * line[0])\n",
    "    \n",
    "    # reduce to bring it all back together to compute the gradient\n",
    "    weightedLogProbabilities = augmentedData.map(lambda line: (partialGrad(line), 1)) \\\n",
    "                                            .reduce(lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "    \n",
    "    nonRegGrad = weightedLogProbabilities[0]/weightedLogProbabilities[1]\n",
    "    \n",
    "    if regType == 'ridge':\n",
    "        reg = 2*regParam * sum(W[1:])\n",
    "    elif regType == 'lasso':\n",
    "        reg = regParam * sum(W[1:]/np.sign(W[1:]))   \n",
    "    else:\n",
    "        reg = 0\n",
    "    grad = nonRegGrad + reg\n",
    "    \n",
    "    new_model = W - (grad * learningRate)    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "def GradientDescentWithReg(trainRDD, testRDD, wInit, nSteps = 20, learningRate = 0.1,\n",
    "                         regType = None, regParam = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of regularized gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    trainHistory, testHistory, modelHistory = [], [], []\n",
    "    \n",
    "    model = wInit\n",
    "    for idx in range(nSteps):  \n",
    "        # update the model\n",
    "        model = GDUpdateWithReg(trainRDD, model, learningRate, regType, regParam)\n",
    "        trainingLoss = logLoss(trainRDD, model) \n",
    "        testLoss = logLoss(testRDD, model) \n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        trainHistory.append(trainingLoss)\n",
    "        testHistory.append(testLoss)\n",
    "        modelHistory.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {trainingLoss}\")\n",
    "            print(f\"test loss: {testLoss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return trainHistory, testHistory, modelHistory\n",
    "\n",
    "\n",
    "def predictionChecker(line):\n",
    "    \"\"\"\n",
    "    line - tuple of (features array including bias, y)\n",
    "    Takes final model from gradient descent iterations and makes a prediction \n",
    "    on the row of test dataset values.\n",
    "    Returns true positive, false negative, false positive, or true negative\n",
    "    \"\"\"\n",
    "    TP, FN, FP, TN = [0, 0, 0, 0]\n",
    "    predictionProbability = 1/(1 + np.exp(-1 * np.dot(bModel.value, line[0])))\n",
    "    if predictionProbability >= 0.5:\n",
    "        prediction = 1\n",
    "    else:\n",
    "        prediction = 0\n",
    "    if prediction == 1 and line[1] == 1:\n",
    "        TP = 1\n",
    "    elif prediction == 0 and line[1] == 1:\n",
    "        FN = 1\n",
    "    elif prediction == 1 and line[1] == 0:\n",
    "        FP = 1\n",
    "    elif prediction == 0 and line[1] == 0:\n",
    "        TN = 1\n",
    "    return (TP, FN, FP, TN)\n",
    "\n",
    "\n",
    "# load data\n",
    "df = loadData()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "\n",
    "# get top n most frequent categories for each column (in training set only)\n",
    "n = 3\n",
    "mostFreqCatDict = getMostFrequentCats(trainDf, NUMERICCOLS+1, n)\n",
    "\n",
    "# get dict of sets of most frequent categories in each column for fast lookups during \n",
    "# filtering (in later code)\n",
    "setsMostFreqCatDict = {key: set(value) for key, \n",
    "                       value in mostFreqCatDict.items()}\n",
    "\n",
    "# get the top category from each column for imputation of missing values \n",
    "# (in training set only)\n",
    "fillNADictCat = {key: (value[0] if value[0] is not None else value[1]) for key, \n",
    "                 value in mostFreqCatDict.items()}\n",
    "\n",
    "# get dict of median numeric values for imputation of missing values (in training set only)\n",
    "fillNADictNum = {key: value for (key, \n",
    "                                 value) in zip(trainDf.columns[1:NUMERICCOLS+1], \n",
    "                                                    [x[0] for x in getMedians(trainDf,\n",
    "                                                                              trainDf.columns[1:NUMERICCOLS+1])])}\n",
    "\n",
    "# impute missing values in training and test set\n",
    "trainDf = trainDf.na.fill(fillNADictNum) \\\n",
    "                 .na.fill(fillNADictCat)\n",
    "testDf = testDf.na.fill(fillNADictNum) \\\n",
    "               .na.fill(fillNADictCat)\n",
    "\n",
    "# replace low-frequency categories with 'rare' string in training and test set\n",
    "trainDf = rareReplacer(trainDf, setsMostFreqCatDict) # df gets cached in function\n",
    "testDf = rareReplacer(testDf, setsMostFreqCatDict) # df gets cached in function\n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = trainDf.rdd.map(dfToRDD).cache()\n",
    "testRDD = testDf.rdd.map(dfToRDD).cache()\n",
    "        \n",
    "# create and broadcast reference dictionary to be used in constructing 1 hot encoded RDD\n",
    "oneHotReference = trainRDD.flatMap(emitColumnAndCat) \\\n",
    "                          .reduceByKeyLocally(add) # note: only the zero values are being added here \n",
    "                                                   # (main goal is to output a dictionary)\n",
    "bOneHotReference = sc.broadcast(oneHotReference)\n",
    "\n",
    "# replace rows with new rows having categorical columns 1-hot encoded\n",
    "trainRDD = trainRDD.map(oneHotEncoder).cache()\n",
    "testRDD = testRDD.map(oneHotEncoder).cache()\n",
    "\n",
    "# normalize RDD\n",
    "featureMeans, featureStDevs = getMeanAndVar(trainRDD)\n",
    "trainRDD = normalize(trainRDD, featureMeans, featureStDevs).cache()\n",
    "testRDD = normalize(testRDD, featureMeans, featureStDevs).cache() # use the mean and st. dev. from trainRDD\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(trainRDD.take(1)[0][0])\n",
    "wInit = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# run training iterations\n",
    "start = time.time()\n",
    "logLossTrain, logLossTest, models = GradientDescentWithReg(trainRDD, testRDD, wInit, nSteps=500, \n",
    "                                                           learningRate = 0.1,\n",
    "                                                           regType=\"ridge\", regParam=0.001)\n",
    "\n",
    "# get model accuracy, precision, recall, f1 score\n",
    "bModel = sc.broadcast(models[-1])\n",
    "predictionResults = testRDD.map(dataAugmenter) \\\n",
    "                           .map(predictionChecker) \\\n",
    "                           .reduce(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3]))\n",
    "TP = predictionResults[0]\n",
    "FN = predictionResults[1]\n",
    "FP = predictionResults[2]\n",
    "TN = predictionResults[3]\n",
    "accuracy = (TP+TN)/(TP+FN+FP+TN)\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "f1Score = 2/((1/recall)+(1/precision))\n",
    "\n",
    "print(\"LOG LOSSES OVER TRAINING SET:\")\n",
    "print(logLossTrain)\n",
    "print(\"LOG LOSSES OVER TEST SET:\")\n",
    "print(logLossTest)\n",
    "print(\"FINAL MODEL:\")\n",
    "print(bModel.value)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")\n",
    "print(\"TEST SET ACCURACY:\")\n",
    "print(accuracy)\n",
    "print(\"TEST SET PRECISION:\")\n",
    "print(precision)\n",
    "print(\"TEST SET RECALL:\")\n",
    "print(recall)\n",
    "print(\"TEST SET F1 SCORE:\")\n",
    "print(f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustration of command to run job on Google Cloud Platform cluster\n",
    "\n",
    "`!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=finalprojectcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=algorithmImplementation.py --instance_type=n1-standard-16 --worker_nodes=6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of Google Cloud Platform cluster execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`... trained 500 iterations in 16138.672616243362 seconds`<br>\n",
    "`TEST SET ACCURACY:`<br>\n",
    "`0.7481226873902687`<br>\n",
    "`TEST SET PRECISION:`<br>\n",
    "`0.5290065024278094`<br>\n",
    "`TEST SET RECALL:`<br>\n",
    "`0.1462423298173223`<br>\n",
    "`TEST SET F1 SCORE:`<br>\n",
    "`0.22913965847621373`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that with the logistic regression model trained on the Criteo dataset, an ad impression event predicted to result in a click will be correct around 53% of the time (precision = 0.529). Of all of the ad impressions that result in real clicks, the model will correctly identify around 14.6% of them (recall = 0.146).\n",
    "\n",
    "These metrics are low, in part, due to the imbalanced dataset upon which they were trained. More non-click events were present in the training dataset than click events, leading the model to favor predicting non-click events. This is potentially a cost saver for the advertiser, since only the most determined of ad viewers will be identified as ad clickers by the model, thus leading to less ad expenses while obtaining more user purchases, since you're capturing those who are most likely to buy.\n",
    "\n",
    "Alternatively, with a more balanced dataset, more ad impressions that result in real clicks can be correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training Losses Displayed\n",
    "Below is the trend of log losses as the model was trained over the Criteo dataset using the 3 most common categories from each of the categorical columns. The graph shows that 500 iterations was just about enough to reach the (global) minimum of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotErrorCurves(trainLoss, testLoss, title = None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting.\n",
    "    Args: trainLoss (list of log loss) , testLoss (list of log loss)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "    x = list(range(len(trainLoss)))[1:]\n",
    "    ax.plot(x, trainLoss[1:], 'k--', label='Training Loss')\n",
    "    ax.plot(x, testLoss[1:], 'r--', label='Test Loss')\n",
    "    ax.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Log Loss')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "trainLoss = [1.5787012113535561, 1.5723073156232616, 1.5659893938297644, 1.5597462810329858, 1.5535768364958433, 1.5474799795616592, 1.5414546705752208, 1.535499907911388, 1.5296147223668979, 1.5237981995375063, 1.5180494413162642, 1.5123675682931528, 1.5067517332673732, 1.5012011049296066, 1.4957149157958212, 1.49029244388353, 1.4849329756775673, 1.4796358237412888, 1.4744003216624533, 1.4692258225846728, 1.4641117673045636, 1.45905767333725, 1.4540630020701744, 1.4491272205405494, 1.4442499190608646, 1.4394306262651875, 1.43466872691463, 1.4299637879628884, 1.425315469444809, 1.4207234833851938, 1.4161875520964533, 1.4117074136368781, 1.4072829021079183, 1.4029139015061465, 1.3986002748502713, 1.394341876144385, 1.3901386230464603, 1.3859905227119256, 1.3818976538330765, 1.377860118603312, 1.3738780141456264, 1.3699514453866068, 1.3660805244377978, 1.3622653486088678, 1.3585059938102055, 1.3548025113606015, 1.35115490683714, 1.3475631138782727, 1.3440269768944433, 1.340546249428199, 1.3371205858621449, 1.333749519619518, 1.330432452684038, 1.3271686580177653, 1.3239572796266927, 1.3207973345770896, 1.3176877225451837, 1.3146272395052252, 1.3116145922126958, 1.3086484165888126, 1.3057272938434823, 1.3028497703730249, 1.3000143831241715, 1.2972196746533997, 1.294464200085042, 1.2917465360248452, 1.2890652910325608, 1.2864191124943154, 1.283806690839084, 1.2812267651697151, 1.278678130166966, 1.2761596416649459, 1.2736702192539169, 1.2712088461011752, 1.268774567603475, 1.266366489896782, 1.263983777473093, 1.2616256491819875, 1.2592913736482314, 1.2569802654052487, 1.2546916817630458, 1.252425019902442, 1.2501797137986816, 1.2479552308848327, 1.2457510687024753, 1.2435667517249296, 1.241401828296584, 1.2392558677335201, 1.2371284578778095, 1.2350192032861096, 1.232927723865538, 1.2308536536237547, 1.2287966393714183, 1.2267563393950935, 1.2247324221518714, 1.2227245650414014, 1.2207324533579302, 1.2187557795258908, 1.2167942426099885, 1.2148475479586007, 1.2129154068341177, 1.2109975360066203, 1.2090936574069566, 1.2072034979537896, 1.2053267895715385, 1.2034632692771443, 1.2016126791682047, 1.1997747662458191, 1.1979492821557725, 1.1961359829972957, 1.1943346292952495, 1.192544986138381, 1.1907668234323174, 1.1889999162046223, 1.187244044901439, 1.1854989956259891, 1.1837645602901157, 1.1820405366785878, 1.1803267284537464, 1.1786229451447223, 1.1769290021620165, 1.1752447208530508, 1.1735699285843442, 1.1719044588199732, 1.1702481511725489, 1.1686008514226653, 1.166962411520865, 1.1653326895911833, 1.1637115499454478, 1.1620988631024611, 1.1604945057987366, 1.1588983609799814, 1.1573103177690158, 1.1557302714090367, 1.1541581231810527, 1.1525937802931336, 1.1510371557396348, 1.1494881681306433, 1.14794674149523, 1.1464128050650035, 1.1448862930459014, 1.1433671443848965, 1.1418553025348637, 1.140350715216993, 1.1388533341775007, 1.137363114935626, 1.1358800165219571, 1.1344040012087357, 1.1329350342357751, 1.1314730835361244, 1.1300181194653323, 1.1285701145375209, 1.1271290431708163, 1.1256948814446217, 1.124267606871271, 1.1228471981850798, 1.1214336351518113, 1.1200268984016488, 1.1186269692883921, 1.1172338297769688, 1.1158474623606986, 1.1144678500089589, 1.113094976145225, 1.111728824654641, 1.1103693799197598, 1.109016626882329, 1.1076705511283325, 1.106331138992782, 1.1049983776798555, 1.1036722553930376, 1.1023527614686885, 1.1010398865054825, 1.0997336224806886, 1.0984339628434847, 1.097140902574266, 1.095854438198642, 1.094574567744561, 1.093301290631509, 1.0920346074820058, 1.090774519847337, 1.0895210298422504, 1.0882741396865423, 1.0870338511554891, 1.0858001649451785, 1.0845730799637057, 1.0833525925632426, 1.082138695732606, 1.0809313782730636, 1.0797306239831088, 1.0785364108791258, 1.0773487104792583, 1.076167487176452, 1.0749926977242439, 1.0738242908549671, 1.0726622070453853, 1.07150637843926, 1.0703567289306168, 1.069213174405464, 1.0680756231344417, 1.0669439763037971, 1.0658181286681705, 1.0646979693055596, 1.0635833824528875, 1.0624742483996052, 1.0613704444168626, 1.0602718457005873, 1.0591783263084218, 1.0580897600725396, 1.0570060214728358, 1.0559269864575862, 1.0548525332014234, 1.0537825427930765, 1.0527168998479086, 1.0516554930424147, 1.0505982155700242, 1.0495449655191993, 1.0484956461761872, 1.0474501662561104, 1.0464084400667233, 1.0453703876099765, 1.0443359346267467, 1.0433050125904995, 1.042277558655484, 1.0412535155651685, 1.0402328315262914, 1.0392154600538057, 1.0382013597915007, 1.037190494312908, 1.0361828319066215, 1.0351783453498329, 1.0341770116735014, 1.0331788119221672, 1.0321837309111834, 1.031191756983613, 1.0302028817689013, 1.0292170999450758, 1.028234409005882, 1.0272548090341476, 1.0262783024823148, 1.025304893961016, 1.024334590036217, 1.0233673990354544, 1.022403330863455, 1.0214423968273554, 1.02048460947155, 1.0195299824222475, 1.0185785302415762, 1.0176302682911547, 1.0166852126048795, 1.0157433797706947, 1.0148047868210694, 1.0138694511318607, 1.0129373903292231, 1.012008622204207, 1.0110831646346803, 1.0101610355142259, 1.0092422526876015, 1.0083268338923905, 1.007414796706527, 1.0065061585012485, 1.005600936399206, 1.004699147237299, 1.0038008075339764, 1.002905933460647, 1.0020145408168795, 1.0011266450091334, 1.000242261032722, 0.9993614034567173, 0.9984840864115951, 0.997610323579331, 0.9967401281857855, 0.9958735129950772, 0.9950104903058409, 0.994151071949123, 0.9932952692877719, 0.992443093217129, 0.9915945541669359, 0.9907496621042212, 0.9899084265371295, 0.9890708565195446, 0.9882369606563363, 0.9874067471092468, 0.9865802236032072, 0.9857573974331006, 0.9849382754707825, 0.9841228641724238, 0.9833111695859841, 0.9825031973588481, 0.9816989527455172, 0.980898440615366, 0.9801016654603423, 0.9793086314026758, 0.9785193422024694, 0.9777338012652156, 0.9769520116491638, 0.9761739760725447, 0.9753996969206512, 0.9746291762527116, 0.9738624158085867, 0.9730994170152472, 0.9723401809930851, 0.9715847085619644, 0.9708330002470975, 0.97008505628466, 0.969340876627225, 0.9686004609489685, 0.9678638086506328, 0.9671309188643312, 0.9664017904580753, 0.9656764220401631, 0.9649548119633116, 0.9642369583286242, 0.9635228589893735, 0.9628125115545833, 0.9621059133924303, 0.9614030616335207, 0.9607039531739424, 0.9600085846782135, 0.9593169525820526, 0.9586290530950441, 0.9579448822031206, 0.9572644356709828, 0.9565877090443409, 0.955914697652098, 0.9552453966084038, 0.9545798008146175, 0.9539179049611847, 0.9532597035294409, 0.9526051907933201, 0.9519543608210218, 0.9513072074765972, 0.9506637244214818, 0.9500239051159833, 0.949387742820735, 0.9487552305980829, 0.9481263613134628, 0.9475011276367445, 0.9468795220435496, 0.9462615368165496, 0.9456471640467441, 0.9450363956347574, 0.9444292232920778, 0.9438256385423476, 0.9432256327226293, 0.9426291969846666, 0.9420363222961801, 0.9414469994421631, 0.9408612190261707, 0.9402789714716767, 0.9397002470233952, 0.9391250357486678, 0.9385533275388469, 0.9379851121107231, 0.9374203790079806, 0.9368591176026696, 0.9363013170967314, 0.9357469665235474, 0.9351960547495157, 0.9346485704756896, 0.9341045022394223, 0.9335638384160745, 0.9330265672207589, 0.9324926767101224, 0.9319621547841644, 0.9314349891880951, 0.9309111675142626, 0.9303906772040904, 0.9298735055500807, 0.9293596396978522, 0.9288490666482028, 0.9283417732592736, 0.9278377462486792, 0.927336972195751, 0.9268394375437594, 0.9263451286022472, 0.9258540315493339, 0.9253661324341225, 0.9248814171790999, 0.9243998715826112, 0.9239214813213427, 0.9234462319528707, 0.9229741089182376, 0.9225050975445348, 0.9220391830475659, 0.9215763505345222, 0.9211165850066759, 0.9206598713621236, 0.9202061943985473, 0.919755538816013, 0.9193078892197825, 0.9188632301231524, 0.9184215459503163, 0.9179828210392564, 0.9175470396446387, 0.9171141859407349, 0.9166842440243445, 0.9162571979177634, 0.9158330315717212, 0.9154117288683656, 0.9149932736242199, 0.9145776495931822, 0.9141648404695109, 0.9137548298907878, 0.9133476014409394, 0.9129431386531957, 0.9125414250130955, 0.9121424439614532, 0.9117461788973195, 0.9113526131809844, 0.9109617301368709, 0.9105735130565319, 0.9101879452015377, 0.9098050098064154, 0.9094246900815283, 0.9090469692159551, 0.9086718303803466, 0.9082992567297745, 0.9079292314065199, 0.9075617375428823, 0.9071967582639354, 0.9068342766902516, 0.9064742759406407, 0.9061167391347905, 0.905761649395938, 0.9054089898534728, 0.9050587436455454, 0.9047108939215707, 0.9043654238447874, 0.9040223165947056, 0.9036815553695711, 0.9033431233887489, 0.9030070038951034, 0.9026731801573228, 0.902341635472201, 0.902012353166893, 0.9016853166011108, 0.9013605091692957, 0.9010379143027514, 0.9007175154716932, 0.900399296187333, 0.9000832400038273, 0.8997693305202746, 0.8994575513825885, 0.8991478862853911, 0.8988403189738169, 0.8985348332453076, 0.8982314129513309, 0.897930041999101, 0.8976307043532024, 0.8973333840371982, 0.897038065135212, 0.8967447317934213, 0.8964533682215634, 0.8961639586943445, 0.8958764875528451, 0.8955909392058712, 0.8953072981312502, 0.895025548877121, 0.8947456760631384, 0.8944676643816717, 0.894191498598941, 0.8939171635561394, 0.8936446441704854, 0.8933739254362479, 0.8931049924257615, 0.8928378302903544, 0.8925724242612565, 0.8923087596505244, 0.8920468218518242, 0.8917865963412849, 0.8915280686782405, 0.89127122450598, 0.8910160495524606, 0.8907625296309584, 0.8905106506407134, 0.8902603985675408, 0.8900117594843946, 0.8897647195519149, 0.8895192650189576, 0.8892753822230354, 0.8890330575907976, 0.8887922776384659, 0.8885530289722107, 0.8883152982885201, 0.8880790723745458, 0.8878443381084223, 0.8876110824595467, 0.8873792924888606, 0.8871489553490632, 0.8869200582848479, 0.8866925886330803, 0.8864665338229715, 0.8862418813762256, 0.8860186189071607, 0.8857967341228107, 0.8855762148230015, 0.8853570489004196, 0.8851392243406457, 0.8849227292221805, 0.8847075517164364]\n",
    "testLoss = [1.5770905268111706, 1.5707048292093126, 1.5643949428474166, 1.5581597415561348, 1.5519981262223967, 1.5459090615921895, 1.5398915523508006, 1.533944640067131, 1.5280673705483858, 1.5222588031317197, 1.5165180211414653, 1.5108441507384405, 1.5052363506019897, 1.4996938004055502, 1.494215736892781, 1.4888014318896587, 1.4834501698773155, 1.4781612643335933, 1.4729341160101668, 1.4677680303010756, 1.4626623617460734, 1.4576165919988344, 1.4526301927744532, 1.4477026143718565, 1.442833329939756, 1.4380219073227776, 1.4332678052765297, 1.4285706477488507, 1.4239301557526083, 1.4193460509199824, 1.4148180073783865, 1.4103457180486278, 1.4059289717376389, 1.4015676369953975, 1.3972615805929436, 1.3930106884186315, 1.3888149267770697, 1.3846743572343432, 1.3805891097460112, 1.3765593094708675, 1.3725850365635082, 1.3686663672260289, 1.364803394067879, 1.36099620114023, 1.3572448446651897, 1.3535493523954767, 1.3499097133823934, 1.3463258563339218, 1.3427976288814067, 1.33932479096117, 1.3359070029004196, 1.3325438033397417, 1.3292345905846035, 1.325978616647685, 1.322774993730245, 1.3196227103397795, 1.316520649679794, 1.313467603350082, 1.3104622815233724, 1.307503326836383, 1.3045893276257707, 1.3017188308995395, 1.2988903627753783, 1.2961024504999932, 1.2933536396134406, 1.290642504828308, 1.2879676584157747, 1.285327756543857, 1.2827215015686855, 1.2801476444979403, 1.277604990467153, 1.2750924047910015, 1.2726088160210705, 1.2701532154708182, 1.2677246549304138, 1.2653222435487779, 1.2629451454186913, 1.2605925784369114, 1.2582638124277392, 1.2559581657534458, 1.2536750012691118, 1.2514137222323407, 1.249173768330795, 1.2469546118912438, 1.2447557543243197, 1.2425767227588926, 1.2404170668703645, 1.2382763562242252, 1.2361541784868275, 1.2340501382679774, 1.2319638559535457, 1.2298949662016938, 1.2278431162496077, 1.2258079643052093, 1.2237891782020673, 1.2217864344166218, 1.2197994174838152, 1.217827819725358, 1.2158713410653603, 1.2139296886580992, 1.212002576167331, 1.210089722790976, 1.2081908523578386, 1.2063056928222047, 1.204433976214452, 1.202575438826605, 1.2007298213749147, 1.1988968690424955, 1.1970763314620374, 1.1952679627436502, 1.1934715216072216, 1.1916867716151023, 1.1899134814666636, 1.1881514253078278, 1.186400383011838, 1.1846601404000803, 1.1829304893900987, 1.1812112280747697, 1.1795021607479377, 1.1778030978978247, 1.176113856189334, 1.1744342584488423, 1.1727641336534804, 1.1711033169188263, 1.1694516494789442, 1.1678089786590486, 1.1661751578481734, 1.1645500464814935, 1.162933510036884, 1.1613254200409633, 1.1597256540731153, 1.1581340957558601, 1.156550634724304, 1.1549751665726073, 1.1534075927786687, 1.1518478206089637, 1.1502957630043102, 1.148751338445141, 1.1472144707939138, 1.1456850891129995, 1.144163127458681, 1.1426485246539597, 1.141141224043436, 1.1396411732328653, 1.1381483238150125, 1.136662631083319, 1.1351840537358409, 1.1337125535734272, 1.1322480951972442, 1.1307906457113905, 1.1293401744359783, 1.1278966526354208, 1.1264600532654259, 1.1250303507412138, 1.1236075207283311, 1.122191539956761, 1.1207823860585417, 1.1193800374290295, 1.117984473112196, 1.1165956727106292, 1.1152136163213067, 1.1138382844982322, 1.1124696582428353, 1.1111077190225256, 1.109752448816926, 1.108403830190382, 1.1070618463882063, 1.1057264814530783, 1.104397720356869, 1.1030755491421174, 1.1017599550662547, 1.1004509267405886, 1.0991484542550136, 1.097852529278471, 1.0965631451243283, 1.0952802967695447, 1.0940039808162947, 1.092734195385424, 1.091470939932163, 1.0902142149764633, 1.0889640217428531, 1.0877203617078643, 1.086483236056928, 1.0852526450566582, 1.0840285873529207, 1.082811059209386, 1.08160005370536, 1.0803955599152417, 1.0791975620945686, 1.0780060388993422, 1.0768209626655256, 1.0756422987747063, 1.074470005129482, 1.0733040317585607, 1.0721443205669479, 1.070990805241221, 1.0698434113142108, 1.0687020563875445, 1.0675666505050876, 1.0664370966653471, 1.065313291456822, 1.064195125797147, 1.0630824857547285, 1.0619752534305718, 1.060873307877824, 1.0597765260373573, 1.0586847836692266, 1.0575979562617643, 1.0565159199025587, 1.0554385520981724, 1.0543657325321012, 1.0532973437532511, 1.0522332717896128, 1.0511734066842127, 1.050117642952472, 1.0490658799618122, 1.0480180222358808, 1.0469739796869133, 1.0459336677806446, 1.0448970076388406, 1.0438639260848968, 1.0428343556382356, 1.0418082344631956, 1.0407855062781204, 1.0397661202301003, 1.0387500307405921, 1.0377371973268263, 1.036727584403523, 1.0357211610691048, 1.0347179008801857, 1.0337177816177316, 1.032720785047922, 1.0317268966804096, 1.0307361055262882, 1.0297484038578064, 1.0287637869715633, 1.0277822529566172, 1.0268038024687718, 1.0258284385119834, 1.0248561662277087, 1.0238869926928185, 1.0229209267265034, 1.0219579787065185, 1.0209981603949414, 1.0200414847735613, 1.0190879658888712, 1.0181376187066011, 1.0171904589756475, 1.016246503101208, 1.0153057680268536, 1.0143682711252893, 1.0134340300974733, 1.012503062879746, 1.011575387558675, 1.0106510222931788, 1.0097299852436041, 1.0088122945073985, 1.0078979680609557, 1.0069870237073122, 1.0060794790293053, 1.0051753513478578, 1.004274657685029, 1.0033774147315122, 1.0024836388182454, 1.0015933458918502, 1.0007065514935838, 0.9998232707415216, 0.998943518315737, 0.9980673084461774, 0.997194654903057, 0.996325570989477, 0.9954600695361133, 0.9945981628977522, 0.9937398629514861, 0.9928851810964001, 0.9920341282545916, 0.991186714873359, 0.9903429509284423, 0.9895028459281484, 0.9886664089182893, 0.987833648487782, 0.9870045727748241, 0.9861791894735571, 0.9853575058411214, 0.9845395287050381, 0.9837252644708228, 0.9829147191298009, 0.9821078982670389, 0.9813048070693504, 0.9805054503333286, 0.9797098324733801, 0.9789179575296799, 0.9781298291760732, 0.9773454507278516, 0.976564825149392, 0.9757879550616664, 0.9750148427495332, 0.9742454901688945, 0.973479898953612, 0.9727180704222461, 0.9719600055845542, 0.9712057051477919, 0.9704551695227683, 0.969708398829695, 0.9689653929037962, 0.9682261513006986, 0.9674906733015916, 0.9667589579181827, 0.9660310038974055, 0.9653068097259527, 0.9645863736345691, 0.9638696936021675, 0.963156767359726, 0.9624475923940173, 0.9617421659511498, 0.9610404850399146, 0.9603425464350015, 0.9596483466800197, 0.9589578820903961, 0.9582711487561014, 0.9575881425442622, 0.9569088591016301, 0.9562332938569377, 0.9555614420231408, 0.9548932985995428, 0.9542288583738331, 0.9535681159240335, 0.9529110656203482, 0.9522577016269453, 0.9516080179036757, 0.9509620082076947, 0.9503196660950615, 0.94968098492227, 0.9490459578477255, 0.948414577833191, 0.9477868376451957, 0.9471627298564052, 0.946542246846966, 0.9459253808058528, 0.9453121237321513, 0.9447024674363783, 0.9440964035417557, 0.9434939234855093, 0.942895018520136, 0.9422996797147112, 0.9417078979561626, 0.941119663950589, 0.9405349682245701, 0.9399538011264954, 0.9393761528279209, 0.9388020133249414, 0.9382313724395764, 0.9376642198212, 0.937100544947991, 0.9365403371284033, 0.9359835855026843, 0.9354302790444136, 0.9348804065620895, 0.9343339567007355, 0.933790917943559, 0.9332512786136413, 0.9327150268756673, 0.9321821507377015, 0.931652638052998, 0.9311264765218543, 0.9306036536935118, 0.9300841569680927, 0.9295679735985888, 0.9290550906928806, 0.9285454952158134, 0.9280391739913051, 0.927536113704511, 0.9270363009040055, 0.9265397220040443, 0.9260463632868378, 0.9255562109048723, 0.925069250883279, 0.9245854691222429, 0.9241048513994438, 0.9236273833725398, 0.9231530505816868, 0.9226818384521019, 0.9222137322966429, 0.9217487173184387, 0.9212867786135571, 0.9208279011736732, 0.9203720698888057, 0.9199192695500551, 0.9194694848523762, 0.9190227003973799, 0.9185789006961508, 0.9181380701720928, 0.9177001931637903, 0.917265253927886, 0.9168332366419906, 0.9164041254075783, 0.915977904252923, 0.9155545571360263, 0.9151340679475738, 0.914716420513877, 0.9143015985998327, 0.9138895859118901, 0.9134803661010056, 0.913073922765615, 0.912670239454595, 0.9122692996702101, 0.9118710868710801, 0.9114755844751165, 0.9110827758624651, 0.9106926443784286, 0.9103051733363854, 0.9099203460206917, 0.9095381456895608, 0.909158555577932, 0.9087815589003384, 0.9084071388537146, 0.9080352786202148, 0.9076659613700084, 0.9072991702640231, 0.906934888456706, 0.9065730990987046, 0.9062137853395777, 0.9058569303304242, 0.90550251722652, 0.9051505291898998, 0.9048009493919231, 0.9044537610157887, 0.9041089472590293, 0.9037664913359662, 0.9034263764801244, 0.9030885859466118, 0.9027531030144658, 0.9024199109889557, 0.9020889932038394, 0.9017603330235993, 0.9014339138456207, 0.9011097191023294, 0.900787732263307, 0.9004679368373354, 0.9001503163744242, 0.8998348544677868, 0.8995215347557639, 0.8992103409237231, 0.8989012567058994, 0.8985942658872048, 0.8982893523049846, 0.8979864998507362, 0.8976856924717846, 0.897386914172911, 0.897090149017948, 0.8967953811313154, 0.8965025946995349, 0.8962117739726807, 0.8959229032658037, 0.8956359669603079, 0.8953509495052847, 0.895067835418807, 0.8947866092891807, 0.8945072557761565, 0.8942297596121123, 0.8939541056031686, 0.8936802786302982, 0.893408263650369, 0.8931380456971669, 0.8928696098823752, 0.8926029413965081, 0.8923380255098186, 0.8920748475731726, 0.8918133930188717, 0.8915536473614583, 0.8912955961984647, 0.891039225211163, 0.8907845201652322, 0.8905314669114439, 0.8902800513862792, 0.8900302596125267, 0.8897820776998533, 0.8895354918453338, 0.8892904883339531, 0.8890470535390991, 0.8888051739229871, 0.8885648360370918, 0.8883260265225328, 0.8880887321104426, 0.887852939622293, 0.8876186359702187, 0.8873858081572875, 0.8871544432777755, 0.8869245285173871, 0.8866960511534796, 0.8864689985552402, 0.8862433581838562, 0.8860191175926603, 0.8857962644272454, 0.8855747864255661, 0.8853546714180197, 0.8851359073275049, 0.8849184821694545, 0.88470238405186, 0.8844876011752705]\n",
    "plotErrorCurves(trainLoss, testLoss, title = 'Logistic Regression, n most common categories = 3' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Application of Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1  Normalization\n",
    "Data is normalized to create independence in the scales and ranges of features. For example, one might consider normalizing a dataset with the features 'years employed' and 'salary' because years employed has a range from 0 to 30 and salary ranges from 10,000 to 500,000. The wider range of salary might overshadow contributions of years employed.  Once the data is normalized, it has the benefit of being unit neutural. That is, years can be compared to dollars in an apples to apples way.  We chose to normalize the data of all features because some columns had immense ranges compared to other columns.  The contributions of the one hot encoded columns and other features with small ranges would be overshadowed.   Further, we did not know what units were used to measure the features (or if they were even measurable). We chose to standardize the data using the z score: $x_n= (x - \\mu)/\\sigma$ where $\\mu$ is the mean value and $\\sigma$ is the standard deviation. This figure expresses the number of standard deviations from the mean an x value is. It casts each datum as a magnitude against the feature set's own statistical characteristics.\n",
    "\n",
    "Another benefit of normalization is better performance of gradient descent, at least anecdotely. This is most obvious in a linear regression.  Consider a graph minimizing the loss function for the salary/years regression discussed earlier. The graph of the coeficients $\\theta_{1}$ and $\\theta_{2}$ would be concentric elipses. However the elipses would be extremely elongated and have an aspect ratio of year_range/salary_range (or 30/490000). Extreme surface changes will increase the number of gradients. Now consider standardizing the values using a z score. Most of the data will fall between 3 standard deviations from the mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2  One Hot Encoding\n",
    "One Hot Encoding is a technique used to convert categorical data into numeric data. This is a useful and often essential task in data preperation because many classification and regression methods, logistic regression included, require all features to be in numeric form.\n",
    "\n",
    "In order to encode a feature, you first must identify all the unique categories. This requires all the categories of a feature to be shared among all machines in the cluster. The categorical feature is removed and the data structure is expanded to add new columns (features) for each category. A '1' or '0' value is placed in each column acording to the original value of the category. For example, the second data set is a one hot encoded version of the first.\n",
    "\n",
    "|count|fruit  |\n",
    "|-----|----------|\n",
    "|10   |bannana|\n",
    "|5    |apple  |\n",
    "|20   |orange|\n",
    "|12   |orange|\n",
    "|15   |bannana|\n",
    "\n",
    "\n",
    "|count|bannana  | apple |orange|\n",
    "|-----|---------|-------|------|\n",
    "|10   |1        | 0     |  0   |\n",
    "|5    |0        | 1     |  0   |\n",
    "|20   |0        | 0     |  1   |\n",
    "|12   |0        | 0     |  1   |\n",
    "|15   |1        | 0     |  0   |\n",
    "\n",
    "\n",
    "For toy data sets this representation is workable, however for large datasets like ours there are tradeoffs to consider. In our small data set, among all the categorical features, there are about 72000 unique values. In the large dataet there were over 30 milion. We observed that there were more columns than rows. If every category is promoted in importance to be represented by its own coefficient in the regression, there would probably not be enough data and more important contributers would be crowded out. Our team avoided this by only using the top 3 categories per feature.\n",
    "\n",
    "Also troubling is the task of loading and storing in this format. It would create an explosion in size that would consume resources and slow down development and debugging. Say goodbye to head(5) with 72000, let alone 30m, additional columns! If you could even load the structure into memory.\n",
    "\n",
    "Thankfully, PySpark.ml provides striped representation of the data. Stripes is a more dense representation for one hot encoded features because the 0 values are not actually stored, but presumed to be default. Instead of creating new columns for each new category-feature, the Spark OneHotEncoderEstimator will produce a single column with values representing the entire row of categorical data. For example, using this dense representation on the original count/fruit table yields:\n",
    "\n",
    "|count|fruit  |\n",
    "|-----|----------|\n",
    "|10   |(3,[0],[1])|\n",
    "|5    |(3,[1],[1])  |\n",
    "|20   |(3,[2],[1])|\n",
    "|12   |(3,[2],[1])|\n",
    "|15   |(3,[0],[1])|\n",
    "\n",
    "where each item is a tupple in the form (NUM,[POS],[VAL])\n",
    "Where NUM is the number of columns (features) in the one hot encode, POS is the position of the feature in the row, and VAL is the value of the feature.  For example, the first row (3,[0],[1]) translates to 3 columns total, with a 1 in position 0 in this row.  Which is equivilant to:\n",
    "\n",
    "|count|bannana  | apple |orange|\n",
    "|-----|---------|-------|------|\n",
    "|10   |1        | 0     |  0   |\n",
    "\n",
    "\n",
    "Naturally, this dense representation works well in the Spark.ml API ecosystem.  Having implemented a homegrown version of logistic regression, we  implemented a homegrown version of one hot encoding as well, in our logloss and gdupdate methods.  The PySpark.ml representation would probably have sped things up by minimizing data moving on the network, memory utilization, and maybe even disk swaping.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4  Scaleability\n",
    "Had it not been for cloud based comodity hardware, there would be no MIDS program, and little data science. It makes big things possible, but it is not a cure all for bad programming habits.  Maybe you've heard of Wirth's law regarding software bloat.  'Software expands to fill all new resources', 'Software gets slower more rapidly than hardware becomes faster' are a few forms.  Having more computing power at our fingertips makes abuse easier.\n",
    "\n",
    "Consider the above example, one hot encoding.  It is very easy to be lulled into thinking that one hot encoding the whole dataset would be a workable part of the solution.  Thankfully we caught it before we tried to implement it.\n",
    "\n",
    "But far from relieving programmers from thinking about scaleability, programming in this environment makes you very conscious of it in a way that you don't have to be when programming on a single machine.  For example map() and flatMap()s are narrow transformations.  They can work with data in the local partition.  Our code does that 15 times. The 4 reduces are wide transformations that require a shuffle.  Even though reduce is done strategically, it is sometime not avoidable:\n",
    "\n",
    "Occurances of Reduce:\n",
    "In GDUpdateWithReg, this is most expensive because it occurs once per iteration.\n",
    "\n",
    "To calculate predictionResults.  This only occurs once per execution.\n",
    "\n",
    "To calculate LogLoss:  Unfortunately, this is done twice per iteration.  We could have executed it a few times for the whole execution.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
