{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "W261 Machine Learning at Scale<br>\n",
    "12 December 2018\n",
    "\n",
    "Wei Wang;\n",
    "Alice Lam;\n",
    "John Tabbone;\n",
    "Noah Randolph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Formulation\n",
    "\n",
    "Alice Lam: \"enhancing CTR means improved monetization of your current traffic (eyeballs/views). The algorithm to predict CTR accurately is useful for the platform to show specific ads to specific people who would have the highest CTR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting toyDataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile toyDataset.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SEED = 2615\n",
    "NUMERICCOLS = 2\n",
    "ONEHOTCOLS = 2\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def generateToyDataset(w=[8, -3, -1, 3, 8]):\n",
    "    '''generate toy logistic regression dataset with numerical and 1-hot encoded features'''\n",
    "    nrows=8\n",
    "    np.random.seed(SEED)\n",
    "    x1 = np.random.randint(0, 10, nrows)\n",
    "    x2 = np.random.randint(0, 10, nrows)\n",
    "    x3 = np.random.randint(0, 2, nrows) # simulate 1-hot\n",
    "    x4 = np.ones(nrows, np.int8) - x3   # with x3 and x4\n",
    "    noise = np.random.normal(5, 1, nrows)\n",
    "    v = (w[0] + x1*w[1] + x2*w[2] + x3*w[3] + x4*w[4] + noise)\n",
    "    y = (v>0) * 2 - 1 # y = 1 or -1\n",
    "    df = spark.createDataFrame(zip(y.tolist(), x1.tolist(), x2.tolist(), x3.tolist(), x4.tolist()))\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,2)]+['C{}'.format(i) for i in range(0,2)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    return df\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0])) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    new_model = W - learningRate * grad \n",
    "    return new_model\n",
    "\n",
    "\n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + [row['C{}'.format(i)] for i in range(0, ONEHOTCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data around the mean of each feature.\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStdev, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "# create a toy dataset that includes 1-hot columns for development\n",
    "df = generateToyDataset()   \n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = df.rdd.map(dfToRDD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# normalize RDD\n",
    "normedRDDcached = normalize(trainRDD).cache()\n",
    "print(normedRDDcached.take(1))\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(normedRDDcached.take(1)[0][0])\n",
    "wInitial = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# 1 iteration of gradient descent\n",
    "w = GDUpdate(normedRDDcached, wInitial)\n",
    "\n",
    "nSteps = 5\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    w = GDUpdate(normedRDDcached, w)\n",
    "    loss = logLoss(normedRDDcached, w)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(i,3) for i in w]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 22:39:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-05 22:39:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-12-05 22:39:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "[(array([-1.63525964,  0.62123652,  1.        , -1.        ]), 1)]\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 7.7014879401802405\n",
      "Model: [1.101, -0.162, -1.391, -0.468, -0.797]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 6.1460241873746195\n",
      "Model: [0.865, -0.478, -1.282, -0.469, -0.795]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 5.006600355698076\n",
      "Model: [0.66, -0.743, -1.182, -0.491, -0.774]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 4.183106733189442\n",
      "Model: [0.485, -0.964, -1.092, -0.52, -0.744]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 3.591671886745936\n",
      "Model: [0.337, -1.15, -1.016, -0.55, -0.714]\n"
     ]
    }
   ],
   "source": [
    "!python toyDataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA & Discussion of Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting loadAndEDA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loadAndEDA.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import udf, col, countDistinct, isnan, when, count, desc\n",
    "import pandas as pd\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "MINICLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "MINILOCALPATH = 'data/train_005.txt'\n",
    "\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; MINICLOUDPATH; MINILOCALPATH\n",
    "    df = spark.read.csv(path=MINILOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,13)]+['C{}'.format(i) for i in range(0,26)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def displayHead(df, n=5):\n",
    "    '''returns head of the training dataset'''\n",
    "    return df.head(n)\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''returns approximate median values of the columns given, with null values ignored'''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "def getDescribe(df, cols):\n",
    "    return df.select(cols).describe().show()\n",
    "\n",
    "def getDistinctCount(df, cols):\n",
    "    return df.agg(*(countDistinct(col(c)).alias(c) for c in cols)).show()\n",
    "\n",
    "def checkNA(df, cols):\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()\n",
    "\n",
    "def getCorrMatrix(df, cols):\n",
    "    df = df.select(cols)\n",
    "    col_names = df.columns\n",
    "    features = df.rdd.map(lambda row: row[0:])\n",
    "    corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "    corr_df = pd.DataFrame(corr_mat)\n",
    "    corr_df.index, corr_df.columns = col_names, col_names\n",
    "    return corr_df\n",
    "    \n",
    "    \n",
    "df = loadData().cache()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "#print(\"\\nTEST DATASET ROW COUNTS: \", testDf.count())\n",
    "#print(\"\\nTRAIN DATASET ROW COUNTS: \", trainDf.count())\n",
    "## print(\"HEAD\\n\", displayHead(trainDf))\n",
    "#print(\"\\nCOLUMN TYPES\\n\", df.dtypes)\n",
    "#print(\"\\nMEDIAN OF NUMERIC COLUMNS\\n\", getMedians(trainDf, trainDf.columns[1:14]))\n",
    "\n",
    "#print(\"\\nDESCRIPTIONS OF NUMERICAL COLUMNS 1-7\\n\")\n",
    "#getDescribe(trainDf, trainDf.columns[1:8])\n",
    "#print(\"\\nDESCRIPTIONS OF NUMERICAL COLUMNS 8-14\\n\")\n",
    "#getDescribe(trainDf, trainDf.columns[8:15])\n",
    "#print(\"\\nCOUNTS OF DISTINCT VALUE FOR CATEGORICAL VARIABLE COLUMNS\")\n",
    "#getDistinctCount(trainDf, trainDf.columns[15:])\n",
    "\n",
    "#print(\"\\nCOUNTS OF NAs FOR COLUMN 0 - 19\")\n",
    "#checkNA(trainDf, trainDf.columns[:20])\n",
    "#print(\"\\nCOUNTS OF NAs FOR COLUMN 20 - 39\")\n",
    "#checkNA(trainDf, trainDf.columns[20:])\n",
    "\n",
    "##getCorrMatrix(trainDf, trainDf.columns[1:14]) # This doesn't work if there's NA in there\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 23:32:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-05 23:32:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-12-05 23:32:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2018-12-05 23:32:19 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "2018-12-05 23:32:23 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2018-12-05 23:32:23 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "!python loadAndEDA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'submit_job_to_cluster.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=testcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=row_counts.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from running EDA code above:\n",
    "Main dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 9164811)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 36675806)<br>\n",
    "\n",
    "Toy dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 4578)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 18379)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting algorithmImplementation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile algorithmImplementation.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, desc, isnan, when\n",
    "import numpy as np\n",
    "from operator import add\n",
    "import copy\n",
    "\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "TOYCLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "TOYLOCALPATH = 'data/train_005.txt'\n",
    "NUMERICCOLS = 13\n",
    "CATEGORICALCOLS = 26\n",
    "NUMERICCOLNAMES = ['I{}'.format(i) for i in range(0,NUMERICCOLS)]\n",
    "CATCOLNAMES = ['C{}'.format(i) for i in range(0,CATEGORICALCOLS)]\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"featureEngineering\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; TOYCLOUDPATH; TOYLOCALPATH\n",
    "    df = spark.read.csv(path=TOYLOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label'] + NUMERICCOLNAMES + CATCOLNAMES\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''\n",
    "    returns approximate median values of the columns given, with null values ignored\n",
    "    '''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "\n",
    "def getMostFrequentCats(df, cols, n):\n",
    "    '''\n",
    "    returns a dict where the key is the column and value is an ordered list\n",
    "    of the top n categories in that column in descending order\n",
    "    '''\n",
    "    freqCatDict = {col: None for col in df.columns[cols:]}\n",
    "    for col in df.columns[cols:]:\n",
    "        listOfRows = df.groupBy(col).count().sort('count', ascending=False).take(n)\n",
    "        topCats = [row[col] for row in listOfRows]\n",
    "        freqCatDict[col] = topCats[:n]\n",
    "    return freqCatDict\n",
    "    \n",
    "\n",
    "def rareReplacer(df, dictOfMostFreqSets):\n",
    "    '''\n",
    "    Iterates through columns and replaces non-Frequent categories with 'rare' string.\n",
    "    '''\n",
    "    for colName in df.columns[NUMERICCOLS+1:]:\n",
    "        bagOfCats = dictOfMostFreqSets[colName]\n",
    "        df = df.withColumn(colName, udf(lambda x: 'rare' if x not in bagOfCats else x, StringType())(df[colName])).cache()\n",
    "    return df\n",
    "\n",
    "    \n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + [row['C{}'.format(i)] for i in range(0, CATEGORICALCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def emitColumnAndCat(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits a record for each categorical column value along with a zero for one-hot encoding.\n",
    "    The emitted values will become a reference dictionary for one-hot encoding in later steps.\n",
    "        Input: (array([features], dtype='<U21'), 0) or (features, label)\n",
    "        Output: ((categorical column, category), 0) or (complex key, value)\n",
    "    The last zero in the output is for initializing one-hot encoding.\n",
    "    \"\"\"\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        yield ((catColName, element), 0)\n",
    "\n",
    "\n",
    "def oneHotEncoder(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits row where categorical columns are replaced with 1-hot encoded columns.\n",
    "        Input: (numerical and categorical features, label)\n",
    "        Output: (numerical and one-hot encoded categorical features, label)\n",
    "    \"\"\"\n",
    "    oneHotDict = copy.deepcopy(oneHotReference)\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        oneHotDict[(catColName, element)] = 1\n",
    "    numericElements = list(line[0][:NUMERICCOLS])\n",
    "    features = np.array(numericElements + [value for key, value in oneHotDict.items()], dtype=np.float)\n",
    "    return (features, line[1])\n",
    "\n",
    "\n",
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data around the mean of each feature.\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStdev, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0])) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    new_model = W - learningRate * grad \n",
    "    return new_model\n",
    "\n",
    "\n",
    "# load data\n",
    "df = loadData()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "testDf.cache()\n",
    "trainDf.cache()\n",
    "\n",
    "# get top n most frequent categories for each column (in training set only)\n",
    "n = 10\n",
    "mostFreqCatDict = getMostFrequentCats(trainDf, NUMERICCOLS+1, n)\n",
    "\n",
    "# get dict of sets of most frequent categories in each column for fast lookups during filtering (in later code)\n",
    "setsMostFreqCatDict = {key: set(value) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get the top category from each column for imputation of missing values (in training set only)\n",
    "fillNADictCat = {key: (value[0] if value[0] is not None else value[1]) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get dict of median numeric values for imputation of missing values (in training set only)\n",
    "fillNADictNum = {key: value for (key, value) in zip(trainDf.columns[1:NUMERICCOLS+1], \n",
    "                                                    [x[0] for x in getMedians(trainDf,\n",
    "                                                                              trainDf.columns[1:NUMERICCOLS+1])])}\n",
    "\n",
    "# impute missing values in training and test set\n",
    "trainDf = trainDf.na.fill(fillNADictNum) \\\n",
    "                 .na.fill(fillNADictCat).cache()\n",
    "testDf = testDf.na.fill(fillNADictNum) \\\n",
    "               .na.fill(fillNADictCat).cache()\n",
    "\n",
    "# replace low-frequency categories with 'rare' string in training and test set\n",
    "trainDf = rareReplacer(trainDf, setsMostFreqCatDict) # df gets cached in function\n",
    "testDf = rareReplacer(testDf, setsMostFreqCatDict) # df gets cached in function\n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = trainDf.rdd.map(dfToRDD).cache()\n",
    "testRDD = testDf.rdd.map(dfToRDD).cache()\n",
    "        \n",
    "# create and broadcast reference dictionary to be used in constructing 1 hot encoded RDD\n",
    "oneHotReference = trainRDD.flatMap(emitColumnAndCat) \\\n",
    "                          .reduceByKeyLocally(add) # note: only the zero values are being added here (main goal is to output a dictionary)\n",
    "sc.broadcast(oneHotReference)\n",
    "\n",
    "# replace rows with new rows having categorical columns 1-hot encoded\n",
    "trainRDD = trainRDD.map(oneHotEncoder).cache()\n",
    "testRDD = testRDD.map(oneHotEncoder).cache()\n",
    "\n",
    "# normalize RDD\n",
    "normedTrainRDD = normalize(trainRDD).cache()\n",
    "normedTestRDD = normalize(testRDD).cache() # use the mean and st. dev. from trainRDD\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(normedTrainRDD.take(1)[0][0])\n",
    "wInitial = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# 1 iteration of gradient descent\n",
    "w = GDUpdate(normedTrainRDD, wInitial)\n",
    "\n",
    "nSteps = 10\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    w = GDUpdate(normedTrainRDD, w)\n",
    "    loss = logLoss(normedTrainRDD, w)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(i,3) for i in w]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-08 02:44:29 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-08 02:44:41 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: inf\n",
      "Model: [358.729, 21.922, 5.146, -1.764, -18.671, -41.75, -36.87, 10.065, -21.623, 0.408, 46.777, 51.258, 12.894, -45.742, -27.931, -10.78, -3.445, 3.091, 26.472, 13.583, -13.006, -5.371, 43.822, -25.15, -16.464, -13.975, -10.827, -17.257, -18.932, -15.589, -40.385, -18.487, -14.221, 1.164, -12.466, -5.398, -40.994, -0.752, -5.702, 12.001, 10.257, -35.896, -0.159, -5.749, -20.26, -23.03, 28.947, 26.019, 24.446, -8.47, 24.25, 10.691, -3.33, 2.283, -1.795, 9.354, -14.616, 12.23, 0.696, -31.107, 7.783, 2.427, -8.924, -3.717, -0.58, 20.644, 8.011, 9.975, 7.573, 66.543, 25.153, 11.681, 5.89, 12.175, 26.758, -9.116, 4.48, 0.614, -45.127, -8.233, -25.388, -9.299, -6.104, 3.881, 60.388, 2.311, -9.239, -6.592, -1.365, 3.519, 1.124, -2.633, -15.961, 12.201, 8.381, 9.492, 11.493, -29.699, -33.499, -32.849, -30.075, -12.864, 5.862, 10.63, -15.727, -2.115, 41.46, -2.835, 17.069, -5.193, -25.782, -3.242, -4.084, -3.483, -27.527, 1.738, 2.097, 9.377, 9.103, -3.133, 3.595, -10.325, 10.901, -7.085, -0.913, -6.262, -3.414, 15.215, -14.187, -8.526, -5.257, 11.551, -5.495, 7.843, -10.302, -39.571, 4.759, -1.985, -4.105, 1.319, 1.17, 0.403, -4.47, -10.916, 0.245, 2.73, 6.499, 2.149, -10.759, -5.364, 0.268, 3.176, -7.272, 4.737, 4.12, 18.263, 12.905, 13.309, -19.18, -2.131, -9.03, -6.863, -3.54, -5.296, -6.008, -4.991, 15.781, 19.891, 2.785, -21.652, 0.963, -3.036, -2.349, 0.655, 20.856, -2.627, 3.856, -2.144, 9.301, 5.84, -0.249, 0.261, -2.197, 19.745, -0.452, 2.139, -13.347, 9.451, 16.032, -12.523, -13.213, -15.208, -13.703, -13.882, -4.108, 11.798, 2.25, -1.55, 0.181, -7.405, -7.955, -6.651, -2.691, -7.416, -4.605, 11.654, 8.151, 8.322, -4.526, -19.19, -6.002, -3.573, -1.111, 25.761, 26.094, 25.993, 26.592, 9.207, 4.649, -1.057, 13.825, -0.497, 0.784, 12.202, 9.763, 18.268, 23.116, 5.191, 6.672, 6.46, 7.271, -10.334, -6.455, -0.995, 5.303, -9.037, 17.974, -16.142, -19.708, -5.677, 9.14, -15.515, -14.912, -7.983, 8.954, 54.049, 55.738, 55.144, 55.301, 54.203, 47.505, 6.673, -1.423, -4.755, -19.271, -17.91, 1.634, 2.603, 3.151, 6.628, -4.601, -4.683, 0.53, 3.727, -2.992, -18.87, 7.884, 8.247, 9.243]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 25889.968447056082\n",
      "Model: [400.071, 16.338, 4.169, -2.684, -14.508, -32.405, -32.103, 3.583, -22.162, -2.427, 46.458, 40.807, 11.469, -42.938, -24.485, -12.303, -29.196, -20.009, 22.576, 13.65, -6.026, -3.388, 22.275, -3.103, -5.777, -34.042, -2.772, -11.159, -8.78, -36.949, -36.954, -0.986, 3.683, 17.742, -33.826, -3.555, -23.672, -11.311, 8.05, -2.535, 9.771, -22.853, 16.975, 16.618, -14.356, -21.914, 8.239, 16.099, 14.525, -4.058, 14.33, 7.541, -4.679, -9.664, 11.363, 2.956, -10.325, 8.835, -3.142, -26.435, 1.384, 3.498, -24.22, -0.406, -7.32, 12.534, 4.765, 5.042, 5.711, 46.344, 17.572, -3.644, 0.485, 3.755, 29.296, 1.287, 1.82, -0.276, -23.851, 2.287, -8.696, 1.221, -5.782, 4.851, 49.106, 4.11, 1.466, -0.021, -6.141, 3.77, 1.556, -1.804, -13.442, 5.323, 1.982, 5.844, 4.615, 6.225, -30.973, 3.074, 5.848, 20.051, 4.265, 7.807, -9.691, -1.955, 37.581, -1.58, 9.533, 8.629, -4.999, 10.58, 9.738, 10.339, -7.287, 0.142, 3.088, 4.003, 2.75, -10.63, 0.959, -11.636, 9.727, -4.037, -5.002, -6.582, -2.819, 16.165, -11.765, -8.531, -6.494, 4.13, -6.669, 1.302, -4.488, -21.481, -1.398, -3.147, -6.029, -0.725, -3.716, 0.748, -1.825, -7.205, -2.48, 1.354, 3.466, -1.0, -12.203, -5.536, -2.834, -2.077, -6.652, 2.127, 1.28, 14.304, 8.13, 8.534, -19.66, -0.027, -7.23, 5.444, 8.766, 7.01, 6.299, 7.315, 11.412, 16.226, 1.348, -17.152, -1.668, -8.084, -5.185, -1.976, 21.296, -2.694, 3.459, -5.712, 5.731, 6.548, -3.413, -1.441, -5.686, 17.326, -2.623, -1.534, -9.65, 6.317, 12.477, -10.507, 2.878, 0.883, 2.388, 2.209, -4.335, 8.061, -1.141, -2.816, -2.411, -6.746, -7.297, -5.992, -4.142, -6.758, -4.347, 5.464, 2.665, 2.835, -3.812, -18.548, -5.478, -2.841, 0.081, 22.126, 22.459, 22.358, 22.957, 4.234, 0.619, -4.309, 12.747, -2.86, -1.932, 8.364, 7.215, 14.065, 20.506, 6.097, 3.292, 2.43, 3.241, -10.324, -5.602, -3.852, 5.082, -7.186, 14.392, -6.957, -23.305, -4.484, 6.702, -8.503, -7.9, -11.578, 5.821, 50.299, 51.925, 51.394, 51.551, 50.453, 42.668, 3.631, -3.789, 5.459, -13.691, -12.331, 0.945, 1.411, 2.462, 5.821, -7.107, -5.293, 5.912, 2.98, -3.423, -20.264, 7.453, 7.637, 8.938]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 11941.587684150702\n",
      "Model: [417.494, 14.332, 3.028, -2.366, -6.55, -27.225, -26.557, 2.926, -17.532, 0.257, 42.105, 40.285, 10.237, -33.866, -17.323, -8.862, -23.675, -12.59, 20.283, 9.489, -8.396, -1.78, 16.154, -1.429, -7.451, -27.994, -3.338, -10.23, -5.646, -29.746, -33.327, -6.761, -0.862, 6.866, -26.623, -0.955, -19.782, -4.314, -0.408, 6.827, 6.461, -18.551, 12.628, 11.863, -9.004, -16.038, 5.051, 11.739, 10.166, -2.482, 9.97, 3.926, -1.987, -10.946, 1.285, 4.891, -8.836, 7.318, -2.584, -22.609, 3.319, 8.015, -17.111, -1.364, -5.746, 9.116, 2.912, 3.392, 3.37, 39.205, 14.376, -0.317, 6.906, 0.718, 24.45, -0.32, 0.525, -0.759, -17.707, 0.691, -8.377, -0.375, -9.163, 3.283, 45.241, 2.727, 0.19, -1.811, -4.697, 3.998, -0.552, -2.215, -15.864, 2.424, -0.715, 4.307, 1.716, 3.547, -30.573, 0.397, 3.171, 16.322, 3.013, 5.864, -8.395, -1.561, 33.876, -3.258, 7.836, 7.033, -7.524, 8.984, 8.141, 8.743, -9.856, -0.542, 2.917, 0.737, 0.072, -12.036, 0.522, -10.66, 7.014, -1.387, -5.461, -4.762, 0.098, 14.623, -7.942, -8.078, -4.095, 1.852, -4.024, -0.514, -2.054, -18.519, -0.991, -4.243, -7.205, -2.175, -3.1, 0.891, -1.397, -6.274, -3.094, 1.472, 2.215, -2.327, -9.553, -6.832, -2.271, -4.29, -7.745, -0.016, -0.93, 12.635, 6.117, 6.521, -18.594, -0.642, -8.282, 3.725, 7.048, 5.292, 4.58, 5.597, 8.15, 14.008, 2.138, -10.67, -2.455, -7.915, -5.697, -2.764, 19.472, -1.073, 4.791, -7.607, 4.227, 10.809, -2.117, 0.171, -7.157, 14.556, -3.997, -3.083, -9.155, 4.996, 11.518, -9.133, 1.064, -0.931, 0.574, 0.395, -1.137, 6.486, -0.978, -4.548, -3.504, -8.596, -9.147, -7.843, -5.476, -8.608, -6.079, 12.391, 12.276, 12.447, -1.25, 4.649, -5.318, -1.595, 0.723, 20.594, 20.927, 20.826, 21.425, 11.622, -0.044, -5.679, 11.632, -0.527, -3.077, 6.747, 6.29, 12.294, 19.405, 6.743, 1.868, 0.732, 1.543, -8.272, -5.705, 0.462, 3.819, -5.415, 12.882, -6.763, 0.055, -5.661, 8.662, -10.007, -9.405, -7.3, 4.5, 48.718, 50.318, 49.813, 49.971, 48.873, 40.312, 3.715, -3.033, 4.366, -10.617, -9.256, -0.698, -0.082, 0.819, 5.481, -1.792, 1.23, 5.625, 2.665, -3.605, -8.354, 7.271, 7.38, 8.809]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 10392.87320215267\n",
      "Model: [420.798, 13.684, 2.653, -2.381, -4.78, -23.032, -22.077, 2.32, -17.045, 0.626, 41.701, 39.049, 9.872, -30.779, -15.836, -7.533, -22.7, -11.492, 19.848, 9.203, -7.503, -1.36, 12.579, -0.079, -6.826, -26.916, -2.559, -10.09, -5.284, -28.663, -30.603, -5.5, 0.124, 7.124, -25.54, -0.845, -16.982, -3.462, 0.581, 6.034, 6.257, -17.812, 11.804, 10.962, -7.345, -15.106, 3.758, 10.912, 9.339, -2.6, 9.144, 3.696, -2.314, -11.956, 1.324, 4.379, -8.267, 7.088, -2.891, -20.635, 2.807, 7.778, -17.669, -1.572, -6.331, 8.468, 2.187, 2.938, 2.926, 36.946, 13.77, -1.179, 7.136, 0.047, 23.859, -0.625, -0.34, -1.155, -14.124, 0.388, -8.914, -0.678, -9.891, 2.812, 44.008, 3.145, -0.052, -2.151, -5.06, 3.25, -0.846, -3.138, -15.616, 1.875, -1.226, 4.015, 1.167, 3.04, -29.296, -0.111, 2.663, 15.615, 3.309, 6.047, -8.606, -1.881, 33.174, -3.576, 7.311, 6.73, -7.308, 8.681, 7.839, 8.44, -9.66, -0.838, 3.442, 0.669, -0.435, -12.874, 0.468, -10.934, 6.5, -0.588, -4.648, -3.931, 0.419, 14.725, -8.144, -8.542, -4.476, 1.216, -4.015, -1.084, -1.287, -16.651, -1.576, -4.204, -6.673, -1.744, -3.55, 1.553, -0.253, -6.552, -2.569, 1.201, 1.818, -2.579, -8.936, -7.235, -2.657, -4.71, -7.952, -0.423, -1.349, 12.319, 5.735, 6.14, -17.976, 0.096, -7.702, 3.4, 6.723, 4.966, 4.255, 5.272, 7.926, 13.588, 1.642, -9.352, -2.905, -8.377, -6.195, -3.213, 19.496, -1.306, 4.131, -7.967, 3.942, 12.026, -2.37, -0.137, -7.435, 13.927, -4.586, -3.376, -8.384, 4.746, 11.16, -6.948, 0.721, -1.275, 0.23, 0.051, -1.402, 6.187, -1.468, -4.29, -2.111, -7.995, -8.546, -7.241, -4.414, -8.007, -5.392, 11.897, 11.664, 11.835, -1.506, 4.082, -6.022, -0.555, 0.335, 20.304, 20.637, 20.536, 21.135, 11.118, -0.366, -5.939, 11.421, -0.191, -3.294, 6.44, 6.711, 11.958, 19.197, 6.546, 1.598, 0.41, 1.221, -7.229, -5.106, -0.133, 3.58, -3.484, 12.595, -6.381, -0.402, -6.043, 8.374, -10.293, -9.69, -7.588, 4.249, 48.418, 50.013, 49.513, 49.671, 48.573, 39.865, 3.472, -3.222, 4.159, -10.907, -9.546, -1.01, -0.365, 0.508, 5.416, -2.083, 1.181, 5.57, 2.605, -3.639, -8.581, 7.237, 7.331, 8.785]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 9784.337498360266\n",
      "Model: [422.65, 13.396, 2.238, -2.351, -3.533, -21.164, -17.852, 1.995, -16.744, 0.607, 41.639, 38.241, 9.667, -28.709, -14.616, -5.903, -21.986, -10.592, 19.604, 8.886, -6.912, -0.913, 10.069, 0.68, -6.099, -26.153, -1.767, -9.692, -5.155, -27.897, -29.042, -4.845, 0.559, 6.847, -24.774, -0.692, -15.053, -2.675, 0.909, 6.069, 6.389, -17.565, 11.342, 10.456, -6.242, -14.555, 3.282, 10.449, 8.875, -2.345, 8.68, 3.338, -2.712, -12.455, 0.783, 4.093, -8.014, 7.209, -3.063, -19.416, 2.521, 7.645, -17.71, -1.874, -6.659, 8.105, 1.788, 2.684, 2.677, 35.488, 13.431, -1.374, 7.517, -0.233, 23.49, -0.795, -0.825, -1.561, -11.609, 0.219, -9.216, -0.847, -10.301, 2.798, 43.316, 3.054, -0.188, -2.341, -5.509, 2.83, -1.077, -3.699, -15.022, 1.566, -1.513, 3.852, 0.859, 2.755, -28.523, -0.395, 2.378, 15.219, 3.086, 5.768, -8.748, -2.06, 32.78, -3.755, 7.189, 6.56, -7.576, 8.511, 7.669, 8.271, -9.933, -1.071, 4.131, 0.322, -0.72, -13.528, 0.126, -11.088, 6.212, -0.268, -4.533, -3.721, 0.426, 15.061, -8.257, -8.802, -4.689, 0.86, -4.248, -1.404, -1.461, -15.765, -1.904, -4.415, -6.945, -2.036, -3.802, 1.639, 0.37, -6.708, -1.994, 1.0, 1.595, -2.72, -9.13, -7.461, -2.874, -4.945, -8.068, -0.65, -1.584, 12.142, 5.521, 5.926, -17.208, 0.972, -7.25, 3.217, 6.54, 4.783, 4.072, 5.089, 7.527, 13.352, 1.364, -8.431, -3.157, -8.635, -6.474, -3.465, 19.176, -1.437, 3.728, -8.168, 3.782, 13.075, -2.511, -0.31, -7.592, 13.575, -4.916, -3.541, -7.477, 4.605, 10.959, -6.309, 0.528, -1.467, 0.038, -0.142, -1.59, 6.02, -1.744, -4.551, -2.227, -8.192, -8.742, -7.438, -4.556, -8.203, -5.577, 11.619, 11.321, 11.492, -1.649, 3.765, -6.416, -0.699, 0.118, 20.141, 20.474, 20.373, 20.972, 10.835, -0.546, -6.084, 11.302, 0.336, -3.416, 6.269, 6.512, 11.77, 19.08, 6.436, 1.446, 0.229, 1.04, -6.208, -5.43, -0.467, 3.446, -3.652, 12.435, -6.616, -0.658, -6.26, 8.213, -10.453, -9.85, -7.749, 4.109, 48.25, 49.843, 49.345, 49.503, 48.405, 39.615, 3.335, -3.328, 4.043, -11.069, -9.708, -1.184, -0.524, 0.333, 5.38, -2.247, 1.153, 5.54, 2.572, -3.658, -8.709, 7.217, 7.304, 8.771]\n",
      "----------\n",
      "STEP: 6\n",
      "Loss: 9621.770420236842\n",
      "Model: [423.312, 13.356, 2.071, -2.323, -2.449, -21.032, -15.635, 1.968, -16.647, 0.544, 41.241, 38.028, 9.594, -26.865, -14.448, -5.033, -21.747, -10.284, 19.517, 8.707, -6.7, -0.377, 9.619, 0.803, -5.839, -25.895, -1.484, -9.756, -4.944, -27.639, -29.183, -4.737, 0.511, 6.409, -24.516, -0.534, -14.766, -2.351, 1.146, 6.118, 6.534, -17.638, 11.177, 10.276, -5.701, -14.422, 3.237, 10.283, 8.71, -2.604, 8.515, 3.233, -2.571, -12.47, 0.653, 3.99, -8.132, 7.155, -3.124, -18.387, 2.418, 7.598, -17.355, -1.982, -6.777, 7.975, 1.596, 2.593, 2.588, 34.999, 13.309, -1.229, 7.708, -0.099, 23.248, -0.857, -0.998, -1.751, -11.158, 0.158, -9.323, -0.908, -10.447, 2.931, 43.069, 2.852, -0.236, -2.409, -5.669, 2.68, -1.08, -3.996, -15.097, 1.456, -1.615, 3.794, 0.748, 2.653, -27.28, -0.497, 2.277, 15.077, 3.007, 5.706, -8.798, -2.125, 32.639, -3.819, 7.395, 6.5, -7.672, 8.451, 7.608, 8.21, -10.03, -0.954, 4.057, 0.229, -0.822, -13.744, 0.003, -11.143, 6.109, 0.117, -4.643, -3.78, 0.338, 14.954, -8.297, -8.895, -4.765, 0.732, -4.332, -1.518, -1.523, -15.48, -2.021, -4.491, -7.043, -2.141, -3.893, 1.497, 1.135, -6.764, -2.187, 1.159, 1.515, -2.77, -9.2, -7.542, -2.951, -5.03, -8.11, -0.732, -1.668, 12.078, 5.445, 5.849, -17.277, 0.909, -7.345, 3.152, 6.475, 4.718, 4.007, 5.024, 7.412, 13.268, 1.265, -8.509, -3.247, -8.728, -6.573, -3.555, 19.061, -1.483, 3.584, -8.24, 3.725, 13.431, -2.562, -0.372, -7.648, 13.449, -5.035, -3.6, -7.539, 4.555, 10.887, -6.387, 0.459, -1.536, -0.031, -0.21, -1.658, 5.96, -1.842, -4.605, -2.18, -8.21, -8.76, -7.456, -4.534, -8.221, -5.586, 11.52, 11.199, 11.37, -1.7, 3.651, -6.557, -0.75, 0.04, 20.083, 20.415, 20.315, 20.914, 10.734, -0.611, -6.137, 11.26, 1.021, -3.459, 6.207, 6.441, 11.702, 19.038, 6.397, 1.392, 0.165, 0.976, -6.264, -5.545, -0.586, 3.398, -3.711, 12.378, -6.7, -0.75, -6.337, 8.156, -10.51, -9.907, -7.806, 4.059, 48.19, 49.781, 49.285, 49.443, 48.345, 39.525, 3.287, -3.366, 4.001, -11.127, -9.766, -1.247, -0.58, 0.271, 5.367, -2.305, 1.144, 5.529, 2.56, -3.665, -8.755, 7.21, 7.294, 8.766]\n",
      "----------\n",
      "STEP: 7\n",
      "Loss: 9520.971005689567\n",
      "Model: [423.812, 13.355, 1.939, -2.291, -1.31, -20.967, -14.28, 1.978, -16.492, 0.544, 40.883, 37.883, 9.539, -25.041, -14.243, -4.13, -21.555, -10.042, 19.451, 8.47, -6.541, 0.037, 9.45, 1.013, -5.643, -25.69, -1.27, -9.911, -4.785, -27.433, -29.289, -4.716, 0.528, 6.172, -24.31, -0.327, -14.44, -2.012, 1.37, 6.297, 6.629, -17.631, 11.052, 10.139, -5.101, -14.34, 3.104, 10.158, 8.585, -2.8, 8.39, 3.073, -2.612, -12.623, 0.573, 3.913, -8.22, 7.21, -3.17, -17.625, 2.341, 7.562, -17.07, -2.064, -6.865, 7.877, 1.309, 2.524, 2.521, 34.541, 13.218, -1.169, 7.722, -0.201, 23.065, -0.903, -1.129, -1.894, -10.988, 0.112, -9.405, -0.954, -10.558, 3.149, 42.883, 2.699, -0.273, -2.46, -5.79, 2.567, -1.014, -4.219, -15.092, 1.373, -1.692, 3.749, 0.665, 2.577, -26.504, -0.574, 2.2, 14.97, 2.947, 5.631, -8.837, -2.173, 32.533, -3.867, 7.646, 6.454, -7.745, 8.405, 7.563, 8.164, -10.104, -0.907, 4.0, 0.135, -0.898, -13.92, -0.089, -11.185, 6.031, 0.017, -4.747, -3.825, 0.271, 14.873, -8.327, -8.966, -4.822, 0.636, -4.395, -1.604, -1.57, -15.156, -2.109, -4.548, -7.116, -2.22, -3.961, 1.389, 1.075, -6.806, -2.333, 1.368, 1.455, -2.808, -9.252, -7.603, -3.01, -5.093, -8.141, -0.793, -1.732, 12.03, 5.387, 5.792, -17.329, 0.861, -7.416, 3.102, 6.425, 4.669, 3.957, 4.974, 7.304, 13.204, 1.19, -8.568, -3.315, -8.798, -6.649, -3.623, 18.975, -1.519, 3.475, -8.294, 3.682, 13.806, -2.6, -0.419, -7.69, 13.354, -5.124, -3.644, -7.586, 4.517, 10.832, -6.446, 0.407, -1.588, -0.083, -0.262, -1.709, 5.915, -1.916, -4.675, -2.211, -8.262, -8.813, -7.509, -4.572, -8.274, -5.636, 11.445, 11.106, 11.277, -1.738, 3.566, -6.663, -0.789, -0.019, 20.039, 20.372, 20.271, 20.87, 10.658, -0.66, -6.176, 11.228, 1.727, -3.492, 6.161, 6.387, 11.652, 19.007, 6.367, 1.351, 0.116, 0.927, -6.307, -5.633, -0.676, 3.361, -3.757, 12.334, -6.763, -0.819, -6.396, 8.112, -10.553, -9.95, -7.85, 4.021, 48.145, 49.735, 49.24, 49.398, 48.3, 39.458, 3.25, -3.395, 3.97, -11.171, -9.81, -1.294, -0.623, 0.224, 5.357, -2.349, 1.136, 5.52, 2.551, -3.671, -8.789, 7.205, 7.287, 8.762]\n",
      "----------\n",
      "STEP: 8\n",
      "Loss: 9469.944393148617\n",
      "Model: [424.115, 13.408, 1.861, -2.264, -0.843, -20.882, -12.893, 2.031, -16.44, 0.594, 40.634, 37.802, 9.505, -23.714, -13.979, -3.69, -21.439, -9.895, 19.411, 8.325, -6.444, 0.287, 9.216, 1.328, -5.524, -25.565, -1.141, -9.923, -4.688, -27.308, -29.354, -4.798, 0.442, 5.99, -24.185, -0.06, -14.068, -1.807, 1.425, 6.443, 6.53, -17.583, 10.976, 10.057, -4.687, -14.446, 2.847, 10.083, 8.509, -2.919, 8.314, 2.847, -2.681, -12.716, 0.488, 3.866, -8.274, 7.2, -3.199, -17.409, 2.294, 7.54, -16.693, -2.113, -6.919, 7.818, 1.097, 2.483, 2.48, 34.264, 13.162, -1.024, 7.575, -0.262, 22.953, -0.931, -1.208, -1.982, -10.754, 0.084, -9.454, -0.981, -10.625, 3.471, 42.77, 2.606, -0.295, -2.492, -5.864, 2.498, -0.865, -4.355, -15.234, 1.323, -1.739, 3.723, 0.615, 2.53, -26.509, -0.621, 2.153, 14.905, 2.91, 5.587, -8.86, -2.202, 32.469, -3.896, 7.952, 6.426, -7.789, 8.377, 7.535, 8.136, -10.149, -0.773, 3.966, 0.08, -0.945, -14.026, -0.146, -11.21, 5.984, -0.044, -4.809, -3.852, 0.231, 14.824, -8.346, -9.008, -4.857, 0.578, -4.433, -1.657, -1.598, -14.786, -2.163, -4.582, -7.161, -2.268, -4.002, 1.324, 1.039, -6.831, -2.421, 1.635, 1.419, -2.831, -9.284, -7.64, -3.045, -5.132, -8.16, -0.831, -1.77, 12.001, 5.352, 5.756, -17.361, 0.832, -7.459, 3.073, 6.395, 4.639, 3.928, 4.944, 7.24, 13.166, 1.145, -8.604, -3.356, -8.84, -6.694, -3.664, 18.923, -1.54, 3.409, -8.327, 3.655, 14.224, -2.624, -0.447, -7.715, 13.296, -5.178, -3.671, -7.614, 4.494, 10.8, -6.482, 0.375, -1.62, -0.115, -0.294, -1.739, 5.887, -1.961, -4.716, -2.225, -8.292, -8.843, -7.538, -4.591, -8.304, -5.663, 11.4, 11.05, 11.221, -1.762, 3.514, -6.728, -0.812, -0.054, 20.012, 20.345, 20.244, 20.843, 10.611, -0.689, -6.2, 11.208, 2.458, -3.512, 6.133, 6.355, 11.621, 18.987, 6.349, 1.327, 0.087, 0.897, -6.333, -5.686, -0.73, 3.339, -3.784, 12.308, -6.802, -0.861, -6.431, 8.086, -10.579, -9.976, -7.876, 3.998, 48.118, 49.707, 49.212, 49.37, 48.272, 39.417, 3.227, -3.412, 3.951, -11.197, -9.836, -1.322, -0.649, 0.195, 5.351, -2.376, 1.132, 5.515, 2.545, -3.674, -8.81, 7.202, 7.282, 8.76]\n",
      "----------\n",
      "STEP: 9\n",
      "Loss: 9439.686465983483\n",
      "Model: [424.266, 13.378, 1.822, -2.267, -0.897, -20.756, -11.641, 2.006, -16.521, 0.566, 40.691, 37.767, 9.489, -23.765, -13.668, -3.721, -21.515, -9.937, 19.391, 8.253, -6.396, 0.321, 8.931, 1.486, -5.464, -25.631, -1.076, -10.033, -4.64, -27.373, -29.385, -4.718, 0.168, 5.898, -24.25, 0.254, -14.104, -1.801, 1.26, 6.516, 6.48, -17.55, 10.938, 10.015, -4.48, -14.301, 2.619, 10.045, 8.471, -2.978, 8.276, 2.569, -2.724, -12.762, 0.354, 3.843, -8.301, 7.09, -3.213, -17.131, 2.271, 7.529, -16.504, -2.138, -6.946, 7.788, 0.895, 2.462, 2.46, 34.215, 13.134, -0.814, 7.503, -0.293, 22.898, -0.945, -1.247, -2.025, -10.468, 0.071, -9.478, -0.995, -10.659, 3.392, 42.713, 2.56, -0.306, -2.507, -5.9, 2.464, -0.651, -4.423, -15.293, 1.298, -1.763, 3.709, 0.59, 2.507, -26.102, -0.644, 2.13, 14.873, 2.892, 5.869, -8.872, -2.217, 32.436, -3.911, 8.299, 6.412, -7.811, 8.363, 7.521, 8.122, -10.171, -0.84, 3.949, 0.3, -0.968, -13.937, -0.174, -11.222, 5.96, -0.075, -4.616, -3.865, 0.211, 14.8, -8.355, -9.03, -4.874, 0.549, -4.452, -1.683, -1.612, -14.822, -2.19, -4.6, -7.183, -2.291, -4.023, 1.291, 1.021, -6.844, -2.465, 1.589, 1.4, -2.843, -9.3, -7.659, -3.063, -5.151, -8.17, -0.849, -1.789, 11.987, 5.335, 5.739, -17.377, 0.818, -7.481, 3.058, 6.381, 4.624, 3.913, 4.93, 7.427, 13.146, 1.122, -8.622, -3.375, -8.859, -6.715, -3.683, 18.896, -1.551, 3.377, -8.344, 3.642, 14.674, -2.635, -0.461, -7.728, 13.267, -5.205, -3.685, -7.628, 4.483, 10.783, -6.5, 0.36, -1.635, -0.131, -0.31, -1.755, 5.873, -1.984, -4.412, -1.518, -7.88, -8.431, -7.126, -4.014, -7.892, -5.222, 11.377, 11.022, 11.193, -1.773, 3.488, -6.76, -0.824, -0.072, 19.999, 20.332, 20.231, 20.83, 10.588, -0.704, -6.212, 11.199, 2.438, -3.522, 6.119, 6.339, 11.605, 18.978, 6.34, 1.314, 0.072, 0.883, -6.346, -5.712, -0.756, 3.328, -3.798, 12.295, -6.821, -0.882, -6.449, 8.073, -10.592, -9.989, -7.889, 3.986, 48.104, 49.693, 49.199, 49.356, 48.258, 39.396, 3.216, -3.421, 3.941, -11.211, -9.85, -1.337, -0.662, 0.181, 5.348, -2.389, 1.13, 5.513, 2.542, -3.675, -8.821, 7.2, 7.28, 8.759]\n",
      "----------\n",
      "STEP: 10\n",
      "Loss: 9428.203998984669\n",
      "Model: [424.367, 13.358, 1.795, -2.27, -0.97, -20.62, -10.38, 1.98, -16.602, 0.557, 40.73, 37.715, 9.477, -23.826, -13.341, -3.742, -21.476, -9.889, 19.378, 8.205, -6.364, 0.404, 8.63, 1.672, -5.425, -25.589, -1.033, -10.107, -4.608, -27.331, -29.403, -4.664, -0.025, 5.767, -24.208, 0.582, -14.128, -1.734, 1.143, 6.632, 6.38, -17.466, 10.913, 9.988, -4.251, -14.203, 2.46, 10.019, 8.446, -3.018, 8.25, 2.274, -2.814, -12.793, 0.258, 3.827, -8.319, 7.018, -3.222, -16.852, 2.255, 7.522, -16.292, -2.154, -6.964, 7.768, 0.752, 2.448, 2.446, 34.123, 13.116, -0.582, 7.456, -0.314, 22.861, -0.954, -1.27, -2.054, -10.167, 0.061, -9.495, -1.005, -10.681, 3.338, 42.676, 2.529, -0.313, -2.517, -5.925, 2.441, -0.416, -4.468, -15.34, 1.281, -1.778, 3.7, 0.573, 2.491, -26.111, -0.659, 2.115, 14.851, 2.88, 5.853, -8.879, -2.227, 32.415, -3.92, 8.661, 6.403, -7.825, 8.354, 7.512, 8.113, -10.186, -0.885, 3.938, 0.281, -0.984, -13.972, -0.192, -11.231, 5.944, -0.095, -4.637, -3.874, 0.197, 14.783, -8.361, -9.044, -4.886, 0.529, -4.465, -1.7, -1.622, -14.846, -2.208, -4.611, -7.198, -2.307, -4.036, 1.27, 1.009, -6.852, -2.495, 1.558, 1.388, -2.85, -9.31, -7.671, -3.075, -5.164, -8.176, -0.862, -1.802, 11.977, 5.323, 5.727, -17.388, 0.808, -7.495, 3.048, 6.371, 4.614, 3.903, 4.92, 7.405, 13.133, 1.107, -8.634, -3.383, -8.868, -6.726, -3.692, 18.879, -1.558, 3.355, -8.355, 3.634, 15.136, -2.643, -0.471, -7.737, 13.248, -5.223, -3.694, -7.638, 4.475, 10.772, -6.512, 0.349, -1.646, -0.141, -0.32, -1.765, 5.864, -1.999, -4.426, -1.524, -7.891, -8.441, -7.137, -4.022, -7.902, -5.232, 11.362, 11.003, 11.174, -1.781, 3.47, -6.782, -0.832, -0.084, 19.99, 20.323, 20.222, 20.821, 10.573, -0.714, -6.22, 11.192, 2.425, -3.528, 6.109, 6.328, 11.595, 18.971, 6.334, 1.306, 0.062, 0.873, -6.354, -5.73, -0.771, 3.321, -3.807, 12.286, -6.834, -0.896, -6.461, 8.064, -10.601, -9.998, -7.898, 3.979, 48.095, 49.684, 49.19, 49.347, 48.249, 39.383, 3.209, -3.427, 3.935, -11.219, -9.859, -1.346, -0.671, 0.171, 5.346, -2.398, 1.128, 5.511, 2.541, -3.676, -8.828, 7.199, 7.279, 8.758]\n"
     ]
    }
   ],
   "source": [
    "!python algorithmImplementation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ImportError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "Uploading pyspark file to GCS\n",
      "featengcluster - RUNNING\n",
      "Submitted job ID 387359e2-6961-4429-b516-d455f969d980\n",
      "Waiting for job to finish...\n",
      "Job finished.\n",
      "Downloading output file\n",
      "Received job output b\"Ivy Default Cache set to: /root/.ivy2/cache\\nThe jars for the packages stored in: /root/.ivy2/jars\\n:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\\ncom.databricks#spark-xml_2.11 added as a dependency\\ngraphframes#graphframes added as a dependency\\ncom.databricks#spark-avro_2.11 added as a dependency\\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\\n\\tconfs: [default]\\n\\tfound com.databricks#spark-xml_2.11;0.4.1 in central\\n\\tfound graphframes#graphframes;0.5.0-spark2.1-s_2.11 in spark-packages\\n\\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\\n\\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\\n\\tfound org.scala-lang#scala-reflect;2.11.0 in central\\n\\tfound org.slf4j#slf4j-api;1.7.7 in central\\n\\tfound com.databricks#spark-avro_2.11;4.0.0 in central\\n\\tfound org.apache.avro#avro;1.7.6 in central\\n\\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\\n\\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\\n\\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\\n\\tfound org.xerial.snappy#snappy-java;1.0.5 in central\\n\\tfound org.apache.commons#commons-compress;1.4.1 in central\\n\\tfound org.tukaani#xz;1.0 in central\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.4.1/spark-xml_2.11-0.4.1.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-xml_2.11;0.4.1!spark-xml_2.11.jar (32ms)\\ndownloading http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar ...\\n\\t[SUCCESSFUL ] graphframes#graphframes;0.5.0-spark2.1-s_2.11!graphframes.jar (225ms)\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-avro_2.11;4.0.0!spark-avro_2.11.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2!scala-logging-api_2.11.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2!scala-logging-slf4j_2.11.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.0/scala-reflect-2.11.0.jar ...\\n\\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.0!scala-reflect.jar (54ms)\\ndownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar ...\\n\\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.7!slf4j-api.jar (13ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.6/avro-1.7.6.jar ...\\n\\t[SUCCESSFUL ] org.apache.avro#avro;1.7.6!avro.jar(bundle) (13ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (14ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (15ms)\\ndownloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\\n\\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar ...\\n\\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.0.5!snappy-java.jar(bundle) (15ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\\n\\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (14ms)\\ndownloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\\n\\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (12ms)\\n:: resolution report :: resolve 4008ms :: artifacts dl 463ms\\n\\t:: modules in use:\\n\\tcom.databricks#spark-avro_2.11;4.0.0 from central in [default]\\n\\tcom.databricks#spark-xml_2.11;0.4.1 from central in [default]\\n\\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\\n\\tgraphframes#graphframes;0.5.0-spark2.1-s_2.11 from spark-packages in [default]\\n\\torg.apache.avro#avro;1.7.6 from central in [default]\\n\\torg.apache.commons#commons-compress;1.4.1 from central in [default]\\n\\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\\n\\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\\n\\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\\n\\torg.slf4j#slf4j-api;1.7.7 from central in [default]\\n\\torg.tukaani#xz;1.0 from central in [default]\\n\\torg.xerial.snappy#snappy-java;1.0.5 from central in [default]\\n\\t:: evicted modules:\\n\\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\torg.slf4j#slf4j-api;1.6.4 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\t---------------------------------------------------------------------\\n\\t|                  |            modules            ||   artifacts   |\\n\\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\\n\\t---------------------------------------------------------------------\\n\\t|      default     |   16  |   14  |   14  |   2   ||   14  |   14  |\\n\\t---------------------------------------------------------------------\\n:: retrieving :: org.apache.spark#spark-submit-parent\\n\\tconfs: [default]\\n\\t14 artifacts copied, 0 already retrieved (7998kB/25ms)\\n18/12/08 01:49:37 INFO org.spark_project.jetty.util.log: Logging initialized @7368ms\\n18/12/08 01:49:37 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT\\n18/12/08 01:49:37 INFO org.spark_project.jetty.server.Server: Started @7471ms\\n18/12/08 01:49:37 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@4d0f9577{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n18/12/08 01:49:37 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.10-hadoop2\\n18/12/08 01:49:38 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at featengcluster-m/10.128.0.3:8032\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.databricks_spark-xml_2.11-0.4.1.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.tukaani_xz-1.0.jar added multiple times to distributed cache.\\n18/12/08 01:49:41 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1544233642029_0001\\n18/12/08 01:49:58 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\\n\\r[Stage 1:>                                                          (0 + 2) / 2]\\r[Stage 1:=============================>                             (1 + 1) / 2]\\r[Stage 2:>                                                        (0 + 6) / 200]\\r[Stage 2:=======>                                                (26 + 6) / 200]\\r[Stage 2:===============>                                        (57 + 6) / 200]\\r[Stage 2:====================>                                   (74 + 7) / 200]\\r[Stage 2:===========================>                            (99 + 4) / 200]\\r[Stage 2:===============================>                       (113 + 4) / 200]\\r[Stage 2:===================================>                   (130 + 4) / 200]\\r[Stage 2:=========================================>             (152 + 4) / 200]\\r[Stage 2:==============================================>        (168 + 4) / 200]\\r[Stage 2:====================================================>  (190 + 4) / 200]\\r                                                                                \\r\\r[Stage 4:==============================>                        (111 + 4) / 200]\\r[Stage 4:========================================>              (146 + 4) / 200]\\r[Stage 4:==================================================>    (184 + 4) / 200]\\r                                                                                \\r\\r[Stage 6:=====================>                                  (76 + 4) / 200]\\r[Stage 6:==============================>                        (111 + 4) / 200]\\r[Stage 6:=========================================>             (152 + 4) / 200]\\r[Stage 6:====================================================>  (192 + 4) / 200]\\r                                                                                \\r\\r[Stage 8:============================>                          (105 + 4) / 200]\\r[Stage 8:=======================================>               (142 + 4) / 200]\\r[Stage 8:====================================================>  (191 + 4) / 200]\\r                                                                                \\r\\r[Stage 14:============================================>         (164 + 4) / 200]\\r                                                                                \\r\\r[Stage 20:===========================================>          (162 + 4) / 200]\\r                                                                                \\r\\r[Stage 26:============================================>         (164 + 4) / 200]\\r                                                                                \\r\\r[Stage 53:=============================>                            (1 + 1) / 2]\\r                                                                                \\r\\r[Stage 54:>                                                         (0 + 2) / 2]\\r[Stage 54:=============================>                            (1 + 1) / 2]\\r                                                                                \\r\\r[Stage 55:>                                                         (0 + 2) / 2]\\r[Stage 55:=============================>                            (1 + 1) / 2]\\r                                                                                \\r\\r[Stage 56:>                                                         (0 + 2) / 2]\\r[Stage 56:=============================>                            (1 + 1) / 2]\\r                                                                                \\r[(['1', '31', '17', '12', '2826', '351', '30', '13', '138', '1', '4', '0', '12', 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '751', '4', '4', '7788', '45', '3', '2', '75', '1', '3', '0', '4', 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['0', '1', '1', '22', '1910', '200', '5', '25', '149', '0', '1', '0', '29', 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '2', '29', '12', '17435', '1365', '1', '14', '191', '1', '1', '0', '14', 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '184', '4', '4', '2715', '31', '0', '4', '4', '1', '0', '0', '4', 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0)]\\n18/12/08 01:51:46 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@4d0f9577{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n\"\n",
      "Tearing down cluster\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=featengcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=featureEngineering.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
