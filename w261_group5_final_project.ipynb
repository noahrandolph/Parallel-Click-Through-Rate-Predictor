{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "W261 Machine Learning at Scale<br>\n",
    "12 December 2018\n",
    "\n",
    "Wei Wang;\n",
    "Alice Lam;\n",
    "John Tabbone;\n",
    "Noah Randolph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Formulation\n",
    "\n",
    "Alice Lam: \"enhancing CTR means improved monetization of your current traffic (eyeballs/views). The algorithm to predict CTR accurately is useful for the platform to show specific ads to specific people who would have the highest CTR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting toyDataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile toyDataset.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SEED = 2615\n",
    "NUMERICCOLS = 2\n",
    "ONEHOTCOLS = 2\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def generateToyDataset(w=[8, -3, -1, 3, 8]):\n",
    "    '''generate toy logistic regression dataset with numerical and 1-hot encoded features'''\n",
    "    nrows=8\n",
    "    np.random.seed(SEED)\n",
    "    x1 = np.random.randint(0, 10, nrows)\n",
    "    x2 = np.random.randint(0, 10, nrows)\n",
    "    x3 = np.random.randint(0, 2, nrows) # simulate 1-hot\n",
    "    x4 = np.ones(nrows, np.int8) - x3   # with x3 and x4\n",
    "    noise = np.random.normal(5, 1, nrows)\n",
    "    v = (w[0] + x1*w[1] + x2*w[2] + x3*w[3] + x4*w[4] + noise)\n",
    "    y = (v>0) * 2 - 1 # y = 1 or -1\n",
    "    df = spark.createDataFrame(zip(y.tolist(), x1.tolist(), x2.tolist(), x3.tolist(), x4.tolist()))\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,2)]+['C{}'.format(i) for i in range(0,2)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    return df\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0])) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    new_model = W - learningRate * grad \n",
    "    return new_model\n",
    "\n",
    "\n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + [row['C{}'.format(i)] for i in range(0, ONEHOTCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data around the mean of each feature.\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStdev, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "# create a toy dataset that includes 1-hot columns for development\n",
    "df = generateToyDataset()   \n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = df.rdd.map(dfToRDD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# normalize RDD\n",
    "normedRDDcached = normalize(trainRDD).cache()\n",
    "print(normedRDDcached.take(1))\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(normedRDDcached.take(1)[0][0])\n",
    "wInitial = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# 1 iteration of gradient descent\n",
    "w = GDUpdate(normedRDDcached, wInitial)\n",
    "\n",
    "nSteps = 5\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    w = GDUpdate(normedRDDcached, w)\n",
    "    loss = logLoss(normedRDDcached, w)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(i,3) for i in w]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 22:39:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-05 22:39:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-12-05 22:39:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "[(array([-1.63525964,  0.62123652,  1.        , -1.        ]), 1)]\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 7.7014879401802405\n",
      "Model: [1.101, -0.162, -1.391, -0.468, -0.797]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 6.1460241873746195\n",
      "Model: [0.865, -0.478, -1.282, -0.469, -0.795]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 5.006600355698076\n",
      "Model: [0.66, -0.743, -1.182, -0.491, -0.774]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 4.183106733189442\n",
      "Model: [0.485, -0.964, -1.092, -0.52, -0.744]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 3.591671886745936\n",
      "Model: [0.337, -1.15, -1.016, -0.55, -0.714]\n"
     ]
    }
   ],
   "source": [
    "!python toyDataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA & Discussion of Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting loadAndEDA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loadAndEDA.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.functions import udf, col, countDistinct, isnan, when, count, desc\n",
    "import pandas as pd\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "MINICLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "MINILOCALPATH = 'data/train_005.txt'\n",
    "\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"loadAndEDA\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; MINICLOUDPATH; MINILOCALPATH\n",
    "    df = spark.read.csv(path=MINILOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label']+['I{}'.format(i) for i in range(0,13)]+['C{}'.format(i) for i in range(0,26)]\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def displayHead(df, n=5):\n",
    "    '''returns head of the training dataset'''\n",
    "    return df.head(n)\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''returns approximate median values of the columns given, with null values ignored'''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "def getDescribe(df, cols):\n",
    "    return df.select(cols).describe().show()\n",
    "\n",
    "def getDistinctCount(df, cols):\n",
    "    return df.agg(*(countDistinct(col(c)).alias(c) for c in cols)).show()\n",
    "\n",
    "def checkNA(df, cols):\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()\n",
    "\n",
    "def getCorrMatrix(df, cols):\n",
    "    df = df.select(cols)\n",
    "    col_names = df.columns\n",
    "    features = df.rdd.map(lambda row: row[0:])\n",
    "    corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "    corr_df = pd.DataFrame(corr_mat)\n",
    "    corr_df.index, corr_df.columns = col_names, col_names\n",
    "    return corr_df\n",
    "    \n",
    "    \n",
    "df = loadData().cache()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "#print(\"\\nTEST DATASET ROW COUNTS: \", testDf.count())\n",
    "#print(\"\\nTRAIN DATASET ROW COUNTS: \", trainDf.count())\n",
    "## print(\"HEAD\\n\", displayHead(trainDf))\n",
    "#print(\"\\nCOLUMN TYPES\\n\", df.dtypes)\n",
    "#print(\"\\nMEDIAN OF NUMERIC COLUMNS\\n\", getMedians(trainDf, trainDf.columns[1:14]))\n",
    "\n",
    "#print(\"\\nDESCRIPTIONS OF NUMERICAL COLUMNS 1-7\\n\")\n",
    "#getDescribe(trainDf, trainDf.columns[1:8])\n",
    "#print(\"\\nDESCRIPTIONS OF NUMERICAL COLUMNS 8-14\\n\")\n",
    "#getDescribe(trainDf, trainDf.columns[8:15])\n",
    "#print(\"\\nCOUNTS OF DISTINCT VALUE FOR CATEGORICAL VARIABLE COLUMNS\")\n",
    "#getDistinctCount(trainDf, trainDf.columns[15:])\n",
    "\n",
    "#print(\"\\nCOUNTS OF NAs FOR COLUMN 0 - 19\")\n",
    "#checkNA(trainDf, trainDf.columns[:20])\n",
    "#print(\"\\nCOUNTS OF NAs FOR COLUMN 20 - 39\")\n",
    "#checkNA(trainDf, trainDf.columns[20:])\n",
    "\n",
    "##getCorrMatrix(trainDf, trainDf.columns[1:14]) # This doesn't work if there's NA in there\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 23:32:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-05 23:32:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-12-05 23:32:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2018-12-05 23:32:19 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "2018-12-05 23:32:23 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2018-12-05 23:32:23 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "!python loadAndEDA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'submit_job_to_cluster.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=testcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=row_counts.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from running EDA code above:\n",
    "Main dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 9164811)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 36675806)<br>\n",
    "\n",
    "Toy dataset:<br>\n",
    "('TEST DATASET ROW COUNTS: ', 4578)<br>\n",
    "('TRAIN DATASET ROW COUNTS: ', 18379)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting algorithmImplementation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile algorithmImplementation.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, desc, isnan, when\n",
    "import numpy as np\n",
    "from operator import add\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "MAINCLOUDPATH = 'gs://w261_final_project/train.txt'\n",
    "TOYCLOUDPATH = 'gs://w261_final_project/train_005.txt'\n",
    "TOYLOCALPATH = 'data/train_005.txt'\n",
    "NUMERICCOLS = 13\n",
    "CATEGORICALCOLS = 26\n",
    "NUMERICCOLNAMES = ['I{}'.format(i) for i in range(0,NUMERICCOLS)]\n",
    "CATCOLNAMES = ['C{}'.format(i) for i in range(0,CATEGORICALCOLS)]\n",
    "SEED = 2615\n",
    "\n",
    "\n",
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"featureEngineering\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    '''load the data into a Spark dataframe'''\n",
    "    # select path to data: MAINCLOUDPATH; TOYCLOUDPATH; TOYLOCALPATH\n",
    "    df = spark.read.csv(path=TOYLOCALPATH, sep='\\t')\n",
    "    # change column names\n",
    "    oldColNames = df.columns\n",
    "    newColNames = ['Label'] + NUMERICCOLNAMES + CATCOLNAMES\n",
    "    for oldName, newName in zip(oldColNames, newColNames):\n",
    "        df = df.withColumnRenamed(oldName, newName)\n",
    "    # change int column types to int from string\n",
    "    for col in df.columns[:14]:\n",
    "        df = df.withColumn(col, df[col].cast('int'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def splitIntoTestAndTrain(df):\n",
    "    '''randomly splits 80/20 into training and testing dataframes'''\n",
    "    splits = df.randomSplit([0.2, 0.8], seed=SEED)\n",
    "    testDf = splits[0]\n",
    "    trainDf = splits[1]\n",
    "    return testDf, trainDf\n",
    "\n",
    "\n",
    "def getMedians(df, cols):\n",
    "    '''\n",
    "    returns approximate median values of the columns given, with null values ignored\n",
    "    '''\n",
    "    # 0.5 relative quantile probability and 0.05 relative precision error\n",
    "    return df.approxQuantile(cols, [0.5], 0.05)\n",
    "\n",
    "\n",
    "def getMostFrequentCats(df, cols, n):\n",
    "    '''\n",
    "    returns a dict where the key is the column and value is an ordered list\n",
    "    of the top n categories in that column in descending order\n",
    "    '''\n",
    "    freqCatDict = {col: None for col in df.columns[cols:]}\n",
    "    for col in df.columns[cols:]:\n",
    "        listOfRows = df.groupBy(col).count().sort('count', ascending=False).take(n)\n",
    "        topCats = [row[col] for row in listOfRows]\n",
    "        freqCatDict[col] = topCats[:n]\n",
    "    return freqCatDict\n",
    "    \n",
    "\n",
    "def rareReplacer(df, dictOfMostFreqSets):\n",
    "    '''\n",
    "    Iterates through columns and replaces non-Frequent categories with 'rare' string.\n",
    "    '''\n",
    "    for colName in df.columns[NUMERICCOLS+1:]:\n",
    "        bagOfCats = dictOfMostFreqSets[colName]\n",
    "        df = df.withColumn(colName, udf(lambda x: 'rare' if x not in bagOfCats else x, StringType())(df[colName])).cache()\n",
    "    return df\n",
    "\n",
    "    \n",
    "def dfToRDD(row):\n",
    "    '''\n",
    "    Converts dataframe row to rdd format.\n",
    "        From: DataFrame['Label', 'I0', ..., 'C0', ...]\n",
    "        To:   (features_array, y)\n",
    "    '''    \n",
    "    features_list = [row['I{}'.format(i)] for i in range(0, NUMERICCOLS)] + [row['C{}'.format(i)] for i in range(0, CATEGORICALCOLS)]\n",
    "    features_array = np.array(features_list)\n",
    "    y = row['Label']\n",
    "    return (features_array, y)\n",
    "\n",
    "\n",
    "def emitColumnAndCat(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits a record for each categorical column value along with a zero for one-hot encoding.\n",
    "    The emitted values will become a reference dictionary for one-hot encoding in later steps.\n",
    "        Input: (array([features], dtype='<U21'), 0) or (features, label)\n",
    "        Output: ((categorical column, category), 0) or (complex key, value)\n",
    "    The last zero in the output is for initializing one-hot encoding.\n",
    "    \"\"\"\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        yield ((catColName, element), 0)\n",
    "\n",
    "\n",
    "def oneHotEncoder(line):\n",
    "    \"\"\"\n",
    "    Takes in a row from RDD and emits row where categorical columns are replaced with 1-hot encoded columns.\n",
    "        Input: (numerical and categorical features, label)\n",
    "        Output: (numerical and one-hot encoded categorical features, label)\n",
    "    \"\"\"\n",
    "    oneHotDict = copy.deepcopy(oneHotReference)\n",
    "    elements = line[0][NUMERICCOLS:]\n",
    "    for catColName, element in zip(CATCOLNAMES, elements):\n",
    "        oneHotDict[(catColName, element)] = 1\n",
    "    numericElements = list(line[0][:NUMERICCOLS])\n",
    "    features = np.array(numericElements + [value for key, value in oneHotDict.items()], dtype=np.float)\n",
    "    return (features, line[1])\n",
    "\n",
    "\n",
    "def getMeanAndVar(trainRDD):\n",
    "    \"\"\"\n",
    "    Returns the mean and variance of the training dataset for use in normalizing future records\n",
    "    (e.g. the test set) to be run on model.\n",
    "    \"\"\"\n",
    "    featureMeans = trainRDD.map(lambda x: x[0]).mean()\n",
    "    featureStDevs = np.sqrt(trainRDD.map(lambda x: x[0]).variance())\n",
    "    return featureMeans, featureStDevs\n",
    "    \n",
    "\n",
    "def normalize(dataRDD, featureMeans, featureStDevs):\n",
    "    \"\"\"\n",
    "    Scale and center data around the mean of each feature.\n",
    "    \"\"\"\n",
    "    normedRDD = dataRDD.map(lambda x: ((x[0] - featureMeans)/featureStDevs, x[1]))\n",
    "    return normedRDD\n",
    "\n",
    "\n",
    "def logLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    loss = augmentedData.map(lambda p: (np.log(1 + np.exp(-p[1] * np.dot(W, p[0]))))) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    grad = augmentedData.map(lambda p: (-p[1] * (1 - (1 / (1 + np.exp(-p[1] * np.dot(W, p[0]))))) * p[0])) \\\n",
    "                        .reduce(lambda a, b: a + b)\n",
    "    new_model = W - learningRate * grad \n",
    "    return new_model\n",
    "\n",
    "\n",
    "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, \n",
    "                    learningRate = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of log loss gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps): \n",
    "        model = GDUpdate(trainRDD, model, learningRate)\n",
    "        training_loss = logLoss(trainRDD, model) \n",
    "        test_loss = logLoss(testRDD, model) \n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history\n",
    "\n",
    "\n",
    "\n",
    "# load data\n",
    "df = loadData()\n",
    "testDf, trainDf = splitIntoTestAndTrain(df)\n",
    "testDf.cache()\n",
    "trainDf.cache()\n",
    "\n",
    "# get top n most frequent categories for each column (in training set only)\n",
    "n = 10\n",
    "mostFreqCatDict = getMostFrequentCats(trainDf, NUMERICCOLS+1, n)\n",
    "\n",
    "# get dict of sets of most frequent categories in each column for fast lookups during filtering (in later code)\n",
    "setsMostFreqCatDict = {key: set(value) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get the top category from each column for imputation of missing values (in training set only)\n",
    "fillNADictCat = {key: (value[0] if value[0] is not None else value[1]) for key, value in mostFreqCatDict.items()}\n",
    "\n",
    "# get dict of median numeric values for imputation of missing values (in training set only)\n",
    "fillNADictNum = {key: value for (key, value) in zip(trainDf.columns[1:NUMERICCOLS+1], \n",
    "                                                    [x[0] for x in getMedians(trainDf,\n",
    "                                                                              trainDf.columns[1:NUMERICCOLS+1])])}\n",
    "\n",
    "# impute missing values in training and test set\n",
    "trainDf = trainDf.na.fill(fillNADictNum) \\\n",
    "                 .na.fill(fillNADictCat).cache()\n",
    "testDf = testDf.na.fill(fillNADictNum) \\\n",
    "               .na.fill(fillNADictCat).cache()\n",
    "\n",
    "# replace low-frequency categories with 'rare' string in training and test set\n",
    "trainDf = rareReplacer(trainDf, setsMostFreqCatDict) # df gets cached in function\n",
    "testDf = rareReplacer(testDf, setsMostFreqCatDict) # df gets cached in function\n",
    "\n",
    "# convert dataframe to RDD \n",
    "trainRDD = trainDf.rdd.map(dfToRDD).cache()\n",
    "testRDD = testDf.rdd.map(dfToRDD).cache()\n",
    "        \n",
    "# create and broadcast reference dictionary to be used in constructing 1 hot encoded RDD\n",
    "oneHotReference = trainRDD.flatMap(emitColumnAndCat) \\\n",
    "                          .reduceByKeyLocally(add) # note: only the zero values are being added here (main goal is to output a dictionary)\n",
    "sc.broadcast(oneHotReference)\n",
    "\n",
    "# replace rows with new rows having categorical columns 1-hot encoded\n",
    "trainRDD = trainRDD.map(oneHotEncoder).cache()\n",
    "testRDD = testRDD.map(oneHotEncoder).cache()\n",
    "\n",
    "# normalize RDD\n",
    "featureMeans, featureStDevs = getMeanAndVar(trainRDD)\n",
    "trainRDD = normalize(trainRDD, featureMeans, featureStDevs).cache()\n",
    "testRDD = normalize(testRDD, featureMeans, featureStDevs).cache() # use the mean and st. dev. from trainRDD\n",
    "\n",
    "# create initial weights to train\n",
    "featureLen = len(trainRDD.take(1)[0][0])\n",
    "wInit = np.random.normal(size=featureLen+1) # add 1 for bias\n",
    "\n",
    "# # 1 iteration of gradient descent\n",
    "# w = GDUpdate(trainRDD, wInit)\n",
    "\n",
    "# nSteps = 10\n",
    "# for idx in range(nSteps):\n",
    "#     print(\"----------\")\n",
    "#     print(f\"STEP: {idx+1}\")\n",
    "#     w = GDUpdate(trainRDD, w)\n",
    "#     loss = logLoss(trainRDD, w)\n",
    "#     print(f\"Loss: {loss}\")\n",
    "#     print(f\"Model: {[round(i,3) for i in w]}\")\n",
    "\n",
    "# run 50 iterations\n",
    "start = time.time()\n",
    "logLosstrain, logLosstest, models = GradientDescent(trainRDD, testRDD, wInit, nSteps = 50, verbose = True)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-09 23:56:11 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2018-12-09 23:56:22 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "----------\n",
      "STEP: 1\n",
      "training loss: inf\n",
      "test loss: inf\n",
      "Model: [203.595, 59.047, 21.184, -6.28, -20.14, -18.014, -20.705, 61.376, -5.957, 23.287, 39.628, 93.487, 4.525, -34.819, -9.038, -21.719, -34.23, -55.994, -15.637, 13.909, -62.275, 4.82, 15.977, -38.855, -68.467, -44.268, -54.422, 6.892, 4.857, -55.062, -28.031, -17.61, -33.216, 1.458, -54.287, -23.624, -35.573, -50.275, -2.733, -23.022, 11.849, -2.311, 54.2, 42.465, 10.451, -26.255, 34.814, 62.072, 60.908, -4.732, 62.18, 19.492, 18.881, 44.48, 13.567, -11.129, 41.196, -22.173, -17.378, -25.902, -12.99, -10.669, 4.743, -8.117, -16.825, 30.255, -32.166, 17.701, -6.945, 45.273, 24.845, 19.439, -2.09, 27.195, 5.302, 20.129, 14.353, 8.363, -16.206, 21.677, -14.619, 21.099, 29.066, -10.959, 18.379, 4.432, -9.0, -9.701, 0.863, 16.188, 6.361, -3.922, -20.598, 41.861, 26.757, 19.356, 41.289, 16.097, -14.078, 17.466, 16.106, 20.733, 17.682, 28.902, 4.053, 19.57, 19.185, 11.441, 10.946, -8.353, -3.969, -7.237, -7.109, -9.212, -4.201, -20.203, 2.059, -7.896, -17.466, -6.061, 9.653, -7.706, -8.355, -16.987, -3.532, -12.586, -11.571, -14.162, 3.22, 0.086, -12.451, -14.121, -8.387, -18.332, -4.666, -36.619, 18.873, 0.006, -1.513, -2.207, 22.945, -4.205, -15.16, -11.239, -7.911, 9.259, -11.028, 12.571, 0.937, -1.44, -6.035, 21.607, 0.255, 12.385, 11.189, 44.764, 42.753, 43.391, -9.018, -8.086, -8.79, 9.035, 8.681, 6.435, 7.883, 7.193, -4.681, -4.222, -13.644, 5.936, 52.791, 31.566, 42.885, 52.473, 50.469, -10.541, -6.709, 2.492, 9.184, 8.026, 37.967, -9.688, -10.666, -14.057, 2.076, -15.93, 11.779, 30.728, -14.266, -12.179, -19.604, -20.088, -20.073, -19.081, -7.545, -9.442, -14.475, 3.521, -5.31, 3.119, 6.777, 7.669, 4.573, 5.956, 7.719, 65.24, 77.849, 77.679, -14.56, 44.736, 12.194, -4.099, -6.793, 39.971, 41.908, 38.755, 39.305, 40.455, -17.854, -2.775, 1.974, -10.314, -3.686, 21.059, 20.313, 3.068, 2.476, -8.185, -2.475, -6.341, -4.935, -9.792, -17.237, -12.023, 9.085, 22.394, 19.549, 0.418, 35.745, 1.602, -2.059, -1.199, -0.707, -3.184, 14.823, -19.192, -16.264, -17.952, -16.316, -16.941, -15.15, 2.134, -0.504, 3.444, -13.031, -14.18, -9.956, -12.573, -9.574, 10.972, 36.145, -2.519, 2.1, 6.027, -2.913, 32.296, 6.831, 10.214, -1.287]\n",
      "----------\n",
      "STEP: 2\n",
      "training loss: inf\n",
      "test loss: inf\n",
      "Model: [438.06, 42.135, 16.265, 13.031, -39.234, -41.652, -38.589, 34.828, -41.971, -9.194, 36.828, 68.1, 1.183, -66.471, -7.198, -11.368, 24.519, 25.611, -1.904, 21.527, -3.672, 6.446, 41.147, -15.496, 7.745, 25.15, 14.702, -15.348, 23.706, 16.952, -42.816, 12.364, -2.232, 3.737, 17.727, -5.882, -58.465, 26.827, -11.833, 2.773, 5.638, 7.592, -4.297, -17.585, 2.426, -20.281, 22.48, 3.402, 2.238, 8.127, 3.51, 2.536, 26.2, -12.392, -4.892, 6.291, 23.649, 14.596, 1.063, -21.108, 4.43, -21.873, -17.607, -9.282, 1.154, -14.677, -6.927, -4.841, -4.36, 61.924, -17.026, 4.378, 17.061, -16.465, 20.806, -1.494, -5.065, -5.755, -41.528, 0.195, -29.425, -0.384, -19.028, -8.661, 39.578, 8.031, -17.928, -12.257, -2.243, 5.769, -14.332, -4.869, -22.284, 2.854, -9.533, -1.334, 2.282, -19.933, -35.793, -18.564, -19.924, -16.935, -8.827, 7.209, -0.745, 7.264, 47.259, 2.473, -6.992, -26.534, -27.37, -25.417, -25.289, -27.393, -28.367, -4.849, 6.125, -8.357, -7.688, -11.359, -1.726, -1.75, -4.09, -15.869, 5.426, -21.176, -15.441, -4.997, 0.462, -2.391, -19.161, 8.437, 8.057, -3.523, -12.644, -53.189, -8.006, -25.847, -30.485, -28.84, -3.772, -4.067, -7.814, -4.599, 5.465, -0.899, -4.248, 2.632, -4.378, -13.453, -16.122, -8.184, -12.848, -16.459, -17.749, 22.312, 15.67, 16.308, -19.151, -8.407, -10.217, -14.086, -14.44, -16.686, -15.238, -15.927, -8.21, 29.356, -1.849, 2.062, 20.893, 5.448, 9.593, 20.575, 39.048, -1.436, -12.1, -8.462, -11.06, 24.88, 18.178, -17.115, -17.451, 19.765, -9.105, -11.813, 10.393, 20.314, 6.894, -14.99, -34.296, -34.78, -34.765, -33.774, -7.537, -0.532, -10.793, -17.225, -7.207, -21.783, -18.125, -17.233, -12.055, -18.945, -15.599, 30.134, 34.444, 34.274, -11.832, 12.335, 6.597, -15.81, -3.248, 19.355, 21.292, 18.139, 18.69, 24.743, 4.545, -8.423, -5.192, -2.641, 23.708, -0.704, -2.947, -20.768, 8.338, 4.799, -20.409, -24.017, -22.612, -7.393, -14.809, -0.673, 7.356, 9.301, -0.77, 0.897, 3.316, -18.957, -6.234, -9.775, -9.283, -7.36, -2.95, 42.899, 46.315, 44.139, 45.775, 45.15, 42.01, 1.274, -8.663, 3.148, -24.372, -25.52, 1.099, -3.281, 1.481, 6.395, 15.456, 0.8, 4.296, 7.325, -5.359, 16.126, 4.385, 6.755, -3.017]\n",
      "----------\n",
      "STEP: 3\n",
      "training loss: inf\n",
      "test loss: inf\n",
      "Model: [478.574, 40.892, 13.305, 11.673, -30.99, -40.988, -34.569, 30.697, -32.622, -8.607, 34.864, 61.859, 0.122, -60.731, -7.472, -14.232, -20.696, -21.366, -3.372, 21.867, -22.661, 4.235, 38.023, -14.219, -14.814, -26.845, -7.36, -7.101, 10.441, -35.147, -45.097, 12.274, 9.009, 21.803, -34.372, -3.488, -51.664, -3.287, 4.966, -15.895, 6.093, 3.5, 26.935, 19.428, 3.324, -18.486, 11.839, 23.417, 22.253, 9.021, 23.525, 1.134, 24.558, -9.52, 13.064, 0.682, 21.692, 8.578, -2.698, -19.045, -1.179, -23.379, -28.146, -7.719, -4.275, -12.703, -6.037, -2.999, -6.022, 56.867, -13.893, -8.942, 5.147, -15.134, 18.798, 3.518, -5.566, -0.948, -38.363, 5.287, -24.371, 4.709, -1.482, -9.817, 30.581, 5.31, -10.65, -9.082, -8.59, 4.482, -12.869, -4.548, -17.2, 21.441, 9.337, 11.081, 20.869, 16.534, -33.167, 17.903, 16.543, 17.929, -4.428, 9.947, 0.206, 6.462, 45.035, 3.808, -1.879, -10.435, -16.331, -9.318, -9.191, -11.294, -17.708, -1.916, 6.042, -1.044, -13.914, -13.246, -4.208, -5.116, -1.188, -11.571, 4.493, -12.41, -9.426, -1.371, -0.36, -5.908, -19.417, 3.868, 3.761, -7.536, -11.058, -51.349, -10.55, 5.949, 1.671, 2.376, -4.797, -3.542, -5.07, -5.61, -1.14, 1.217, -7.433, 0.866, -4.83, 7.317, -3.526, 6.664, -7.363, -7.415, -9.27, 18.433, 11.867, 12.505, -21.473, -3.856, -9.55, 10.59, 10.235, 7.99, 9.437, 8.748, -0.39, 24.199, -5.887, 0.676, 15.381, 1.805, 3.489, 15.063, 34.099, -2.869, -10.288, -11.94, -4.087, 18.52, 14.759, -3.128, -6.118, 12.602, -8.815, -15.413, 9.847, 17.243, 2.497, -8.62, -14.236, -14.719, -14.705, -13.713, -5.695, -1.965, -11.998, 0.943, -8.147, 1.528, 5.185, 6.077, 3.247, 4.365, 7.789, 24.067, 27.502, 27.332, -11.054, 7.188, 5.826, -3.396, -1.962, 15.793, 17.73, 14.577, 15.127, 18.555, 1.631, -11.61, 3.195, -6.439, 21.046, -1.205, 1.186, -14.21, 7.37, 2.386, -0.33, -6.225, -4.819, -10.852, -11.885, -7.4, 9.969, 5.639, 0.37, 1.366, -2.288, -15.984, -6.283, -13.273, -12.781, -9.726, -0.713, 39.224, 42.579, 40.464, 42.1, 41.475, 36.532, -1.707, -5.722, 5.409, -19.895, -21.043, 2.627, -2.052, 3.01, 5.604, 11.881, 0.203, 3.628, 6.593, -5.782, 13.332, 3.962, 6.157, -3.315]\n",
      "----------\n",
      "STEP: 4\n",
      "training loss: 14601.025050084354\n",
      "test loss: 4444.922378438207\n",
      "Model: [502.595, 37.466, 9.659, 12.142, -20.357, -32.955, -29.533, 26.901, -31.657, -8.782, 34.312, 54.966, -0.614, -50.952, -6.956, -12.157, -14.428, -13.517, -0.11, 15.835, -17.066, 3.615, 23.762, -4.262, -10.964, -19.503, -1.792, -1.605, -3.94, -26.922, -43.468, -2.429, -1.803, 8.176, -26.147, -2.895, -38.358, 1.825, -6.018, -2.672, 5.036, -8.646, 20.942, 12.882, 5.411, -11.098, 3.994, 17.406, 16.242, 7.204, 17.514, 0.719, 18.603, -10.081, 1.077, 4.92, 19.174, 4.014, -1.639, -15.428, 3.058, -12.485, -19.473, -6.32, -0.964, 8.509, -6.654, 1.864, -4.927, 48.502, 8.739, -1.968, 13.423, 1.963, 14.783, 2.396, -4.1, -0.699, -24.062, 4.187, -19.157, 3.609, -5.375, -12.858, 26.487, 4.406, -5.543, -6.632, -9.695, 2.755, -9.723, -2.432, -18.78, 17.445, 5.619, 8.961, 16.873, 12.843, -36.078, 14.212, 12.852, 12.799, -3.099, 7.011, -0.318, 5.176, 41.408, 2.542, 5.887, -7.131, -15.584, -6.014, -5.887, -7.99, -17.091, -0.246, 6.025, 4.392, -5.615, -8.352, -3.751, -4.688, 0.913, -5.356, 2.492, -6.674, -2.497, -1.437, -1.824, -6.382, -16.014, 2.35, 1.542, -8.839, -6.867, -38.357, -7.816, 3.209, -1.866, -1.416, -3.585, -3.863, -2.037, -6.431, -1.904, 1.44, -7.791, 2.999, -5.422, 4.384, -6.335, 3.612, -5.667, -8.72, -10.714, 16.132, 12.602, 13.24, -22.065, -5.102, -10.826, 8.221, 7.866, 5.621, 7.068, 6.379, 1.737, 21.141, -6.087, 0.392, 13.61, 0.637, 1.227, 13.292, 30.539, -3.137, -8.752, -13.624, 4.341, 23.32, 12.731, -4.966, -8.362, 10.239, -9.631, -2.803, 10.843, 15.422, 1.803, -8.023, -16.714, -17.198, -17.183, -16.192, -3.292, -0.818, -8.689, 0.452, -9.653, -1.023, 2.634, 3.526, 1.408, 1.814, 5.4, 20.471, 23.055, 22.886, -7.699, 6.071, 1.694, -5.259, -3.056, 13.681, 15.618, 12.465, 13.015, 17.508, 0.11, -13.499, 1.658, -3.433, 19.468, -3.435, 2.382, 7.2, 9.033, 2.639, -2.293, -8.567, -7.161, -9.371, -14.321, -8.34, 8.228, 3.467, 12.202, -0.88, -4.873, -17.069, 6.647, -14.181, -13.688, -5.992, -1.207, 37.045, 40.364, 38.285, 39.921, 39.296, 33.284, -3.475, -5.344, 3.903, -17.399, -18.547, 0.362, -4.11, 0.744, 5.135, 9.761, -0.152, 3.231, 6.159, -6.032, 11.675, 3.711, 5.803, -3.493]\n",
      "----------\n",
      "STEP: 5\n",
      "training loss: 11055.713437862898\n",
      "test loss: 3480.4972745151044\n",
      "Model: [507.746, 36.687, 8.792, 12.119, -14.728, -28.629, -25.961, 25.99, -30.351, -8.237, 34.44, 53.076, -0.827, -45.766, -7.46, -10.259, -15.723, -14.074, -0.015, 15.447, -15.738, 3.841, 19.513, -1.339, -9.235, -20.502, -0.119, -0.768, -2.644, -27.059, -40.649, -1.134, -0.038, 8.459, -26.284, -1.66, -31.642, 1.874, -4.458, -2.481, 5.025, -9.155, 19.657, 13.052, 5.843, -10.326, 1.69, 16.117, 14.953, 6.362, 16.225, -0.76, 17.034, -11.659, 0.893, 4.123, 18.26, 3.6, -2.118, -13.195, 2.261, -11.452, -19.894, -5.902, -1.876, 7.499, -5.757, 1.157, -5.619, 44.989, 7.794, -3.414, 13.487, 0.914, 14.998, 1.921, -5.041, -1.048, -19.804, 3.715, -17.604, 3.137, -5.082, -12.445, 24.809, 4.258, -4.664, -6.261, -10.118, 1.588, -8.905, -2.105, -19.305, 16.588, 4.822, 8.507, 16.016, 12.052, -36.702, 13.421, 12.061, 13.653, -2.875, 6.236, -0.712, 4.677, 40.313, 2.046, 5.242, -3.2, -12.807, -2.084, -1.956, -4.06, -14.385, 0.105, 6.342, 3.427, -6.406, -9.215, -3.093, -5.116, 2.029, -3.799, 1.92, -6.008, -1.66, -0.998, -2.138, -7.106, -14.92, 1.896, 0.893, -9.132, -4.12, -34.274, -7.096, 2.622, -2.624, -2.229, -3.539, -3.512, -0.12, -6.864, -1.173, 0.558, -8.411, 2.607, -4.998, 3.755, -6.937, 2.957, -5.989, -9.353, -11.367, 15.639, 12.007, 12.645, -21.641, -5.595, -11.561, 7.713, 7.358, 5.113, 6.56, 5.871, 1.115, 20.486, -6.179, 0.636, 12.909, -0.083, 0.451, 12.591, 29.648, -2.076, -9.005, -14.184, 3.896, 23.17, 12.297, -5.36, -8.843, 9.803, -10.55, -3.26, 11.441, 15.032, 1.244, -6.056, -13.363, -13.846, -13.831, -12.84, -2.822, -1.283, -8.078, -0.274, -9.976, -1.57, 2.087, 2.979, 1.014, 1.267, 4.888, 19.7, 22.102, 21.932, -6.794, 5.788, 1.099, -5.659, -2.798, 13.228, 15.165, 12.012, 12.562, 16.722, -0.392, -13.904, 1.328, -2.655, 19.13, -3.913, 2.773, 6.677, 8.707, 4.017, -2.714, -9.069, -7.663, -9.811, -14.632, -9.268, 7.855, 3.001, 11.756, -0.732, -5.586, -15.058, 6.199, -14.625, -14.133, -6.44, -1.598, 36.577, 39.889, 37.818, 39.454, 38.829, 32.587, -3.854, -5.639, 3.58, -14.4, -15.548, -0.124, -4.552, 0.259, 5.035, 9.306, -0.228, 3.146, 6.066, -6.086, 11.32, 3.658, 5.727, -3.531]\n",
      "----------\n",
      "STEP: 6\n",
      "training loss: 10189.661486273204\n",
      "test loss: 3186.568099997219\n",
      "Model: [509.859, 36.399, 8.303, 12.109, -11.633, -26.928, -22.51, 25.604, -29.711, -8.357, 34.33, 52.15, -1.06, -42.515, -7.034, -9.186, -15.504, -13.556, -0.293, 15.473, -15.064, 4.117, 17.565, -0.081, -8.765, -20.198, 0.446, -0.63, -1.97, -26.469, -39.141, -0.721, 0.579, 8.008, -25.693, -0.866, -28.147, 2.831, -4.072, -1.878, 4.941, -9.624, 19.129, 12.869, 6.833, -10.067, 0.509, 15.588, 14.424, 6.188, 15.696, -1.689, 16.569, -12.306, 0.109, 3.795, 17.885, 3.53, -2.314, -12.415, 1.934, -10.202, -19.848, -6.247, -2.251, 7.084, -5.972, 0.867, -5.903, 43.454, 7.407, -4.049, 13.998, 0.484, 14.846, 1.726, -5.186, -1.655, -17.851, 3.522, -17.317, 2.943, -5.076, -12.379, 24.02, 4.683, -4.818, -6.479, -10.63, 1.109, -8.16, -2.513, -19.31, 16.236, 4.495, 8.32, 15.664, 11.727, -36.958, 13.096, 11.736, 13.689, -2.285, 5.918, -0.874, 4.472, 39.864, 1.842, 5.047, -3.394, -13.114, -2.277, -2.15, -4.253, -14.696, -0.296, 7.001, 3.03, -6.731, -9.325, -2.927, -5.292, 1.7, -2.718, 1.591, -6.198, -1.254, -0.704, -2.267, -7.403, -15.163, 1.49, 0.627, -9.497, -4.318, -32.118, -6.888, 2.381, -2.935, -2.563, -3.08, -2.508, 1.317, -7.042, -0.673, 0.277, -8.665, 2.446, -4.256, 3.497, -7.185, 2.689, -5.765, -9.429, -11.456, 15.437, 11.762, 12.401, -20.899, -5.798, -11.862, 7.505, 7.15, 4.905, 6.352, 5.663, 0.66, 20.217, -6.496, 0.385, 12.622, -0.378, 0.132, 12.304, 29.283, -0.8, -9.465, -14.414, 3.714, 23.196, 12.118, -5.522, -9.041, 9.946, -10.927, -3.448, 11.244, 14.872, 1.014, -6.115, -12.611, -13.095, -13.08, -12.089, -3.036, -1.474, -7.017, -0.572, -10.109, -1.795, 1.863, 2.755, 0.853, 1.042, 4.677, 19.383, 21.711, 21.541, -5.655, 5.425, 0.649, -5.823, -2.266, 13.042, 14.979, 11.826, 12.377, 16.399, -0.598, -14.071, 1.193, -2.162, 18.991, -4.109, 3.49, 6.462, 8.574, 3.891, -2.887, -9.275, -7.869, -9.991, -14.871, -9.649, 7.702, 2.81, 11.573, -1.0, -5.878, -14.441, 6.015, -14.808, -14.315, -6.624, -1.758, 36.386, 39.694, 37.626, 39.262, 38.637, 32.302, -4.01, -5.76, 3.447, -13.435, -14.583, -0.323, -4.733, 0.059, 4.993, 9.12, -0.259, 3.112, 6.028, -6.108, 11.174, 3.636, 5.696, -3.546]\n",
      "----------\n",
      "STEP: 7\n",
      "training loss: 9797.228369216675\n",
      "test loss: 3047.13409426141\n",
      "Model: [511.247, 36.275, 7.994, 12.122, -9.797, -26.835, -19.83, 25.402, -29.159, -8.421, 33.945, 51.578, -1.214, -40.123, -6.744, -7.963, -15.268, -13.136, -0.476, 15.587, -14.621, 4.069, 16.351, 0.609, -8.771, -19.91, 0.521, -0.385, -1.871, -25.895, -37.968, -0.23, 1.06, 7.829, -25.12, 0.148, -26.256, 3.154, -3.901, -1.647, 4.919, -9.695, 18.783, 12.49, 7.866, -9.928, -0.379, 15.241, 14.077, 6.491, 15.349, -2.535, 15.941, -12.731, -0.048, 3.581, 17.638, 3.36, -2.443, -12.285, 1.719, -10.301, -19.98, -6.474, -2.496, 6.812, -6.273, 0.677, -6.09, 42.384, 7.152, -4.371, 14.349, 0.201, 14.647, 1.598, -5.549, -2.054, -16.635, 3.395, -16.912, 2.816, -4.909, -12.39, 23.502, 4.973, -4.92, -6.621, -10.966, 0.795, -7.915, -2.6, -18.768, 16.005, 4.28, 8.198, 15.433, 11.514, -37.126, 12.883, 11.523, 13.392, -2.452, 5.709, -0.98, 4.337, 39.569, 1.708, 5.053, -3.521, -13.315, -2.405, -2.277, -4.38, -14.901, -0.376, 7.742, 2.77, -6.944, -9.178, -2.625, -5.407, 1.484, -2.997, 1.743, -6.322, -1.439, -0.292, -2.351, -7.598, -15.322, 1.222, 0.452, -9.737, -4.448, -31.121, -7.134, 2.222, -3.139, -2.782, -3.269, -2.32, 1.151, -7.159, -0.333, 0.216, -8.832, 2.34, -4.401, 3.327, -6.486, 2.512, -4.44, -8.872, -10.926, 15.304, 11.602, 12.24, -20.08, -4.883, -11.348, 7.368, 7.013, 4.768, 6.215, 5.526, 0.362, 20.04, -6.704, 0.221, 12.433, -0.572, -0.077, 12.115, 29.043, -0.898, -9.767, -14.565, 3.594, 23.378, 12.001, -5.628, -8.098, 9.682, -11.175, -3.571, 11.114, 14.767, 0.864, -5.522, -12.756, -13.24, -13.225, -12.233, -3.178, -1.6, -5.847, -0.767, -10.196, -1.942, 1.715, 2.607, 0.746, 0.895, 4.539, 19.175, 21.454, 21.284, -4.46, 5.187, 0.354, -5.931, -2.429, 12.92, 14.857, 11.704, 12.255, 16.187, -0.734, -14.18, 1.104, -1.573, 18.899, -4.238, 3.341, 6.321, 8.486, 3.808, -3.001, -9.41, -8.004, -10.11, -14.594, -9.899, 7.602, 2.685, 11.452, -1.176, -6.07, -13.738, 5.894, -14.928, -14.435, -6.745, -1.863, 36.26, 39.566, 37.5, 39.136, 38.512, 32.114, -4.112, -5.839, 3.36, -12.406, -13.554, -0.454, -4.852, -0.072, 4.966, 8.998, -0.279, 3.089, 6.003, -6.122, 11.078, 3.621, 5.675, -3.556]\n",
      "----------\n",
      "STEP: 8\n",
      "training loss: 9702.716632665815\n",
      "test loss: 2949.0377464890826\n",
      "Model: [511.746, 36.332, 7.898, 12.166, -8.567, -26.959, -19.761, 25.413, -28.655, -8.455, 33.588, 51.414, -1.269, -38.151, -6.896, -7.563, -15.373, -12.893, -0.541, 15.607, -14.461, 4.28, 16.517, 0.819, -8.575, -19.988, 0.734, -0.331, -1.712, -25.688, -37.587, -0.209, 1.321, 7.798, -24.913, 0.355, -25.486, 3.493, -3.879, -1.67, 4.815, -9.688, 18.659, 12.354, 8.194, -10.046, -0.735, 15.116, 13.952, 6.295, 15.224, -2.694, 15.9, -12.884, -0.128, 3.503, 17.55, 3.416, -2.489, -12.465, 1.642, -10.337, -19.955, -6.556, -2.585, 6.714, -6.347, 0.608, -6.157, 42.127, 7.061, -4.586, 14.363, 0.099, 14.464, 1.552, -5.68, -2.197, -16.8, 3.349, -16.993, 2.771, -5.02, -12.171, 23.315, 5.176, -4.957, -6.672, -11.087, 0.682, -7.849, -2.824, -18.763, 15.922, 4.203, 8.154, 15.35, 11.437, -37.187, 12.806, 11.446, 13.285, -2.512, 5.634, -1.018, 4.289, 39.462, 1.66, 4.915, -3.567, -13.387, -2.45, -2.323, -4.426, -14.975, -0.329, 7.686, 2.677, -7.021, -9.036, -2.16, -5.448, 1.406, -3.097, 1.639, -6.366, -1.506, 0.26, -2.382, -7.668, -15.38, 1.126, 0.389, -9.823, -4.495, -30.355, -7.223, 2.165, -3.213, -2.861, -3.337, -2.428, 1.091, -7.201, -0.109, 0.425, -8.892, 2.302, -4.454, 3.266, -6.544, 2.449, -4.471, -8.933, -10.989, 15.256, 11.544, 12.183, -19.173, -4.931, -11.42, 7.319, 6.964, 4.718, 6.166, 5.477, 0.254, 19.977, -6.779, 0.161, 12.365, -0.642, -0.152, 12.047, 28.956, -0.933, -9.876, -14.619, 3.551, 23.27, 11.959, -5.666, -8.144, 9.587, -11.264, -3.616, 11.068, 14.729, 0.81, -5.581, -12.808, -13.292, -13.277, -12.285, -3.228, -1.645, -5.234, -0.838, -10.227, -1.995, 1.662, 2.554, 0.708, 0.842, 4.49, 19.101, 21.361, 21.191, -3.197, 5.102, 0.248, -5.97, -2.487, 12.876, 14.813, 11.66, 12.211, 16.111, -0.782, -14.219, 1.072, -0.868, 18.867, -4.284, 3.287, 6.27, 8.455, 3.778, -3.041, -9.459, -8.053, -10.152, -14.682, -9.989, 7.565, 2.64, 11.409, -1.24, -6.139, -13.797, 5.851, -14.971, -14.478, -6.788, -1.901, 36.214, 39.52, 37.455, 39.091, 38.466, 32.046, -4.149, -5.868, 3.329, -11.3, -12.449, -0.501, -4.894, -0.119, 4.957, 8.953, -0.287, 3.08, 5.994, -6.128, 11.044, 3.616, 5.668, -3.56]\n",
      "----------\n",
      "STEP: 9\n",
      "training loss: 9644.23106633303\n",
      "test loss: 2905.9759196558707\n",
      "Model: [512.046, 36.386, 7.846, 12.192, -7.671, -27.032, -19.656, 25.467, -28.359, -8.422, 33.337, 51.334, -1.302, -36.498, -6.988, -7.122, -15.258, -12.748, -0.581, 15.464, -14.366, 4.528, 16.616, 1.136, -8.457, -19.864, 0.863, -0.34, -1.616, -25.564, -37.651, -0.292, 1.478, 7.82, -24.789, 0.263, -24.669, 3.697, -3.623, -1.724, 4.713, -9.643, 18.584, 12.272, 8.337, -10.156, -0.993, 15.041, 13.877, 6.177, 15.149, -2.593, 15.835, -12.976, -0.016, 3.457, 17.497, 3.618, -2.517, -12.572, 1.596, -10.359, -19.836, -6.604, -2.638, 6.655, -6.349, 0.567, -6.197, 41.852, 7.006, -4.715, 14.218, 0.038, 14.354, 1.525, -5.759, -2.283, -16.899, 3.321, -17.042, 2.743, -5.086, -11.846, 23.204, 5.084, -4.979, -6.703, -11.16, 0.614, -7.977, -2.958, -18.911, 15.872, 4.156, 8.127, 15.3, 11.391, -37.223, 12.76, 11.4, 13.221, -2.548, 5.589, -1.041, 4.26, 39.399, 1.631, 4.832, -3.594, -13.43, -2.478, -2.35, -4.454, -15.019, -0.193, 7.652, 2.62, -7.067, -9.142, -1.657, -5.473, 1.359, -3.157, 1.577, -6.393, -1.546, 0.211, -2.4, -7.71, -15.414, 1.068, 0.351, -9.875, -4.523, -29.541, -7.276, 2.131, -3.257, -2.908, -3.378, -2.492, 1.055, -7.226, -0.196, 0.694, -8.928, 2.279, -4.485, 3.23, -6.579, 2.411, -4.49, -8.97, -11.027, 15.227, 11.51, 12.148, -19.205, -4.96, -11.462, 7.289, 6.934, 4.689, 6.136, 5.447, 0.19, 19.938, -6.824, 0.126, 12.324, -0.684, -0.197, 12.006, 28.904, -0.954, -9.941, -14.652, 3.525, 23.205, 11.934, -5.689, -8.172, 9.53, -11.317, -3.642, 11.04, 14.706, 0.777, -5.616, -12.839, -13.323, -13.308, -12.317, -3.259, -1.672, -5.278, -0.88, -10.246, -2.027, 1.63, 2.522, 0.685, 0.81, 4.46, 19.056, 21.306, 21.136, -3.22, 5.05, 0.184, -5.993, -2.523, 12.85, 14.787, 11.634, 12.184, 16.065, -0.812, -14.243, 1.053, -0.135, 18.847, -4.312, 3.255, 6.24, 8.436, 3.761, -3.066, -9.488, -8.082, -10.178, -14.734, -10.043, 7.544, 2.613, 11.383, -1.278, -6.181, -13.832, 5.825, -14.997, -14.504, -6.814, -1.924, 36.187, 39.492, 37.428, 39.064, 38.439, 32.006, -4.171, -5.885, 3.31, -11.327, -12.475, -0.529, -4.92, -0.147, 4.951, 8.927, -0.291, 3.075, 5.988, -6.131, 11.023, 3.613, 5.663, -3.562]\n",
      "----------\n",
      "STEP: 10\n",
      "training loss: 9600.284758480178\n",
      "test loss: 2878.0506370925345\n",
      "Model: [512.246, 36.331, 7.817, 12.223, -6.974, -27.08, -19.58, 25.427, -27.989, -8.44, 33.231, 51.249, -1.324, -35.985, -7.049, -6.66, -15.181, -12.65, -0.607, 15.369, -14.302, 4.694, 16.683, 1.507, -8.378, -19.782, 0.948, -0.486, -1.553, -25.482, -37.694, -0.429, 1.583, 7.767, -24.707, 0.203, -23.828, 3.833, -3.452, -1.692, 4.511, -9.681, 18.534, 12.218, 8.522, -10.163, -1.313, 14.991, 13.827, 6.099, 15.099, -2.525, 15.657, -13.037, -0.008, 3.426, 17.461, 3.893, -2.535, -12.644, 1.565, -10.373, -19.67, -6.637, -2.673, 6.616, -6.421, 0.539, -6.224, 41.669, 6.969, -4.801, 14.121, -0.002, 14.281, 1.506, -5.811, -2.341, -16.966, 3.303, -17.075, 2.725, -5.13, -11.711, 23.129, 5.022, -4.993, -6.724, -11.208, 0.569, -8.062, -3.048, -19.01, 15.839, 4.125, 8.11, 15.267, 11.36, -37.247, 12.729, 11.369, 13.178, -2.572, 5.559, -1.057, 4.241, 39.356, 1.612, 4.776, -3.613, -13.459, -2.496, -2.369, -4.472, -15.048, -0.012, 7.629, 2.583, -7.098, -9.212, -1.136, -5.49, 1.328, -3.198, 1.535, -6.411, -1.572, 0.179, -2.412, -7.738, -15.437, 1.03, 0.326, -9.909, -4.542, -28.703, -7.311, 2.108, -3.287, -2.94, -3.405, -2.535, 1.031, -7.243, -0.255, 0.633, -8.952, 2.264, -4.506, 3.205, -6.603, 2.385, -4.503, -8.995, -11.052, 15.208, 11.487, 12.125, -19.226, -4.979, -11.491, 7.269, 6.915, 4.669, 6.117, 5.427, 0.147, 19.913, -6.854, 0.102, 12.297, -0.712, -0.227, 11.979, 28.87, -0.968, -9.985, -14.674, 3.508, 23.162, 11.917, -5.705, -8.191, 9.492, -11.353, -3.66, 11.021, 14.691, 0.755, -5.64, -12.86, -13.344, -13.329, -12.337, -3.279, -1.69, -5.308, -0.908, -10.259, -2.048, 1.609, 2.501, 0.67, 0.789, 4.44, 19.026, 21.269, 21.099, -3.235, 5.016, 0.141, -6.008, -2.546, 12.832, 14.769, 11.616, 12.167, 16.034, -0.831, -14.258, 1.04, 0.61, 18.834, -4.331, 3.234, 6.219, 8.423, 3.749, -3.082, -9.508, -8.102, -10.195, -14.769, -10.079, 7.529, 2.594, 11.366, -1.303, -6.208, -13.856, 5.808, -15.014, -14.521, -6.832, -1.939, 36.169, 39.473, 37.409, 39.046, 38.421, 31.979, -4.185, -5.896, 3.298, -11.344, -12.492, -0.548, -4.937, -0.166, 4.947, 8.909, -0.294, 3.072, 5.985, -6.133, 11.009, 3.611, 5.66, -3.564]\n",
      "----------\n",
      "STEP: 11\n",
      "training loss: 9556.35581678597\n",
      "test loss: 2850.1717751273463\n",
      "Model: [512.446, 36.277, 7.789, 12.254, -6.278, -27.129, -19.505, 25.388, -27.618, -8.457, 33.124, 51.164, -1.346, -35.471, -7.11, -6.197, -15.104, -12.553, -0.633, 15.274, -14.238, 4.859, 16.749, 1.879, -8.3, -19.699, 1.034, -0.632, -1.489, -25.399, -37.737, -0.565, 1.687, 7.713, -24.624, 0.142, -22.987, 3.969, -3.282, -1.661, 4.309, -9.718, 18.484, 12.163, 8.708, -10.17, -1.634, 14.941, 13.777, 6.02, 15.049, -2.457, 15.479, -13.098, -0.001, 3.395, 17.426, 4.167, -2.554, -12.716, 1.534, -10.387, -19.504, -6.67, -2.709, 6.577, -6.493, 0.512, -6.251, 41.486, 6.932, -4.888, 14.025, -0.043, 14.207, 1.488, -5.863, -2.398, -17.032, 3.285, -17.107, 2.706, -5.175, -11.575, 23.054, 4.961, -5.008, -6.744, -11.257, 0.523, -8.147, -3.137, -19.109, 15.806, 4.094, 8.092, 15.234, 11.329, -37.271, 12.698, 11.338, 13.135, -2.596, 5.529, -1.072, 4.221, 39.314, 1.593, 4.721, -3.631, -13.488, -2.515, -2.387, -4.49, -15.078, 0.168, 7.607, 2.545, -7.128, -9.283, -0.615, -5.507, 1.297, -3.238, 1.493, -6.429, -1.599, 0.147, -2.424, -7.766, -15.46, 0.991, 0.301, -9.944, -4.561, -27.865, -7.347, 2.086, -3.316, -2.972, -3.432, -2.578, 1.007, -7.26, -0.313, 0.573, -8.976, 2.249, -4.527, 3.181, -6.626, 2.36, -4.515, -9.019, -11.078, 15.189, 11.464, 12.102, -19.247, -4.998, -11.519, 7.25, 6.895, 4.649, 6.097, 5.408, 0.104, 19.888, -6.884, 0.078, 12.27, -0.739, -0.257, 11.952, 28.835, -0.983, -10.028, -14.695, 3.49, 23.118, 11.9, -5.72, -8.21, 9.454, -11.389, -3.678, 11.002, 14.676, 0.734, -5.663, -12.881, -13.364, -13.35, -12.358, -3.299, -1.708, -5.338, -0.936, -10.271, -2.07, 1.588, 2.48, 0.655, 0.768, 4.42, 18.996, 21.232, 21.062, -3.251, 4.982, 0.099, -6.024, -2.57, 12.815, 14.752, 11.598, 12.149, 16.004, -0.851, -14.274, 1.027, 1.355, 18.821, -4.349, 3.212, 6.199, 8.41, 3.737, -3.099, -9.527, -8.121, -10.212, -14.804, -10.115, 7.515, 2.576, 11.349, -1.328, -6.236, -13.879, 5.79, -15.031, -14.538, -6.849, -1.954, 36.151, 39.455, 37.391, 39.027, 38.403, 31.952, -4.2, -5.908, 3.285, -11.362, -12.51, -0.567, -4.954, -0.185, 4.943, 8.892, -0.297, 3.069, 5.981, -6.135, 10.996, 3.609, 5.658, -3.565]\n",
      "----------\n",
      "STEP: 12\n",
      "training loss: 9526.4401426551\n",
      "test loss: 2822.59944895163\n",
      "Model: [512.645, 36.223, 7.761, 12.284, -5.588, -27.177, -19.429, 25.348, -27.251, -8.474, 33.017, 51.08, -1.368, -34.963, -7.171, -5.735, -15.027, -12.457, -0.66, 15.179, -14.175, 5.023, 16.815, 2.247, -8.222, -19.618, 1.118, -0.777, -1.426, -25.317, -37.779, -0.703, 1.791, 7.658, -24.542, 0.081, -22.153, 4.103, -3.112, -1.628, 4.11, -9.758, 18.435, 12.109, 8.895, -10.179, -1.951, 14.891, 13.727, 5.943, 14.999, -2.39, 15.303, -13.159, 0.006, 3.364, 17.39, 4.439, -2.572, -12.788, 1.503, -10.402, -19.338, -6.702, -2.744, 6.538, -6.566, 0.485, -6.278, 41.304, 6.896, -4.973, 13.929, -0.084, 14.135, 1.469, -5.915, -2.455, -17.097, 3.266, -17.14, 2.688, -5.218, -11.438, 22.98, 4.901, -5.022, -6.765, -11.305, 0.478, -8.231, -3.226, -19.207, 15.773, 4.064, 8.074, 15.201, 11.299, -37.295, 12.668, 11.308, 13.093, -2.62, 5.499, -1.087, 4.202, 39.271, 1.574, 4.666, -3.649, -13.517, -2.533, -2.405, -4.508, -15.107, 0.35, 7.585, 2.508, -7.159, -9.353, -0.103, -5.523, 1.266, -3.277, 1.452, -6.447, -1.626, 0.115, -2.437, -7.794, -15.483, 0.953, 0.276, -9.978, -4.579, -27.035, -7.382, 2.063, -3.345, -3.003, -3.459, -2.621, 0.983, -7.276, -0.371, 0.512, -9.0, 2.234, -4.548, 3.157, -6.649, 2.335, -4.528, -9.044, -11.103, 15.17, 11.441, 12.079, -19.268, -5.017, -11.548, 7.23, 6.875, 4.63, 6.077, 5.388, 0.061, 19.862, -6.914, 0.055, 12.243, -0.767, -0.287, 11.925, 28.801, -0.997, -10.071, -14.717, 3.473, 23.075, 11.883, -5.735, -8.228, 9.416, -11.424, -3.696, 10.984, 14.661, 0.712, -5.687, -12.901, -13.385, -13.37, -12.379, -3.32, -1.726, -5.367, -0.964, -10.284, -2.091, 1.567, 2.459, 0.639, 0.747, 4.4, 18.966, 21.195, 21.025, -3.266, 4.948, 0.056, -6.039, -2.593, 12.797, 14.734, 11.581, 12.132, 15.973, -0.87, -14.29, 1.015, 2.101, 18.808, -4.368, 3.191, 6.179, 8.398, 3.725, -3.115, -9.546, -8.141, -10.229, -14.839, -10.151, 7.5, 2.558, 11.331, -1.354, -6.263, -13.902, 5.773, -15.048, -14.556, -6.866, -1.969, 36.133, 39.437, 37.373, 39.009, 38.385, 31.925, -4.215, -5.919, 3.273, -11.379, -12.527, -0.586, -4.971, -0.203, 4.939, 8.874, -0.3, 3.066, 5.977, -6.137, 10.982, 3.606, 5.655, -3.567]\n",
      "----------\n",
      "STEP: 13\n",
      "training loss: 9506.96858683315\n",
      "test loss: 2811.61018237893\n",
      "Model: [512.745, 36.189, 7.734, 12.32, -5.246, -27.203, -19.333, 25.334, -27.04, -8.465, 32.872, 51.047, -1.379, -34.711, -7.202, -5.252, -14.989, -12.408, -0.673, 15.132, -14.143, 5.106, 16.848, 2.432, -8.183, -19.576, 1.161, -0.85, -1.394, -25.276, -37.8, -0.892, 1.843, 7.527, -24.5, 0.051, -21.733, 4.171, -3.027, -1.511, 4.009, -9.878, 18.41, 12.082, 9.124, -10.282, -2.112, 14.866, 13.702, 5.903, 14.974, -2.356, 15.215, -13.19, -0.091, 3.349, 17.373, 4.577, -2.582, -12.823, 1.487, -10.409, -19.125, -6.718, -2.762, 6.518, -6.708, 0.471, -6.291, 41.212, 6.878, -5.016, 13.88, -0.104, 14.098, 1.46, -5.941, -2.484, -17.131, 3.257, -17.156, 2.679, -5.241, -11.25, 22.943, 4.87, -5.03, -6.775, -11.329, 0.456, -8.273, -3.271, -19.256, 15.756, 4.048, 8.066, 15.184, 11.283, -37.308, 12.652, 11.292, 13.072, -2.632, 5.484, -1.095, 4.192, 39.25, 1.564, 4.639, -3.658, -13.532, -2.542, -2.414, -4.518, -15.122, 0.575, 7.573, 2.489, -7.174, -9.388, -0.121, -5.531, 1.251, -3.298, 1.431, -6.456, -1.639, 0.099, -2.443, -7.808, -15.495, 0.934, 0.263, -9.995, -4.589, -26.616, -7.399, 2.052, -3.36, -3.019, -3.473, -2.643, 0.971, -7.285, -0.4, 0.482, -9.012, 2.226, -4.558, 3.145, -6.661, 2.322, -4.534, -9.056, -11.116, 15.16, 11.429, 12.067, -19.278, -5.027, -11.562, 7.22, 6.865, 4.62, 6.067, 5.378, 0.039, 19.85, -6.929, 0.043, 12.229, -0.781, -0.302, 11.911, 28.784, -1.004, -10.093, -14.728, 3.465, 23.054, 11.875, -5.743, -8.238, 9.397, -11.442, -3.704, 10.974, 14.653, 0.701, -5.699, -12.912, -13.395, -13.381, -12.389, -3.33, -1.735, -5.382, -0.978, -10.29, -2.101, 1.556, 2.448, 0.632, 0.736, 4.39, 18.951, 21.177, 21.007, -3.274, 4.93, 0.035, -6.047, -2.605, 12.788, 14.726, 11.572, 12.123, 15.958, -0.88, -14.298, 1.008, 2.859, 18.801, -4.377, 3.18, 6.169, 8.392, 3.719, -3.123, -9.556, -8.15, -10.238, -14.856, -10.169, 7.493, 2.549, 11.323, -1.366, -6.277, -13.914, 5.764, -15.057, -14.564, -6.875, -1.977, 36.124, 39.428, 37.364, 39.0, 38.376, 31.911, -4.222, -5.925, 3.266, -11.388, -12.536, -0.595, -4.98, -0.213, 4.937, 8.865, -0.301, 3.064, 5.976, -6.138, 10.975, 3.605, 5.653, -3.568]\n",
      "----------\n",
      "STEP: 14\n",
      "training loss: 9487.49711296384\n",
      "test loss: 2800.621144636079\n",
      "Model: [512.845, 36.154, 7.707, 12.356, -4.904, -27.23, -19.236, 25.319, -26.83, -8.456, 32.728, 51.014, -1.39, -34.459, -7.232, -4.769, -14.95, -12.36, -0.686, 15.084, -14.111, 5.189, 16.881, 2.618, -8.143, -19.535, 1.204, -0.923, -1.362, -25.234, -37.822, -1.081, 1.895, 7.397, -24.459, 0.021, -21.312, 4.239, -2.942, -1.394, 3.908, -9.999, 18.385, 12.054, 9.354, -10.386, -2.272, 14.841, 13.677, 5.864, 14.949, -2.322, 15.126, -13.22, -0.187, 3.333, 17.355, 4.714, -2.591, -12.859, 1.472, -10.416, -18.912, -6.735, -2.779, 6.499, -6.851, 0.457, -6.305, 41.121, 6.859, -5.059, 13.832, -0.124, 14.062, 1.451, -5.968, -2.513, -17.164, 3.248, -17.172, 2.67, -5.263, -11.061, 22.906, 4.839, -5.037, -6.785, -11.353, 0.433, -8.316, -3.316, -19.306, 15.739, 4.033, 8.057, 15.168, 11.268, -37.32, 12.637, 11.277, 13.05, -2.644, 5.469, -1.102, 4.183, 39.229, 1.554, 4.611, -3.668, -13.546, -2.551, -2.424, -4.527, -15.137, 0.8, 7.562, 2.471, -7.19, -9.424, -0.14, -5.54, 1.235, -3.318, 1.41, -6.465, -1.652, 0.082, -2.449, -7.822, -15.506, 0.915, 0.251, -10.012, -4.598, -26.197, -7.417, 2.04, -3.375, -3.034, -3.486, -2.664, 0.959, -7.293, -0.429, 0.452, -9.024, 2.219, -4.569, 3.132, -6.673, 2.309, -4.54, -9.068, -11.128, 15.151, 11.418, 12.056, -19.289, -5.036, -11.576, 7.21, 6.856, 4.61, 6.058, 5.368, 0.018, 19.837, -6.944, 0.031, 12.216, -0.795, -0.317, 11.897, 28.766, -1.011, -10.115, -14.739, 3.456, 23.032, 11.866, -5.75, -8.247, 9.378, -11.46, -3.713, 10.965, 14.646, 0.69, -5.71, -12.922, -13.406, -13.391, -12.4, -3.34, -1.744, -5.397, -0.993, -10.296, -2.112, 1.546, 2.438, 0.624, 0.725, 4.381, 18.936, 21.158, 20.988, -3.282, 4.913, 0.014, -6.055, -2.616, 12.78, 14.717, 11.563, 12.114, 15.943, -0.889, -14.305, 1.002, 3.618, 18.795, -4.386, 3.169, 6.158, 8.385, 3.713, -3.131, -9.566, -8.16, -10.246, -14.874, -10.187, 7.486, 2.54, 11.314, -1.379, -6.291, -13.926, 5.755, -15.066, -14.573, -6.884, -1.984, 36.115, 39.418, 37.355, 38.991, 38.367, 31.898, -4.229, -5.931, 3.26, -11.397, -12.545, -0.605, -4.989, -0.222, 4.935, 8.857, -0.303, 3.062, 5.974, -6.139, 10.968, 3.604, 5.652, -3.568]\n",
      "----------\n",
      "STEP: 15\n",
      "training loss: 9468.025995388802\n",
      "test loss: 2789.6319590167086\n",
      "Model: [512.945, 36.12, 7.68, 12.391, -4.562, -27.256, -19.14, 25.305, -26.62, -8.447, 32.584, 50.981, -1.401, -34.208, -7.263, -4.287, -14.912, -12.311, -0.699, 15.037, -14.079, 5.271, 16.914, 2.804, -8.104, -19.494, 1.247, -0.996, -1.33, -25.193, -37.843, -1.271, 1.947, 7.266, -24.418, -0.01, -20.892, 4.307, -2.856, -1.277, 3.807, -10.119, 18.36, 12.027, 9.583, -10.489, -2.432, 14.816, 13.652, 5.825, 14.924, -2.288, 15.037, -13.251, -0.283, 3.318, 17.337, 4.851, -2.6, -12.895, 1.456, -10.423, -18.699, -6.751, -2.797, 6.479, -6.993, 0.444, -6.318, 41.029, 6.841, -5.102, 13.784, -0.145, 14.025, 1.442, -5.994, -2.541, -17.197, 3.239, -17.188, 2.661, -5.285, -10.872, 22.868, 4.809, -5.044, -6.796, -11.377, 0.41, -8.358, -3.361, -19.355, 15.723, 4.017, 8.048, 15.151, 11.253, -37.332, 12.622, 11.262, 13.029, -2.656, 5.454, -1.11, 4.173, 39.208, 1.545, 4.583, -3.677, -13.561, -2.56, -2.433, -4.536, -15.151, 1.024, 7.551, 2.452, -7.205, -9.459, -0.158, -5.548, 1.219, -3.338, 1.389, -6.474, -1.666, 0.066, -2.455, -7.836, -15.518, 0.895, 0.238, -10.03, -4.608, -25.778, -7.435, 2.029, -3.389, -3.05, -3.5, -2.686, 0.947, -7.301, -0.458, 0.421, -9.036, 2.211, -4.579, 3.12, -6.684, 2.297, -4.546, -9.081, -11.141, 15.141, 11.406, 12.044, -19.299, -5.046, -11.59, 7.2, 6.846, 4.6, 6.048, 5.359, -0.004, 19.824, -6.959, 0.019, 12.202, -0.809, -0.332, 11.884, 28.749, -1.018, -10.137, -14.75, 3.447, 23.01, 11.858, -5.758, -8.256, 9.359, -11.478, -3.722, 10.956, 14.638, 0.679, -5.722, -12.933, -13.416, -13.401, -12.41, -3.35, -1.753, -5.412, -1.007, -10.302, -2.123, 1.535, 2.427, 0.616, 0.715, 4.371, 18.921, 21.139, 20.97, -3.289, 4.896, -0.007, -6.063, -2.628, 12.771, 14.708, 11.555, 12.105, 15.927, -0.899, -14.313, 0.995, 4.376, 18.788, -4.395, 3.159, 6.148, 8.379, 3.707, -3.139, -9.576, -8.17, -10.255, -14.891, -10.205, 7.478, 2.531, 11.305, -1.392, -6.305, -13.937, 5.747, -15.074, -14.582, -6.892, -1.992, 36.106, 39.409, 37.346, 38.982, 38.358, 31.884, -4.237, -5.936, 3.254, -11.405, -12.554, -0.614, -4.997, -0.232, 4.933, 8.848, -0.304, 3.061, 5.972, -6.14, 10.961, 3.603, 5.65, -3.569]\n",
      "----------\n",
      "STEP: 16\n",
      "training loss: 9448.556660839075\n",
      "test loss: 2778.6414359893706\n",
      "Model: [513.045, 36.086, 7.653, 12.427, -4.22, -27.282, -19.043, 25.29, -26.41, -8.438, 32.439, 50.948, -1.412, -33.956, -7.293, -3.804, -14.873, -12.263, -0.712, 14.989, -14.047, 5.354, 16.947, 2.989, -8.065, -19.453, 1.289, -1.069, -1.298, -25.152, -37.864, -1.46, 2.0, 7.136, -24.376, -0.04, -20.471, 4.375, -2.771, -1.16, 3.706, -10.24, 18.335, 12.0, 9.813, -10.593, -2.592, 14.791, 13.627, 5.786, 14.899, -2.255, 14.948, -13.282, -0.379, 3.302, 17.319, 4.988, -2.609, -12.931, 1.441, -10.43, -18.486, -6.767, -2.815, 6.459, -7.135, 0.43, -6.331, 40.938, 6.823, -5.145, 13.735, -0.165, 13.988, 1.432, -6.02, -2.57, -17.23, 3.23, -17.205, 2.652, -5.307, -10.683, 22.831, 4.778, -5.052, -6.806, -11.402, 0.388, -8.4, -3.405, -19.405, 15.706, 4.002, 8.039, 15.134, 11.237, -37.344, 12.606, 11.246, 13.007, -2.668, 5.439, -1.118, 4.163, 39.186, 1.535, 4.556, -3.686, -13.575, -2.569, -2.442, -4.545, -15.166, 1.249, 7.539, 2.433, -7.22, -9.494, -0.177, -5.556, 1.204, -3.358, 1.369, -6.483, -1.679, 0.05, -2.461, -7.851, -15.529, 0.876, 0.226, -10.047, -4.617, -25.359, -7.452, 2.017, -3.404, -3.066, -3.514, -2.707, 0.935, -7.31, -0.487, 0.391, -9.048, 2.203, -4.59, 3.108, -6.696, 2.284, -4.553, -9.093, -11.154, 15.132, 11.394, 12.033, -19.31, -5.056, -11.605, 7.19, 6.836, 4.59, 6.038, 5.349, -0.025, 19.811, -6.974, 0.007, 12.189, -0.823, -0.348, 11.87, 28.732, -1.025, -10.158, -14.76, 3.439, 22.989, 11.849, -5.766, -8.266, 9.34, -11.495, -3.731, 10.946, 14.63, 0.669, -5.734, -12.943, -13.427, -13.412, -12.42, -3.36, -1.762, -5.427, -1.021, -10.309, -2.133, 1.524, 2.416, 0.609, 0.704, 4.361, 18.906, 21.121, 20.951, -3.297, 4.879, -0.029, -6.07, -2.64, 12.762, 14.699, 11.546, 12.097, 15.912, -0.909, -14.321, 0.989, 5.135, 18.781, -4.405, 3.148, 6.138, 8.373, 3.701, -3.147, -9.585, -8.18, -10.263, -14.909, -10.223, 7.471, 2.522, 11.297, -1.404, -6.319, -13.949, 5.738, -15.083, -14.59, -6.901, -2.0, 36.097, 39.4, 37.337, 38.973, 38.348, 31.871, -4.244, -5.942, 3.248, -11.414, -12.562, -0.624, -5.006, -0.241, 4.931, 8.839, -0.306, 3.059, 5.97, -6.141, 10.954, 3.602, 5.649, -3.57]\n",
      "----------\n",
      "STEP: 17\n",
      "training loss: 9429.43797593229\n",
      "test loss: 2767.643867721439\n",
      "Model: [513.145, 36.052, 7.626, 12.463, -3.879, -27.309, -18.946, 25.276, -26.201, -8.429, 32.295, 50.915, -1.424, -33.705, -7.324, -3.321, -14.834, -12.214, -0.725, 14.942, -14.015, 5.437, 16.98, 3.175, -8.025, -19.411, 1.332, -1.142, -1.266, -25.11, -37.885, -1.649, 2.052, 7.005, -24.335, -0.07, -20.051, 4.443, -2.686, -1.043, 3.606, -10.361, 18.31, 11.972, 10.042, -10.696, -2.752, 14.766, 13.602, 5.747, 14.874, -2.221, 14.859, -13.312, -0.475, 3.287, 17.302, 5.125, -2.619, -12.967, 1.425, -10.437, -18.274, -6.784, -2.833, 6.44, -7.277, 0.416, -6.345, 40.846, 6.804, -5.188, 13.687, -0.185, 13.952, 1.423, -6.046, -2.599, -17.262, 3.221, -17.221, 2.642, -5.329, -10.495, 22.793, 4.747, -5.059, -6.816, -11.426, 0.365, -8.443, -3.45, -19.454, 15.69, 3.986, 8.03, 15.118, 11.222, -37.356, 12.591, 11.231, 12.986, -2.68, 5.424, -1.125, 4.153, 39.165, 1.526, 4.528, -3.695, -13.59, -2.579, -2.451, -4.554, -15.181, 1.474, 7.528, 2.414, -7.236, -9.53, -0.195, -5.565, 1.188, -3.378, 1.348, -6.492, -1.692, 0.034, -2.467, -7.865, -15.541, 0.857, 0.213, -10.064, -4.626, -24.94, -7.47, 2.006, -3.419, -3.082, -3.527, -2.728, 0.923, -7.318, -0.516, 0.361, -9.06, 2.196, -4.6, 3.096, -6.708, 2.271, -4.559, -9.105, -11.166, 15.122, 11.383, 12.021, -19.32, -5.065, -11.619, 7.181, 6.826, 4.581, 6.028, 5.339, -0.047, 19.799, -6.989, -0.005, 12.175, -0.837, -0.363, 11.857, 28.714, -1.032, -10.18, -14.771, 3.43, 22.967, 11.841, -5.773, -8.275, 9.321, -11.513, -3.74, 10.937, 14.623, 0.658, -5.746, -12.953, -13.437, -13.422, -12.431, -3.37, -1.771, -5.442, -1.035, -10.315, -2.144, 1.514, 2.406, 0.601, 0.694, 4.351, 18.891, 21.102, 20.933, -3.305, 4.862, -0.05, -6.078, -2.652, 12.753, 14.69, 11.537, 12.088, 15.897, -0.919, -14.329, 0.983, 5.893, 18.775, -4.414, 3.137, 6.128, 8.366, 3.695, -3.156, -9.595, -8.189, -10.272, -14.926, -10.241, 7.464, 2.513, 11.288, -1.417, -6.332, -13.961, 5.729, -15.091, -14.599, -6.91, -2.007, 36.088, 39.391, 37.328, 38.964, 38.339, 31.857, -4.252, -5.948, 3.241, -11.423, -12.571, -0.633, -5.014, -0.251, 4.929, 8.83, -0.307, 3.057, 5.968, -6.142, 10.947, 3.601, 5.647, -3.57]\n",
      "----------\n",
      "STEP: 18\n",
      "training loss: 9428.217154694397\n",
      "test loss: 2759.790349664288\n",
      "Model: [513.217, 36.027, 7.607, 12.488, -3.637, -27.327, -18.875, 25.265, -26.052, -8.423, 32.193, 50.891, -1.431, -33.526, -7.346, -2.978, -14.807, -12.179, -0.735, 14.91, -13.992, 5.495, 17.001, 3.306, -7.997, -19.382, 1.363, -1.194, -1.243, -25.08, -37.896, -1.783, 2.09, 6.913, -24.305, -0.089, -19.752, 4.492, -2.626, -0.959, 3.535, -10.445, 18.292, 11.953, 10.205, -10.771, -2.865, 14.748, 13.584, 5.718, 14.856, -2.199, 14.795, -13.334, -0.544, 3.276, 17.289, 5.222, -2.625, -12.993, 1.414, -10.443, -18.123, -6.796, -2.845, 6.426, -7.378, 0.406, -6.355, 40.78, 6.791, -5.219, 13.652, -0.2, 13.925, 1.417, -6.065, -2.619, -17.284, 3.214, -17.233, 2.636, -5.345, -10.361, 22.767, 4.725, -5.064, -6.824, -11.443, 0.349, -8.471, -3.482, -19.487, 15.678, 3.975, 8.024, 15.106, 11.211, -37.363, 12.58, 11.22, 12.97, -2.689, 5.413, -1.131, 4.146, 39.15, 1.519, 4.508, -3.702, -13.6, -2.585, -2.458, -4.561, -15.191, 1.634, 7.52, 2.401, -7.247, -9.555, -0.209, -5.571, 1.177, -3.392, 1.333, -6.498, -1.702, 0.022, -2.471, -7.875, -15.549, 0.843, 0.204, -10.077, -4.633, -24.643, -7.483, 1.998, -3.43, -3.093, -3.537, -2.739, 0.914, -7.324, -0.534, 0.339, -9.069, 2.19, -4.608, 3.087, -6.716, 2.262, -4.563, -9.114, -11.175, 15.115, 11.375, 12.013, -19.328, -5.071, -11.629, 7.174, 6.819, 4.573, 6.021, 5.332, -0.062, 19.789, -7.0, -0.013, 12.165, -0.847, -0.373, 11.847, 28.702, -1.037, -10.196, -14.779, 3.424, 22.951, 11.835, -5.779, -8.281, 9.307, -11.526, -3.746, 10.93, 14.617, 0.65, -5.754, -12.961, -13.445, -13.43, -12.438, -3.378, -1.778, -5.452, -1.045, -10.319, -2.152, 1.506, 2.398, 0.596, 0.686, 4.344, 18.88, 21.089, 20.919, -3.31, 4.849, -0.065, -6.084, -2.66, 12.747, 14.684, 11.531, 12.081, 15.886, -0.926, -14.335, 0.978, 6.432, 18.77, -4.421, 3.13, 6.121, 8.362, 3.691, -3.162, -9.602, -8.196, -10.278, -14.939, -10.254, 7.459, 2.507, 11.282, -1.426, -6.342, -13.969, 5.723, -15.098, -14.605, -6.916, -2.013, 36.081, 39.384, 37.321, 38.958, 38.333, 31.848, -4.257, -5.952, 3.237, -11.429, -12.577, -0.64, -5.02, -0.257, 4.928, 8.824, -0.308, 3.056, 5.967, -6.143, 10.943, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 19\n",
      "training loss: 9428.209900658436\n",
      "test loss: 2759.660407094897\n",
      "Model: [513.22, 36.026, 7.606, 12.488, -3.638, -27.326, -18.868, 25.264, -26.054, -8.424, 32.194, 50.89, -1.432, -33.528, -7.347, -2.979, -14.806, -12.177, -0.735, 14.915, -13.991, 5.492, 16.994, 3.305, -7.996, -19.38, 1.364, -1.196, -1.243, -25.079, -37.885, -1.782, 2.091, 6.916, -24.304, -0.081, -19.753, 4.493, -2.629, -0.956, 3.538, -10.443, 18.291, 11.952, 10.203, -10.773, -2.864, 14.747, 13.583, 5.718, 14.855, -2.206, 14.793, -13.335, -0.546, 3.275, 17.288, 5.22, -2.626, -12.993, 1.414, -10.443, -18.124, -6.796, -2.846, 6.425, -7.376, 0.406, -6.355, 40.777, 6.79, -5.221, 13.651, -0.201, 13.924, 1.416, -6.066, -2.62, -17.276, 3.214, -17.233, 2.636, -5.346, -10.363, 22.765, 4.725, -5.065, -6.824, -11.444, 0.348, -8.465, -3.483, -19.481, 15.677, 3.975, 8.024, 15.105, 11.21, -37.361, 12.579, 11.219, 12.97, -2.689, 5.412, -1.131, 4.146, 39.149, 1.518, 4.507, -3.702, -13.6, -2.585, -2.458, -4.561, -15.192, 1.633, 7.52, 2.4, -7.247, -9.556, -0.209, -5.571, 1.177, -3.393, 1.332, -6.498, -1.702, 0.022, -2.472, -7.875, -15.549, 0.843, 0.203, -10.077, -4.633, -24.644, -7.483, 1.997, -3.43, -3.094, -3.537, -2.728, 0.914, -7.325, -0.526, 0.338, -9.069, 2.19, -4.608, 3.087, -6.715, 2.262, -4.564, -9.114, -11.176, 15.115, 11.374, 12.012, -19.328, -5.071, -11.628, 7.173, 6.819, 4.573, 6.021, 5.331, -0.063, 19.789, -7.0, -0.013, 12.165, -0.848, -0.374, 11.846, 28.701, -1.037, -10.196, -14.78, 3.423, 22.951, 11.835, -5.779, -8.28, 9.307, -11.527, -3.747, 10.93, 14.617, 0.65, -5.755, -12.961, -13.445, -13.43, -12.439, -3.378, -1.778, -5.453, -1.045, -10.32, -2.152, 1.506, 2.398, 0.595, 0.686, 4.343, 18.88, 21.089, 20.919, -3.311, 4.849, -0.066, -6.084, -2.66, 12.747, 14.684, 11.53, 12.081, 15.885, -0.926, -14.335, 0.978, 6.432, 18.77, -4.421, 3.129, 6.12, 8.362, 3.691, -3.162, -9.602, -8.197, -10.278, -14.939, -10.254, 7.459, 2.506, 11.282, -1.426, -6.343, -13.97, 5.723, -15.098, -14.605, -6.916, -2.013, 36.081, 39.384, 37.321, 38.957, 38.333, 31.847, -4.257, -5.952, 3.237, -11.43, -12.578, -0.64, -5.021, -0.258, 4.928, 8.823, -0.308, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 20\n",
      "training loss: 9428.205951992564\n",
      "test loss: 2759.5641544767263\n",
      "Model: [513.222, 36.026, 7.606, 12.488, -3.64, -27.325, -18.863, 25.264, -26.056, -8.425, 32.194, 50.889, -1.432, -33.529, -7.348, -2.979, -14.805, -12.176, -0.736, 14.918, -13.99, 5.49, 16.989, 3.303, -7.996, -19.38, 1.364, -1.198, -1.243, -25.078, -37.877, -1.781, 2.092, 6.917, -24.303, -0.076, -19.754, 4.495, -2.63, -0.955, 3.54, -10.441, 18.29, 11.951, 10.202, -10.774, -2.862, 14.747, 13.583, 5.717, 14.855, -2.211, 14.791, -13.336, -0.547, 3.275, 17.288, 5.218, -2.626, -12.992, 1.414, -10.443, -18.125, -6.796, -2.846, 6.425, -7.374, 0.406, -6.355, 40.775, 6.79, -5.222, 13.65, -0.201, 13.923, 1.416, -6.066, -2.621, -17.271, 3.214, -17.233, 2.635, -5.346, -10.364, 22.765, 4.724, -5.065, -6.824, -11.445, 0.348, -8.462, -3.484, -19.477, 15.677, 3.974, 8.023, 15.105, 11.21, -37.358, 12.579, 11.219, 12.969, -2.69, 5.412, -1.131, 4.146, 39.149, 1.518, 4.507, -3.702, -13.601, -2.586, -2.458, -4.561, -15.192, 1.632, 7.519, 2.4, -7.248, -9.557, -0.21, -5.571, 1.176, -3.393, 1.332, -6.498, -1.703, 0.022, -2.472, -7.875, -15.549, 0.842, 0.203, -10.078, -4.634, -24.644, -7.484, 1.997, -3.43, -3.094, -3.538, -2.72, 0.914, -7.325, -0.52, 0.337, -9.069, 2.19, -4.608, 3.086, -6.714, 2.262, -4.564, -9.115, -11.176, 15.115, 11.374, 12.012, -19.328, -5.07, -11.628, 7.173, 6.818, 4.573, 6.02, 5.331, -0.063, 19.789, -7.001, -0.014, 12.164, -0.848, -0.374, 11.846, 28.701, -1.037, -10.197, -14.78, 3.423, 22.95, 11.834, -5.779, -8.279, 9.306, -11.527, -3.747, 10.93, 14.617, 0.649, -5.755, -12.961, -13.445, -13.43, -12.439, -3.378, -1.778, -5.453, -1.046, -10.32, -2.152, 1.506, 2.397, 0.595, 0.685, 4.343, 18.88, 21.088, 20.918, -3.311, 4.849, -0.066, -6.084, -2.661, 12.746, 14.684, 11.53, 12.081, 15.885, -0.926, -14.335, 0.978, 6.431, 18.77, -4.421, 3.129, 6.12, 8.361, 3.691, -3.162, -9.603, -8.197, -10.278, -14.94, -10.255, 7.458, 2.506, 11.281, -1.427, -6.343, -13.97, 5.723, -15.098, -14.605, -6.916, -2.013, 36.081, 39.384, 37.321, 38.957, 38.332, 31.847, -4.257, -5.952, 3.237, -11.43, -12.578, -0.64, -5.021, -0.258, 4.928, 8.823, -0.308, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 21\n",
      "training loss: 9428.203404584317\n",
      "test loss: 2759.4861967484267\n",
      "Model: [513.224, 36.026, 7.606, 12.488, -3.641, -27.324, -18.858, 25.263, -26.057, -8.426, 32.195, 50.888, -1.432, -33.53, -7.348, -2.98, -14.804, -12.176, -0.736, 14.921, -13.99, 5.488, 16.985, 3.302, -7.996, -19.379, 1.365, -1.199, -1.242, -25.078, -37.871, -1.78, 2.093, 6.919, -24.302, -0.071, -19.754, 4.496, -2.632, -0.954, 3.541, -10.44, 18.29, 11.951, 10.202, -10.775, -2.861, 14.746, 13.582, 5.717, 14.854, -2.215, 14.789, -13.336, -0.548, 3.275, 17.288, 5.217, -2.626, -12.992, 1.413, -10.443, -18.126, -6.797, -2.847, 6.424, -7.373, 0.405, -6.356, 40.773, 6.79, -5.222, 13.649, -0.201, 13.923, 1.416, -6.067, -2.621, -17.267, 3.213, -17.234, 2.635, -5.347, -10.365, 22.764, 4.723, -5.065, -6.824, -11.445, 0.347, -8.459, -3.485, -19.473, 15.676, 3.974, 8.023, 15.104, 11.21, -37.356, 12.579, 11.219, 12.969, -2.69, 5.412, -1.131, 4.146, 39.148, 1.518, 4.506, -3.702, -13.601, -2.586, -2.458, -4.562, -15.192, 1.632, 7.519, 2.4, -7.248, -9.558, -0.21, -5.571, 1.176, -3.394, 1.331, -6.499, -1.703, 0.022, -2.472, -7.876, -15.55, 0.842, 0.203, -10.078, -4.634, -24.645, -7.484, 1.997, -3.431, -3.094, -3.538, -2.714, 0.913, -7.325, -0.515, 0.337, -9.069, 2.19, -4.609, 3.086, -6.714, 2.261, -4.564, -9.115, -11.176, 15.115, 11.374, 12.012, -19.328, -5.069, -11.627, 7.173, 6.818, 4.573, 6.02, 5.331, -0.064, 19.789, -7.001, -0.014, 12.164, -0.848, -0.375, 11.846, 28.701, -1.037, -10.197, -14.78, 3.423, 22.95, 11.834, -5.779, -8.278, 9.306, -11.527, -3.747, 10.93, 14.617, 0.649, -5.755, -12.962, -13.445, -13.431, -12.439, -3.379, -1.779, -5.453, -1.046, -10.32, -2.152, 1.505, 2.397, 0.595, 0.685, 4.343, 18.879, 21.088, 20.918, -3.311, 4.848, -0.067, -6.084, -2.661, 12.746, 14.683, 11.53, 12.081, 15.885, -0.926, -14.335, 0.978, 6.431, 18.77, -4.421, 3.129, 6.12, 8.361, 3.69, -3.162, -9.603, -8.197, -10.278, -14.94, -10.255, 7.458, 2.506, 11.281, -1.427, -6.343, -13.97, 5.723, -15.098, -14.606, -6.917, -2.013, 36.08, 39.383, 37.321, 38.957, 38.332, 31.847, -4.257, -5.952, 3.236, -11.43, -12.578, -0.64, -5.021, -0.258, 4.928, 8.823, -0.309, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 22\n",
      "training loss: 9428.201598948284\n",
      "test loss: 2759.4199737472454\n",
      "Model: [513.225, 36.025, 7.605, 12.487, -3.642, -27.323, -18.855, 25.263, -26.059, -8.426, 32.195, 50.887, -1.432, -33.531, -7.349, -2.98, -14.803, -12.175, -0.736, 14.923, -13.989, 5.487, 16.982, 3.302, -7.995, -19.378, 1.365, -1.199, -1.242, -25.077, -37.865, -1.779, 2.094, 6.92, -24.302, -0.068, -19.754, 4.496, -2.633, -0.953, 3.542, -10.438, 18.29, 11.95, 10.201, -10.776, -2.86, 14.746, 13.582, 5.716, 14.854, -2.218, 14.788, -13.337, -0.549, 3.275, 17.287, 5.216, -2.626, -12.992, 1.413, -10.443, -18.127, -6.797, -2.847, 6.424, -7.372, 0.405, -6.356, 40.772, 6.789, -5.223, 13.648, -0.202, 13.922, 1.416, -6.067, -2.622, -17.264, 3.213, -17.234, 2.635, -5.347, -10.366, 22.763, 4.723, -5.065, -6.824, -11.445, 0.347, -8.456, -3.485, -19.47, 15.676, 3.974, 8.023, 15.104, 11.21, -37.354, 12.579, 11.219, 12.969, -2.69, 5.412, -1.131, 4.146, 39.148, 1.518, 4.506, -3.702, -13.601, -2.586, -2.458, -4.562, -15.193, 1.631, 7.519, 2.399, -7.248, -9.558, -0.21, -5.571, 1.176, -3.394, 1.331, -6.499, -1.703, 0.021, -2.472, -7.876, -15.55, 0.841, 0.203, -10.078, -4.634, -24.645, -7.484, 1.997, -3.431, -3.095, -3.538, -2.709, 0.913, -7.325, -0.511, 0.336, -9.07, 2.19, -4.609, 3.086, -6.713, 2.261, -4.564, -9.115, -11.176, 15.114, 11.374, 12.012, -19.328, -5.068, -11.627, 7.173, 6.818, 4.573, 6.02, 5.331, -0.064, 19.788, -7.001, -0.014, 12.164, -0.848, -0.375, 11.846, 28.7, -1.038, -10.198, -14.78, 3.423, 22.95, 11.834, -5.779, -8.277, 9.306, -11.528, -3.747, 10.93, 14.617, 0.649, -5.755, -12.962, -13.446, -13.431, -12.439, -3.379, -1.779, -5.453, -1.046, -10.32, -2.152, 1.505, 2.397, 0.595, 0.685, 4.343, 18.879, 21.088, 20.918, -3.311, 4.848, -0.067, -6.084, -2.661, 12.746, 14.683, 11.53, 12.081, 15.885, -0.926, -14.335, 0.977, 6.431, 18.769, -4.421, 3.129, 6.12, 8.361, 3.69, -3.162, -9.603, -8.197, -10.279, -14.94, -10.255, 7.458, 2.506, 11.281, -1.427, -6.344, -13.97, 5.722, -15.098, -14.606, -6.917, -2.013, 36.08, 39.383, 37.321, 38.957, 38.332, 31.846, -4.258, -5.952, 3.236, -11.43, -12.578, -0.641, -5.021, -0.258, 4.928, 8.823, -0.309, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 23\n",
      "training loss: 9428.200238943806\n",
      "test loss: 2759.3620078564045\n",
      "Model: [513.227, 36.025, 7.605, 12.487, -3.643, -27.322, -18.852, 25.263, -26.06, -8.427, 32.195, 50.887, -1.433, -33.532, -7.349, -2.98, -14.803, -12.174, -0.736, 14.925, -13.989, 5.486, 16.979, 3.301, -7.995, -19.378, 1.366, -1.2, -1.242, -25.076, -37.861, -1.778, 2.095, 6.921, -24.301, -0.065, -19.755, 4.497, -2.633, -0.952, 3.543, -10.437, 18.289, 11.95, 10.2, -10.776, -2.859, 14.746, 13.582, 5.716, 14.854, -2.221, 14.787, -13.337, -0.549, 3.274, 17.287, 5.215, -2.626, -12.991, 1.413, -10.443, -18.127, -6.797, -2.847, 6.424, -7.371, 0.405, -6.356, 40.771, 6.789, -5.224, 13.648, -0.202, 13.922, 1.416, -6.068, -2.622, -17.261, 3.213, -17.234, 2.635, -5.347, -10.366, 22.763, 4.723, -5.065, -6.825, -11.446, 0.347, -8.454, -3.485, -19.467, 15.676, 3.974, 8.023, 15.104, 11.209, -37.351, 12.578, 11.218, 12.968, -2.69, 5.411, -1.132, 4.146, 39.148, 1.518, 4.505, -3.703, -13.601, -2.586, -2.459, -4.562, -15.193, 1.631, 7.519, 2.399, -7.248, -9.559, -0.211, -5.571, 1.176, -3.394, 1.331, -6.499, -1.703, 0.021, -2.472, -7.876, -15.55, 0.841, 0.203, -10.078, -4.634, -24.645, -7.485, 1.997, -3.431, -3.095, -3.538, -2.705, 0.913, -7.325, -0.508, 0.336, -9.07, 2.189, -4.609, 3.086, -6.712, 2.261, -4.564, -9.115, -11.177, 15.114, 11.373, 12.012, -19.328, -5.067, -11.626, 7.173, 6.818, 4.573, 6.02, 5.331, -0.064, 19.788, -7.002, -0.014, 12.164, -0.849, -0.375, 11.845, 28.7, -1.038, -10.198, -14.78, 3.423, 22.949, 11.834, -5.78, -8.276, 9.305, -11.528, -3.747, 10.929, 14.617, 0.649, -5.756, -12.962, -13.446, -13.431, -12.439, -3.379, -1.779, -5.454, -1.046, -10.32, -2.153, 1.505, 2.397, 0.595, 0.685, 4.343, 18.879, 21.087, 20.917, -3.311, 4.848, -0.067, -6.084, -2.661, 12.746, 14.683, 11.53, 12.081, 15.884, -0.927, -14.335, 0.977, 6.431, 18.769, -4.422, 3.129, 6.12, 8.361, 3.69, -3.162, -9.603, -8.197, -10.279, -14.941, -10.255, 7.458, 2.506, 11.281, -1.427, -6.344, -13.97, 5.722, -15.099, -14.606, -6.917, -2.013, 36.08, 39.383, 37.32, 38.957, 38.332, 31.846, -4.258, -5.952, 3.236, -11.43, -12.578, -0.641, -5.021, -0.258, 4.928, 8.823, -0.309, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 24\n",
      "training loss: 9428.199169972173\n",
      "test loss: 2759.3102115253487\n",
      "Model: [513.228, 36.025, 7.605, 12.487, -3.643, -27.322, -18.85, 25.262, -26.06, -8.427, 32.196, 50.886, -1.433, -33.532, -7.35, -2.98, -14.802, -12.174, -0.736, 14.926, -13.988, 5.485, 16.977, 3.3, -7.995, -19.377, 1.366, -1.201, -1.242, -25.076, -37.857, -1.777, 2.095, 6.922, -24.301, -0.062, -19.755, 4.498, -2.634, -0.951, 3.544, -10.436, 18.289, 11.95, 10.2, -10.777, -2.859, 14.745, 13.581, 5.716, 14.853, -2.223, 14.786, -13.338, -0.55, 3.274, 17.287, 5.214, -2.626, -12.991, 1.413, -10.443, -18.128, -6.797, -2.847, 6.423, -7.37, 0.405, -6.356, 40.77, 6.789, -5.224, 13.647, -0.202, 13.921, 1.416, -6.068, -2.623, -17.259, 3.213, -17.234, 2.635, -5.348, -10.367, 22.762, 4.722, -5.065, -6.825, -11.446, 0.346, -8.452, -3.486, -19.465, 15.676, 3.973, 8.023, 15.104, 11.209, -37.349, 12.578, 11.218, 12.968, -2.69, 5.411, -1.132, 4.145, 39.147, 1.518, 4.505, -3.703, -13.602, -2.586, -2.459, -4.562, -15.193, 1.631, 7.519, 2.399, -7.249, -9.559, -0.211, -5.571, 1.175, -3.395, 1.33, -6.499, -1.704, 0.021, -2.472, -7.876, -15.55, 0.841, 0.202, -10.079, -4.634, -24.646, -7.485, 1.997, -3.431, -3.095, -3.539, -2.701, 0.913, -7.325, -0.505, 0.336, -9.07, 2.189, -4.609, 3.086, -6.711, 2.261, -4.564, -9.115, -11.177, 15.114, 11.373, 12.012, -19.328, -5.066, -11.626, 7.172, 6.818, 4.572, 6.02, 5.331, -0.065, 19.788, -7.002, -0.014, 12.164, -0.849, -0.375, 11.845, 28.7, -1.038, -10.198, -14.78, 3.423, 22.949, 11.834, -5.78, -8.275, 9.305, -11.528, -3.747, 10.929, 14.616, 0.649, -5.756, -12.962, -13.446, -13.431, -12.44, -3.379, -1.779, -5.454, -1.047, -10.32, -2.153, 1.505, 2.397, 0.595, 0.685, 4.342, 18.879, 21.087, 20.917, -3.311, 4.848, -0.068, -6.085, -2.661, 12.746, 14.683, 11.53, 12.08, 15.884, -0.927, -14.336, 0.977, 6.431, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.162, -9.603, -8.198, -10.279, -14.941, -10.256, 7.458, 2.506, 11.281, -1.427, -6.344, -13.971, 5.722, -15.099, -14.606, -6.917, -2.013, 36.08, 39.383, 37.32, 38.957, 38.332, 31.846, -4.258, -5.952, 3.236, -11.43, -12.578, -0.641, -5.021, -0.258, 4.928, 8.823, -0.309, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 25\n",
      "training loss: 9428.19830277321\n",
      "test loss: 2759.2632245839027\n",
      "Model: [513.229, 36.024, 7.605, 12.487, -3.644, -27.322, -18.847, 25.262, -26.061, -8.428, 32.196, 50.886, -1.433, -33.533, -7.35, -2.981, -14.802, -12.173, -0.736, 14.928, -13.988, 5.484, 16.974, 3.3, -7.995, -19.377, 1.366, -1.201, -1.242, -25.075, -37.854, -1.777, 2.096, 6.923, -24.3, -0.06, -19.755, 4.498, -2.635, -0.951, 3.545, -10.435, 18.289, 11.949, 10.199, -10.777, -2.858, 14.745, 13.581, 5.716, 14.853, -2.225, 14.785, -13.338, -0.55, 3.274, 17.287, 5.213, -2.626, -12.99, 1.412, -10.443, -18.128, -6.798, -2.847, 6.423, -7.369, 0.405, -6.356, 40.769, 6.789, -5.225, 13.646, -0.203, 13.921, 1.415, -6.068, -2.623, -17.257, 3.213, -17.235, 2.635, -5.348, -10.368, 22.762, 4.722, -5.065, -6.825, -11.446, 0.346, -8.451, -3.486, -19.463, 15.676, 3.973, 8.023, 15.104, 11.209, -37.347, 12.578, 11.218, 12.968, -2.69, 5.411, -1.132, 4.145, 39.147, 1.517, 4.505, -3.703, -13.602, -2.586, -2.459, -4.562, -15.193, 1.63, 7.519, 2.399, -7.249, -9.559, -0.211, -5.572, 1.175, -3.395, 1.33, -6.499, -1.704, 0.021, -2.472, -7.876, -15.55, 0.841, 0.202, -10.079, -4.634, -24.646, -7.485, 1.996, -3.431, -3.095, -3.539, -2.697, 0.913, -7.325, -0.503, 0.335, -9.07, 2.189, -4.609, 3.085, -6.711, 2.261, -4.564, -9.116, -11.177, 15.114, 11.373, 12.011, -19.328, -5.065, -11.625, 7.172, 6.818, 4.572, 6.02, 5.33, -0.065, 19.788, -7.002, -0.015, 12.163, -0.849, -0.375, 11.845, 28.7, -1.038, -10.198, -14.781, 3.423, 22.949, 11.834, -5.78, -8.274, 9.305, -11.528, -3.748, 10.929, 14.616, 0.649, -5.756, -12.962, -13.446, -13.431, -12.44, -3.379, -1.779, -5.454, -1.047, -10.32, -2.153, 1.505, 2.397, 0.595, 0.685, 4.342, 18.879, 21.087, 20.917, -3.311, 4.847, -0.068, -6.085, -2.661, 12.746, 14.683, 11.53, 12.08, 15.884, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.603, -8.198, -10.279, -14.941, -10.256, 7.458, 2.506, 11.281, -1.428, -6.344, -13.971, 5.722, -15.099, -14.606, -6.917, -2.013, 36.08, 39.383, 37.32, 38.956, 38.332, 31.846, -4.258, -5.953, 3.236, -11.43, -12.579, -0.641, -5.021, -0.259, 4.928, 8.823, -0.309, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 26\n",
      "training loss: 9428.19758191453\n",
      "test loss: 2759.2201072453713\n",
      "Model: [513.23, 36.024, 7.605, 12.487, -3.645, -27.321, -18.845, 25.262, -26.062, -8.428, 32.196, 50.885, -1.433, -33.533, -7.35, -2.981, -14.802, -12.173, -0.737, 14.929, -13.988, 5.484, 16.973, 3.299, -7.995, -19.376, 1.366, -1.202, -1.242, -25.075, -37.85, -1.776, 2.097, 6.923, -24.3, -0.058, -19.756, 4.499, -2.635, -0.95, 3.546, -10.435, 18.288, 11.949, 10.199, -10.778, -2.857, 14.745, 13.581, 5.716, 14.853, -2.227, 14.784, -13.338, -0.55, 3.274, 17.286, 5.213, -2.627, -12.99, 1.412, -10.444, -18.129, -6.798, -2.848, 6.423, -7.369, 0.404, -6.356, 40.768, 6.789, -5.225, 13.646, -0.203, 13.92, 1.415, -6.068, -2.623, -17.255, 3.213, -17.235, 2.635, -5.348, -10.368, 22.762, 4.722, -5.065, -6.825, -11.447, 0.346, -8.449, -3.486, -19.46, 15.675, 3.973, 8.023, 15.103, 11.209, -37.345, 12.578, 11.218, 12.968, -2.691, 5.411, -1.132, 4.145, 39.147, 1.517, 4.504, -3.703, -13.602, -2.586, -2.459, -4.562, -15.193, 1.63, 7.518, 2.398, -7.249, -9.56, -0.211, -5.572, 1.175, -3.395, 1.33, -6.499, -1.704, 0.021, -2.472, -7.877, -15.55, 0.841, 0.202, -10.079, -4.634, -24.646, -7.485, 1.996, -3.431, -3.095, -3.539, -2.694, 0.913, -7.325, -0.501, 0.335, -9.07, 2.189, -4.609, 3.085, -6.71, 2.26, -4.564, -9.116, -11.177, 15.114, 11.373, 12.011, -19.328, -5.064, -11.625, 7.172, 6.818, 4.572, 6.02, 5.33, -0.065, 19.788, -7.002, -0.015, 12.163, -0.849, -0.375, 11.845, 28.7, -1.038, -10.199, -14.781, 3.423, 22.948, 11.834, -5.78, -8.273, 9.305, -11.529, -3.748, 10.929, 14.616, 0.648, -5.756, -12.962, -13.446, -13.431, -12.44, -3.379, -1.779, -5.454, -1.047, -10.32, -2.153, 1.505, 2.397, 0.595, 0.685, 4.342, 18.878, 21.087, 20.917, -3.311, 4.847, -0.068, -6.085, -2.662, 12.746, 14.683, 11.529, 12.08, 15.884, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.603, -8.198, -10.279, -14.941, -10.256, 7.458, 2.506, 11.281, -1.428, -6.344, -13.971, 5.722, -15.099, -14.606, -6.917, -2.014, 36.08, 39.383, 37.32, 38.956, 38.332, 31.846, -4.258, -5.953, 3.236, -11.43, -12.579, -0.641, -5.022, -0.259, 4.928, 8.823, -0.309, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 27\n",
      "training loss: 9428.196971006386\n",
      "test loss: 2759.180180773195\n",
      "Model: [513.231, 36.024, 7.604, 12.487, -3.645, -27.321, -18.843, 25.262, -26.063, -8.429, 32.196, 50.885, -1.433, -33.534, -7.351, -2.981, -14.801, -12.172, -0.737, 14.93, -13.987, 5.483, 16.971, 3.299, -7.994, -19.376, 1.366, -1.203, -1.242, -25.075, -37.848, -1.776, 2.097, 6.924, -24.299, -0.056, -19.756, 4.499, -2.635, -0.95, 3.547, -10.434, 18.288, 11.949, 10.198, -10.778, -2.857, 14.744, 13.581, 5.716, 14.853, -2.229, 14.783, -13.339, -0.55, 3.274, 17.286, 5.212, -2.627, -12.99, 1.412, -10.444, -18.129, -6.798, -2.848, 6.423, -7.368, 0.404, -6.357, 40.767, 6.788, -5.225, 13.645, -0.203, 13.92, 1.415, -6.069, -2.624, -17.253, 3.213, -17.235, 2.635, -5.348, -10.369, 22.761, 4.721, -5.065, -6.825, -11.447, 0.346, -8.448, -3.486, -19.459, 15.675, 3.973, 8.023, 15.103, 11.209, -37.343, 12.578, 11.218, 12.967, -2.691, 5.411, -1.132, 4.145, 39.147, 1.517, 4.504, -3.703, -13.602, -2.586, -2.459, -4.562, -15.194, 1.63, 7.518, 2.398, -7.249, -9.56, -0.211, -5.572, 1.175, -3.395, 1.33, -6.499, -1.704, 0.021, -2.472, -7.877, -15.55, 0.84, 0.202, -10.079, -4.634, -24.646, -7.485, 1.996, -3.432, -3.095, -3.539, -2.692, 0.912, -7.326, -0.499, 0.335, -9.07, 2.189, -4.609, 3.085, -6.709, 2.26, -4.564, -9.116, -11.177, 15.114, 11.373, 12.011, -19.328, -5.063, -11.624, 7.172, 6.817, 4.572, 6.019, 5.33, -0.065, 19.788, -7.002, -0.015, 12.163, -0.849, -0.376, 11.845, 28.699, -1.038, -10.199, -14.781, 3.423, 22.948, 11.834, -5.78, -8.272, 9.304, -11.529, -3.748, 10.929, 14.616, 0.648, -5.756, -12.962, -13.446, -13.431, -12.44, -3.379, -1.779, -5.454, -1.047, -10.32, -2.153, 1.505, 2.397, 0.595, 0.684, 4.342, 18.878, 21.086, 20.917, -3.311, 4.847, -0.068, -6.085, -2.662, 12.746, 14.683, 11.529, 12.08, 15.884, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.604, -8.198, -10.279, -14.941, -10.256, 7.458, 2.505, 11.281, -1.428, -6.344, -13.971, 5.722, -15.099, -14.606, -6.917, -2.014, 36.08, 39.383, 37.32, 38.956, 38.332, 31.846, -4.258, -5.953, 3.236, -11.43, -12.579, -0.641, -5.022, -0.259, 4.928, 8.822, -0.309, 3.056, 5.967, -6.143, 10.942, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 28\n",
      "training loss: 9428.196445089188\n",
      "test loss: 2759.142937643768\n",
      "Model: [513.232, 36.024, 7.604, 12.487, -3.646, -27.321, -18.842, 25.261, -26.063, -8.429, 32.196, 50.884, -1.433, -33.534, -7.351, -2.981, -14.801, -12.172, -0.737, 14.931, -13.987, 5.482, 16.969, 3.298, -7.994, -19.375, 1.367, -1.203, -1.242, -25.074, -37.845, -1.775, 2.098, 6.925, -24.299, -0.054, -19.756, 4.5, -2.636, -0.949, 3.548, -10.433, 18.288, 11.949, 10.198, -10.778, -2.856, 14.744, 13.58, 5.716, 14.852, -2.23, 14.782, -13.339, -0.551, 3.273, 17.286, 5.211, -2.627, -12.989, 1.412, -10.444, -18.13, -6.798, -2.848, 6.423, -7.367, 0.404, -6.357, 40.766, 6.788, -5.226, 13.645, -0.203, 13.92, 1.415, -6.069, -2.624, -17.252, 3.213, -17.235, 2.634, -5.348, -10.369, 22.761, 4.721, -5.065, -6.825, -11.447, 0.345, -8.447, -3.486, -19.457, 15.675, 3.973, 8.023, 15.103, 11.209, -37.341, 12.578, 11.218, 12.967, -2.691, 5.411, -1.132, 4.145, 39.146, 1.517, 4.504, -3.703, -13.602, -2.586, -2.459, -4.562, -15.194, 1.63, 7.518, 2.398, -7.249, -9.56, -0.212, -5.572, 1.175, -3.395, 1.329, -6.499, -1.704, 0.021, -2.472, -7.877, -15.551, 0.84, 0.202, -10.079, -4.634, -24.647, -7.486, 1.996, -3.432, -3.096, -3.539, -2.689, 0.912, -7.326, -0.497, 0.334, -9.07, 2.189, -4.61, 3.085, -6.709, 2.26, -4.564, -9.116, -11.177, 15.114, 11.373, 12.011, -19.328, -5.062, -11.624, 7.172, 6.817, 4.572, 6.019, 5.33, -0.065, 19.788, -7.002, -0.015, 12.163, -0.849, -0.376, 11.845, 28.699, -1.038, -10.199, -14.781, 3.422, 22.948, 11.834, -5.78, -8.271, 9.304, -11.529, -3.748, 10.929, 14.616, 0.648, -5.756, -12.962, -13.446, -13.431, -12.44, -3.379, -1.779, -5.454, -1.047, -10.32, -2.153, 1.505, 2.396, 0.594, 0.684, 4.342, 18.878, 21.086, 20.916, -3.311, 4.847, -0.069, -6.085, -2.662, 12.746, 14.683, 11.529, 12.08, 15.884, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.604, -8.198, -10.279, -14.941, -10.256, 7.458, 2.505, 11.28, -1.428, -6.345, -13.971, 5.722, -15.099, -14.606, -6.917, -2.014, 36.08, 39.383, 37.32, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.641, -5.022, -0.259, 4.928, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 29\n",
      "training loss: 9428.1959864231\n",
      "test loss: 2759.1079875745736\n",
      "Model: [513.233, 36.024, 7.604, 12.487, -3.646, -27.32, -18.84, 25.261, -26.064, -8.429, 32.197, 50.884, -1.433, -33.535, -7.351, -2.981, -14.801, -12.171, -0.737, 14.932, -13.987, 5.482, 16.968, 3.298, -7.994, -19.375, 1.367, -1.203, -1.242, -25.074, -37.843, -1.775, 2.098, 6.926, -24.299, -0.053, -19.756, 4.5, -2.636, -0.949, 3.548, -10.432, 18.288, 11.948, 10.198, -10.778, -2.856, 14.744, 13.58, 5.715, 14.852, -2.232, 14.781, -13.339, -0.551, 3.273, 17.286, 5.211, -2.627, -12.989, 1.412, -10.444, -18.13, -6.798, -2.848, 6.423, -7.367, 0.404, -6.357, 40.765, 6.788, -5.226, 13.645, -0.203, 13.919, 1.415, -6.069, -2.624, -17.25, 3.213, -17.235, 2.634, -5.349, -10.37, 22.761, 4.721, -5.065, -6.825, -11.447, 0.345, -8.446, -3.487, -19.455, 15.675, 3.973, 8.022, 15.103, 11.208, -37.339, 12.577, 11.217, 12.967, -2.691, 5.41, -1.132, 4.145, 39.146, 1.517, 4.504, -3.703, -13.602, -2.587, -2.459, -4.562, -15.194, 1.63, 7.518, 2.398, -7.249, -9.561, -0.212, -5.572, 1.175, -3.396, 1.329, -6.499, -1.704, 0.021, -2.472, -7.877, -15.551, 0.84, 0.202, -10.079, -4.635, -24.647, -7.486, 1.996, -3.432, -3.096, -3.539, -2.687, 0.912, -7.326, -0.495, 0.334, -9.07, 2.189, -4.61, 3.085, -6.708, 2.26, -4.564, -9.116, -11.177, 15.114, 11.373, 12.011, -19.328, -5.061, -11.623, 7.172, 6.817, 4.572, 6.019, 5.33, -0.066, 19.787, -7.002, -0.015, 12.163, -0.849, -0.376, 11.845, 28.699, -1.038, -10.199, -14.781, 3.422, 22.948, 11.833, -5.78, -8.271, 9.304, -11.529, -3.748, 10.929, 14.616, 0.648, -5.756, -12.963, -13.446, -13.431, -12.44, -3.379, -1.779, -5.455, -1.047, -10.32, -2.153, 1.504, 2.396, 0.594, 0.684, 4.342, 18.878, 21.086, 20.916, -3.312, 4.847, -0.069, -6.085, -2.662, 12.745, 14.683, 11.529, 12.08, 15.883, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.604, -8.198, -10.279, -14.942, -10.256, 7.458, 2.505, 11.28, -1.428, -6.345, -13.971, 5.722, -15.099, -14.606, -6.917, -2.014, 36.08, 39.383, 37.32, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.641, -5.022, -0.259, 4.928, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 30\n",
      "training loss: 9428.195582022216\n",
      "test loss: 2759.0750234244574\n",
      "Model: [513.233, 36.023, 7.604, 12.487, -3.647, -27.32, -18.839, 25.261, -26.065, -8.43, 32.197, 50.884, -1.433, -33.535, -7.351, -2.982, -14.8, -12.171, -0.737, 14.933, -13.987, 5.481, 16.966, 3.297, -7.994, -19.375, 1.367, -1.204, -1.242, -25.073, -37.84, -1.774, 2.098, 6.926, -24.298, -0.051, -19.756, 4.5, -2.637, -0.949, 3.549, -10.432, 18.288, 11.948, 10.197, -10.779, -2.855, 14.744, 13.58, 5.715, 14.852, -2.233, 14.781, -13.339, -0.551, 3.273, 17.286, 5.21, -2.627, -12.988, 1.412, -10.444, -18.131, -6.798, -2.848, 6.422, -7.366, 0.404, -6.357, 40.764, 6.788, -5.227, 13.644, -0.203, 13.919, 1.415, -6.069, -2.624, -17.249, 3.212, -17.235, 2.634, -5.349, -10.37, 22.76, 4.721, -5.066, -6.825, -11.447, 0.345, -8.445, -3.487, -19.453, 15.675, 3.973, 8.022, 15.103, 11.208, -37.337, 12.577, 11.217, 12.967, -2.691, 5.41, -1.132, 4.145, 39.146, 1.517, 4.503, -3.703, -13.602, -2.587, -2.459, -4.562, -15.194, 1.63, 7.518, 2.398, -7.249, -9.561, -0.212, -5.572, 1.174, -3.396, 1.329, -6.499, -1.704, 0.021, -2.472, -7.877, -15.551, 0.84, 0.202, -10.08, -4.635, -24.647, -7.486, 1.996, -3.432, -3.096, -3.539, -2.685, 0.912, -7.326, -0.493, 0.334, -9.071, 2.189, -4.61, 3.085, -6.707, 2.26, -4.564, -9.116, -11.178, 15.114, 11.373, 12.011, -19.328, -5.061, -11.623, 7.172, 6.817, 4.572, 6.019, 5.33, -0.066, 19.787, -7.003, -0.015, 12.163, -0.849, -0.376, 11.845, 28.699, -1.038, -10.199, -14.781, 3.422, 22.948, 11.833, -5.78, -8.27, 9.304, -11.529, -3.748, 10.929, 14.616, 0.648, -5.756, -12.963, -13.446, -13.432, -12.44, -3.38, -1.78, -5.455, -1.047, -10.32, -2.153, 1.504, 2.396, 0.594, 0.684, 4.342, 18.878, 21.086, 20.916, -3.312, 4.847, -0.069, -6.085, -2.662, 12.745, 14.683, 11.529, 12.08, 15.883, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.604, -8.198, -10.279, -14.942, -10.257, 7.458, 2.505, 11.28, -1.428, -6.345, -13.971, 5.722, -15.099, -14.606, -6.917, -2.014, 36.08, 39.382, 37.32, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.641, -5.022, -0.259, 4.928, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 31\n",
      "training loss: 9428.195222141267\n",
      "test loss: 2759.0437987465875\n",
      "Model: [513.234, 36.023, 7.604, 12.487, -3.647, -27.32, -18.837, 25.261, -26.065, -8.43, 32.197, 50.883, -1.433, -33.535, -7.352, -2.982, -14.8, -12.17, -0.737, 14.933, -13.986, 5.481, 16.965, 3.297, -7.994, -19.374, 1.367, -1.204, -1.242, -25.073, -37.838, -1.774, 2.099, 6.927, -24.298, -0.05, -19.757, 4.501, -2.637, -0.949, 3.549, -10.431, 18.287, 11.948, 10.197, -10.779, -2.855, 14.744, 13.58, 5.715, 14.852, -2.234, 14.78, -13.34, -0.551, 3.273, 17.286, 5.21, -2.627, -12.988, 1.412, -10.444, -18.131, -6.798, -2.848, 6.422, -7.366, 0.404, -6.357, 40.764, 6.788, -5.227, 13.644, -0.204, 13.919, 1.415, -6.07, -2.624, -17.248, 3.212, -17.235, 2.634, -5.349, -10.37, 22.76, 4.721, -5.066, -6.825, -11.448, 0.345, -8.444, -3.487, -19.452, 15.675, 3.972, 8.022, 15.103, 11.208, -37.336, 12.577, 11.217, 12.967, -2.691, 5.41, -1.132, 4.145, 39.146, 1.517, 4.503, -3.703, -13.603, -2.587, -2.459, -4.563, -15.194, 1.629, 7.518, 2.398, -7.25, -9.561, -0.212, -5.572, 1.174, -3.396, 1.329, -6.5, -1.704, 0.021, -2.472, -7.877, -15.551, 0.84, 0.202, -10.08, -4.635, -24.647, -7.486, 1.996, -3.432, -3.096, -3.539, -2.683, 0.912, -7.326, -0.492, 0.334, -9.071, 2.189, -4.61, 3.085, -6.707, 2.26, -4.564, -9.116, -11.178, 15.114, 11.373, 12.011, -19.328, -5.06, -11.622, 7.172, 6.817, 4.572, 6.019, 5.33, -0.066, 19.787, -7.003, -0.015, 12.163, -0.85, -0.376, 11.844, 28.699, -1.038, -10.2, -14.781, 3.422, 22.948, 11.833, -5.78, -8.269, 9.304, -11.529, -3.748, 10.929, 14.616, 0.648, -5.757, -12.963, -13.446, -13.432, -12.44, -3.38, -1.78, -5.455, -1.047, -10.321, -2.153, 1.504, 2.396, 0.594, 0.684, 4.342, 18.878, 21.086, 20.916, -3.312, 4.846, -0.069, -6.085, -2.662, 12.745, 14.682, 11.529, 12.08, 15.883, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.604, -8.198, -10.279, -14.942, -10.257, 7.457, 2.505, 11.28, -1.428, -6.345, -13.971, 5.722, -15.099, -14.607, -6.917, -2.014, 36.08, 39.382, 37.32, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.641, -5.022, -0.259, 4.928, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 32\n",
      "training loss: 9428.194899309785\n",
      "test loss: 2759.0141124987576\n",
      "Model: [513.235, 36.023, 7.604, 12.487, -3.647, -27.319, -18.836, 25.261, -26.066, -8.43, 32.197, 50.883, -1.433, -33.536, -7.352, -2.982, -14.8, -12.17, -0.737, 14.934, -13.986, 5.48, 16.964, 3.296, -7.994, -19.374, 1.367, -1.205, -1.242, -25.073, -37.836, -1.774, 2.099, 6.927, -24.298, -0.048, -19.757, 4.501, -2.637, -0.948, 3.55, -10.43, 18.287, 11.948, 10.197, -10.779, -2.854, 14.743, 13.579, 5.715, 14.852, -2.235, 14.779, -13.34, -0.551, 3.273, 17.286, 5.209, -2.627, -12.987, 1.412, -10.444, -18.131, -6.799, -2.849, 6.422, -7.365, 0.404, -6.357, 40.763, 6.788, -5.227, 13.643, -0.204, 13.918, 1.415, -6.07, -2.625, -17.247, 3.212, -17.236, 2.634, -5.349, -10.371, 22.76, 4.72, -5.066, -6.825, -11.448, 0.345, -8.443, -3.487, -19.451, 15.675, 3.972, 8.022, 15.103, 11.208, -37.334, 12.577, 11.217, 12.967, -2.691, 5.41, -1.132, 4.145, 39.146, 1.517, 4.503, -3.703, -13.603, -2.587, -2.459, -4.563, -15.194, 1.629, 7.518, 2.398, -7.25, -9.562, -0.212, -5.572, 1.174, -3.396, 1.329, -6.5, -1.705, 0.02, -2.473, -7.877, -15.551, 0.84, 0.202, -10.08, -4.635, -24.647, -7.486, 1.996, -3.432, -3.096, -3.54, -2.681, 0.912, -7.326, -0.491, 0.333, -9.071, 2.189, -4.61, 3.085, -6.706, 2.26, -4.565, -9.116, -11.178, 15.113, 11.372, 12.011, -19.328, -5.059, -11.622, 7.172, 6.817, 4.572, 6.019, 5.33, -0.066, 19.787, -7.003, -0.015, 12.163, -0.85, -0.376, 11.844, 28.699, -1.038, -10.2, -14.781, 3.422, 22.947, 11.833, -5.78, -8.268, 9.304, -11.529, -3.748, 10.929, 14.616, 0.648, -5.757, -12.963, -13.447, -13.432, -12.44, -3.38, -1.78, -5.455, -1.048, -10.321, -2.153, 1.504, 2.396, 0.594, 0.684, 4.342, 18.878, 21.086, 20.916, -3.312, 4.846, -0.069, -6.085, -2.662, 12.745, 14.682, 11.529, 12.08, 15.883, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.604, -8.198, -10.279, -14.942, -10.257, 7.457, 2.505, 11.28, -1.428, -6.345, -13.971, 5.722, -15.099, -14.607, -6.918, -2.014, 36.079, 39.382, 37.32, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.642, -5.022, -0.259, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 33\n",
      "training loss: 9428.19460769487\n",
      "test loss: 2758.9857983234992\n",
      "Model: [513.236, 36.023, 7.604, 12.487, -3.648, -27.319, -18.835, 25.261, -26.066, -8.43, 32.197, 50.883, -1.434, -33.536, -7.352, -2.982, -14.799, -12.17, -0.737, 14.935, -13.986, 5.48, 16.963, 3.296, -7.994, -19.374, 1.367, -1.205, -1.242, -25.073, -37.835, -1.773, 2.1, 6.928, -24.297, -0.047, -19.757, 4.501, -2.637, -0.948, 3.551, -10.43, 18.287, 11.948, 10.196, -10.779, -2.854, 14.743, 13.579, 5.715, 14.851, -2.237, 14.779, -13.34, -0.551, 3.273, 17.285, 5.208, -2.627, -12.987, 1.411, -10.444, -18.132, -6.799, -2.849, 6.422, -7.365, 0.404, -6.357, 40.762, 6.787, -5.227, 13.643, -0.204, 13.918, 1.415, -6.07, -2.625, -17.245, 3.212, -17.236, 2.634, -5.349, -10.371, 22.76, 4.72, -5.066, -6.825, -11.448, 0.344, -8.442, -3.487, -19.449, 15.674, 3.972, 8.022, 15.102, 11.208, -37.332, 12.577, 11.217, 12.966, -2.691, 5.41, -1.132, 4.145, 39.146, 1.517, 4.503, -3.703, -13.603, -2.587, -2.459, -4.563, -15.194, 1.629, 7.518, 2.397, -7.25, -9.562, -0.212, -5.572, 1.174, -3.396, 1.329, -6.5, -1.705, 0.02, -2.473, -7.877, -15.551, 0.839, 0.201, -10.08, -4.635, -24.647, -7.486, 1.996, -3.432, -3.096, -3.54, -2.679, 0.912, -7.326, -0.489, 0.333, -9.071, 2.189, -4.61, 3.085, -6.706, 2.26, -4.565, -9.116, -11.178, 15.113, 11.372, 12.011, -19.328, -5.058, -11.621, 7.172, 6.817, 4.572, 6.019, 5.33, -0.066, 19.787, -7.003, -0.015, 12.163, -0.85, -0.376, 11.844, 28.699, -1.038, -10.2, -14.781, 3.422, 22.947, 11.833, -5.78, -8.268, 9.304, -11.53, -3.748, 10.929, 14.616, 0.648, -5.757, -12.963, -13.447, -13.432, -12.44, -3.38, -1.78, -5.455, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.684, 4.342, 18.878, 21.086, 20.916, -3.312, 4.846, -0.069, -6.085, -2.662, 12.745, 14.682, 11.529, 12.08, 15.883, -0.927, -14.336, 0.977, 6.43, 18.769, -4.422, 3.128, 6.119, 8.361, 3.69, -3.163, -9.604, -8.198, -10.279, -14.942, -10.257, 7.457, 2.505, 11.28, -1.428, -6.345, -13.971, 5.721, -15.099, -14.607, -6.918, -2.014, 36.079, 39.382, 37.32, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.642, -5.022, -0.259, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 34\n",
      "training loss: 9428.194342668512\n",
      "test loss: 2758.958716843499\n",
      "Model: [513.236, 36.023, 7.603, 12.487, -3.648, -27.319, -18.834, 25.26, -26.067, -8.431, 32.197, 50.882, -1.434, -33.536, -7.352, -2.982, -14.799, -12.169, -0.737, 14.936, -13.986, 5.48, 16.962, 3.296, -7.994, -19.374, 1.367, -1.205, -1.242, -25.072, -37.833, -1.773, 2.1, 6.928, -24.297, -0.046, -19.757, 4.502, -2.637, -0.948, 3.551, -10.429, 18.287, 11.947, 10.196, -10.779, -2.853, 14.743, 13.579, 5.715, 14.851, -2.238, 14.778, -13.34, -0.551, 3.273, 17.285, 5.208, -2.627, -12.986, 1.411, -10.444, -18.132, -6.799, -2.849, 6.422, -7.364, 0.404, -6.357, 40.762, 6.787, -5.228, 13.643, -0.204, 13.918, 1.415, -6.07, -2.625, -17.244, 3.212, -17.236, 2.634, -5.349, -10.372, 22.759, 4.72, -5.066, -6.826, -11.448, 0.344, -8.441, -3.487, -19.448, 15.674, 3.972, 8.022, 15.102, 11.208, -37.33, 12.577, 11.217, 12.966, -2.691, 5.41, -1.132, 4.145, 39.146, 1.517, 4.503, -3.704, -13.603, -2.587, -2.46, -4.563, -15.194, 1.629, 7.518, 2.397, -7.25, -9.562, -0.212, -5.572, 1.174, -3.396, 1.329, -6.5, -1.705, 0.02, -2.473, -7.877, -15.551, 0.839, 0.201, -10.08, -4.635, -24.648, -7.486, 1.996, -3.432, -3.096, -3.54, -2.677, 0.912, -7.326, -0.488, 0.333, -9.071, 2.189, -4.61, 3.085, -6.705, 2.26, -4.565, -9.116, -11.178, 15.113, 11.372, 12.011, -19.328, -5.058, -11.621, 7.172, 6.817, 4.572, 6.019, 5.33, -0.066, 19.787, -7.003, -0.015, 12.162, -0.85, -0.376, 11.844, 28.699, -1.038, -10.2, -14.781, 3.422, 22.947, 11.833, -5.78, -8.267, 9.303, -11.53, -3.748, 10.929, 14.616, 0.648, -5.757, -12.963, -13.447, -13.432, -12.44, -3.38, -1.78, -5.455, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.684, 4.342, 18.877, 21.085, 20.916, -3.312, 4.846, -0.07, -6.085, -2.662, 12.745, 14.682, 11.529, 12.08, 15.883, -0.928, -14.336, 0.977, 6.43, 18.769, -4.422, 3.127, 6.119, 8.361, 3.69, -3.163, -9.604, -8.198, -10.28, -14.942, -10.257, 7.457, 2.505, 11.28, -1.429, -6.345, -13.972, 5.721, -15.099, -14.607, -6.918, -2.014, 36.079, 39.382, 37.32, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.642, -5.022, -0.259, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 35\n",
      "training loss: 9428.194100506404\n",
      "test loss: 2758.932750003629\n",
      "Model: [513.237, 36.023, 7.603, 12.487, -3.648, -27.319, -18.833, 25.26, -26.067, -8.431, 32.197, 50.882, -1.434, -33.537, -7.352, -2.982, -14.799, -12.169, -0.738, 14.936, -13.986, 5.479, 16.961, 3.295, -7.994, -19.373, 1.367, -1.205, -1.242, -25.072, -37.831, -1.773, 2.1, 6.929, -24.297, -0.045, -19.757, 4.502, -2.638, -0.948, 3.552, -10.429, 18.287, 11.947, 10.196, -10.779, -2.853, 14.743, 13.579, 5.715, 14.851, -2.238, 14.778, -13.34, -0.551, 3.273, 17.285, 5.208, -2.627, -12.986, 1.411, -10.444, -18.132, -6.799, -2.849, 6.422, -7.364, 0.403, -6.357, 40.761, 6.787, -5.228, 13.643, -0.204, 13.918, 1.415, -6.07, -2.625, -17.243, 3.212, -17.236, 2.634, -5.35, -10.372, 22.759, 4.72, -5.066, -6.826, -11.448, 0.344, -8.441, -3.487, -19.447, 15.674, 3.972, 8.022, 15.102, 11.208, -37.329, 12.577, 11.217, 12.966, -2.691, 5.41, -1.132, 4.145, 39.145, 1.517, 4.503, -3.704, -13.603, -2.587, -2.46, -4.563, -15.194, 1.629, 7.518, 2.397, -7.25, -9.562, -0.213, -5.572, 1.174, -3.396, 1.328, -6.5, -1.705, 0.02, -2.473, -7.878, -15.551, 0.839, 0.201, -10.08, -4.635, -24.648, -7.487, 1.995, -3.432, -3.096, -3.54, -2.676, 0.912, -7.326, -0.487, 0.333, -9.071, 2.189, -4.61, 3.084, -6.705, 2.26, -4.565, -9.117, -11.178, 15.113, 11.372, 12.01, -19.328, -5.057, -11.621, 7.172, 6.817, 4.572, 6.019, 5.33, -0.067, 19.787, -7.003, -0.016, 12.162, -0.85, -0.377, 11.844, 28.698, -1.038, -10.2, -14.781, 3.422, 22.947, 11.833, -5.78, -8.266, 9.303, -11.53, -3.748, 10.928, 14.616, 0.648, -5.757, -12.963, -13.447, -13.432, -12.44, -3.38, -1.78, -5.455, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.684, 4.342, 18.877, 21.085, 20.916, -3.312, 4.846, -0.07, -6.085, -2.662, 12.745, 14.682, 11.529, 12.08, 15.883, -0.928, -14.336, 0.977, 6.429, 18.769, -4.423, 3.127, 6.118, 8.361, 3.69, -3.163, -9.604, -8.198, -10.28, -14.942, -10.257, 7.457, 2.505, 11.28, -1.429, -6.345, -13.972, 5.721, -15.099, -14.607, -6.918, -2.014, 36.079, 39.382, 37.32, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.642, -5.022, -0.259, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 36\n",
      "training loss: 9428.193878173633\n",
      "test loss: 2758.907796836785\n",
      "Model: [513.238, 36.023, 7.603, 12.487, -3.649, -27.319, -18.832, 25.26, -26.068, -8.431, 32.198, 50.882, -1.434, -33.537, -7.353, -2.982, -14.799, -12.169, -0.738, 14.937, -13.985, 5.479, 16.96, 3.295, -7.994, -19.373, 1.367, -1.206, -1.242, -25.072, -37.83, -1.772, 2.101, 6.929, -24.297, -0.044, -19.757, 4.502, -2.638, -0.948, 3.552, -10.428, 18.287, 11.947, 10.196, -10.779, -2.853, 14.743, 13.579, 5.715, 14.851, -2.239, 14.777, -13.341, -0.551, 3.273, 17.285, 5.207, -2.627, -12.986, 1.411, -10.444, -18.132, -6.799, -2.849, 6.422, -7.363, 0.403, -6.357, 40.761, 6.787, -5.228, 13.642, -0.204, 13.918, 1.415, -6.07, -2.625, -17.243, 3.212, -17.236, 2.634, -5.35, -10.372, 22.759, 4.72, -5.066, -6.826, -11.448, 0.344, -8.44, -3.487, -19.445, 15.674, 3.972, 8.022, 15.102, 11.208, -37.327, 12.577, 11.217, 12.966, -2.692, 5.41, -1.132, 4.144, 39.145, 1.517, 4.502, -3.704, -13.603, -2.587, -2.46, -4.563, -15.195, 1.629, 7.518, 2.397, -7.25, -9.562, -0.213, -5.572, 1.174, -3.397, 1.328, -6.5, -1.705, 0.02, -2.473, -7.878, -15.551, 0.839, 0.201, -10.08, -4.635, -24.648, -7.487, 1.995, -3.433, -3.097, -3.54, -2.674, 0.912, -7.326, -0.486, 0.333, -9.071, 2.189, -4.61, 3.084, -6.704, 2.26, -4.565, -9.117, -11.178, 15.113, 11.372, 12.01, -19.328, -5.056, -11.62, 7.171, 6.817, 4.571, 6.019, 5.33, -0.067, 19.787, -7.003, -0.016, 12.162, -0.85, -0.377, 11.844, 28.698, -1.038, -10.2, -14.781, 3.422, 22.947, 11.833, -5.78, -8.266, 9.303, -11.53, -3.748, 10.928, 14.616, 0.648, -5.757, -12.963, -13.447, -13.432, -12.441, -3.38, -1.78, -5.455, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.684, 4.341, 18.877, 21.085, 20.915, -3.312, 4.846, -0.07, -6.085, -2.662, 12.745, 14.682, 11.529, 12.08, 15.883, -0.928, -14.336, 0.977, 6.429, 18.769, -4.423, 3.127, 6.118, 8.361, 3.69, -3.163, -9.604, -8.198, -10.28, -14.942, -10.257, 7.457, 2.505, 11.28, -1.429, -6.345, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.956, 38.331, 31.845, -4.258, -5.953, 3.236, -11.431, -12.579, -0.642, -5.022, -0.259, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 37\n",
      "training loss: 9428.193673169215\n",
      "test loss: 2758.8837702418746\n",
      "Model: [513.238, 36.022, 7.603, 12.487, -3.649, -27.318, -18.831, 25.26, -26.068, -8.431, 32.198, 50.881, -1.434, -33.537, -7.353, -2.983, -14.799, -12.169, -0.738, 14.937, -13.985, 5.479, 16.959, 3.295, -7.994, -19.373, 1.367, -1.206, -1.242, -25.072, -37.828, -1.772, 2.101, 6.93, -24.296, -0.043, -19.758, 4.503, -2.638, -0.947, 3.552, -10.428, 18.286, 11.947, 10.195, -10.779, -2.852, 14.743, 13.579, 5.715, 14.851, -2.24, 14.777, -13.341, -0.552, 3.273, 17.285, 5.207, -2.627, -12.985, 1.411, -10.444, -18.133, -6.799, -2.849, 6.421, -7.363, 0.403, -6.358, 40.76, 6.787, -5.229, 13.642, -0.204, 13.917, 1.415, -6.071, -2.626, -17.242, 3.212, -17.236, 2.634, -5.35, -10.372, 22.759, 4.72, -5.066, -6.826, -11.449, 0.344, -8.439, -3.487, -19.444, 15.674, 3.972, 8.022, 15.102, 11.208, -37.326, 12.577, 11.217, 12.966, -2.692, 5.41, -1.132, 4.144, 39.145, 1.517, 4.502, -3.704, -13.603, -2.587, -2.46, -4.563, -15.195, 1.629, 7.518, 2.397, -7.25, -9.563, -0.213, -5.572, 1.174, -3.397, 1.328, -6.5, -1.705, 0.02, -2.473, -7.878, -15.551, 0.839, 0.201, -10.08, -4.635, -24.648, -7.487, 1.995, -3.433, -3.097, -3.54, -2.673, 0.912, -7.326, -0.485, 0.332, -9.071, 2.189, -4.61, 3.084, -6.704, 2.259, -4.565, -9.117, -11.178, 15.113, 11.372, 12.01, -19.328, -5.056, -11.62, 7.171, 6.817, 4.571, 6.019, 5.33, -0.067, 19.787, -7.003, -0.016, 12.162, -0.85, -0.377, 11.844, 28.698, -1.038, -10.2, -14.782, 3.422, 22.947, 11.833, -5.78, -8.265, 9.303, -11.53, -3.748, 10.928, 14.616, 0.648, -5.757, -12.963, -13.447, -13.432, -12.441, -3.38, -1.78, -5.455, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.684, 4.341, 18.877, 21.085, 20.915, -3.312, 4.846, -0.07, -6.085, -2.663, 12.745, 14.682, 11.529, 12.079, 15.883, -0.928, -14.336, 0.977, 6.429, 18.769, -4.423, 3.127, 6.118, 8.36, 3.69, -3.163, -9.604, -8.199, -10.28, -14.943, -10.257, 7.457, 2.505, 11.28, -1.429, -6.345, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.956, 38.331, 31.845, -4.258, -5.953, 3.235, -11.431, -12.579, -0.642, -5.022, -0.259, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 38\n",
      "training loss: 9428.193483411282\n",
      "test loss: 2758.8605944952606\n",
      "Model: [513.239, 36.022, 7.603, 12.487, -3.649, -27.318, -18.83, 25.26, -26.068, -8.432, 32.198, 50.881, -1.434, -33.537, -7.353, -2.983, -14.798, -12.168, -0.738, 14.938, -13.985, 5.478, 16.958, 3.294, -7.994, -19.373, 1.368, -1.206, -1.242, -25.071, -37.827, -1.772, 2.101, 6.93, -24.296, -0.042, -19.758, 4.503, -2.638, -0.947, 3.553, -10.427, 18.286, 11.947, 10.195, -10.779, -2.852, 14.743, 13.579, 5.715, 14.851, -2.241, 14.776, -13.341, -0.552, 3.272, 17.285, 5.206, -2.627, -12.985, 1.411, -10.444, -18.133, -6.799, -2.849, 6.421, -7.362, 0.403, -6.358, 40.76, 6.787, -5.229, 13.642, -0.205, 13.917, 1.415, -6.071, -2.626, -17.241, 3.212, -17.236, 2.634, -5.35, -10.373, 22.758, 4.719, -5.066, -6.826, -11.449, 0.344, -8.439, -3.487, -19.443, 15.674, 3.972, 8.022, 15.102, 11.207, -37.324, 12.576, 11.216, 12.966, -2.692, 5.41, -1.132, 4.144, 39.145, 1.516, 4.502, -3.704, -13.603, -2.587, -2.46, -4.563, -15.195, 1.629, 7.517, 2.397, -7.25, -9.563, -0.213, -5.572, 1.174, -3.397, 1.328, -6.5, -1.705, 0.02, -2.473, -7.878, -15.551, 0.839, 0.201, -10.081, -4.635, -24.648, -7.487, 1.995, -3.433, -3.097, -3.54, -2.671, 0.912, -7.326, -0.484, 0.332, -9.071, 2.188, -4.61, 3.084, -6.703, 2.259, -4.565, -9.117, -11.178, 15.113, 11.372, 12.01, -19.328, -5.055, -11.619, 7.171, 6.817, 4.571, 6.019, 5.329, -0.067, 19.787, -7.003, -0.016, 12.162, -0.85, -0.377, 11.844, 28.698, -1.038, -10.201, -14.782, 3.422, 22.947, 11.833, -5.78, -8.264, 9.303, -11.53, -3.748, 10.928, 14.616, 0.648, -5.757, -12.963, -13.447, -13.432, -12.441, -3.38, -1.78, -5.455, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.684, 4.341, 18.877, 21.085, 20.915, -3.312, 4.846, -0.07, -6.085, -2.663, 12.745, 14.682, 11.529, 12.079, 15.883, -0.928, -14.336, 0.977, 6.429, 18.769, -4.423, 3.127, 6.118, 8.36, 3.69, -3.163, -9.604, -8.199, -10.28, -14.943, -10.258, 7.457, 2.505, 11.28, -1.429, -6.345, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.956, 38.331, 31.845, -4.259, -5.953, 3.235, -11.431, -12.579, -0.642, -5.022, -0.259, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 39\n",
      "training loss: 9428.193307151048\n",
      "test loss: 2758.838203302853\n",
      "Model: [513.239, 36.022, 7.603, 12.487, -3.65, -27.318, -18.829, 25.26, -26.069, -8.432, 32.198, 50.881, -1.434, -33.538, -7.353, -2.983, -14.798, -12.168, -0.738, 14.938, -13.985, 5.478, 16.958, 3.294, -7.994, -19.372, 1.368, -1.207, -1.242, -25.071, -37.825, -1.771, 2.101, 6.931, -24.296, -0.041, -19.758, 4.503, -2.638, -0.947, 3.553, -10.427, 18.286, 11.947, 10.195, -10.779, -2.851, 14.742, 13.578, 5.715, 14.851, -2.242, 14.776, -13.341, -0.552, 3.272, 17.285, 5.206, -2.627, -12.984, 1.411, -10.444, -18.133, -6.799, -2.849, 6.421, -7.362, 0.403, -6.358, 40.759, 6.787, -5.229, 13.641, -0.205, 13.917, 1.415, -6.071, -2.626, -17.24, 3.212, -17.236, 2.634, -5.35, -10.373, 22.758, 4.719, -5.066, -6.826, -11.449, 0.344, -8.438, -3.488, -19.442, 15.674, 3.972, 8.022, 15.102, 11.207, -37.323, 12.576, 11.216, 12.966, -2.692, 5.409, -1.133, 4.144, 39.145, 1.516, 4.502, -3.704, -13.603, -2.587, -2.46, -4.563, -15.195, 1.629, 7.517, 2.397, -7.25, -9.563, -0.213, -5.572, 1.174, -3.397, 1.328, -6.5, -1.705, 0.02, -2.473, -7.878, -15.551, 0.839, 0.201, -10.081, -4.635, -24.648, -7.487, 1.995, -3.433, -3.097, -3.54, -2.67, 0.911, -7.326, -0.483, 0.332, -9.071, 2.188, -4.61, 3.084, -6.703, 2.259, -4.565, -9.117, -11.178, 15.113, 11.372, 12.01, -19.328, -5.055, -11.619, 7.171, 6.817, 4.571, 6.019, 5.329, -0.067, 19.787, -7.003, -0.016, 12.162, -0.85, -0.377, 11.844, 28.698, -1.039, -10.201, -14.782, 3.422, 22.946, 11.833, -5.781, -8.264, 9.303, -11.53, -3.748, 10.928, 14.616, 0.647, -5.757, -12.963, -13.447, -13.432, -12.441, -3.38, -1.78, -5.456, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.684, 4.341, 18.877, 21.085, 20.915, -3.312, 4.846, -0.07, -6.085, -2.663, 12.745, 14.682, 11.529, 12.079, 15.882, -0.928, -14.336, 0.977, 6.429, 18.769, -4.423, 3.127, 6.118, 8.36, 3.69, -3.163, -9.604, -8.199, -10.28, -14.943, -10.258, 7.457, 2.505, 11.28, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.955, 38.331, 31.844, -4.259, -5.953, 3.235, -11.431, -12.579, -0.642, -5.022, -0.259, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 40\n",
      "training loss: 9428.193142907354\n",
      "test loss: 2758.8165382569414\n",
      "Model: [513.24, 36.022, 7.603, 12.487, -3.65, -27.318, -18.828, 25.26, -26.069, -8.432, 32.198, 50.881, -1.434, -33.538, -7.353, -2.983, -14.798, -12.168, -0.738, 14.939, -13.985, 5.478, 16.957, 3.294, -7.994, -19.372, 1.368, -1.207, -1.243, -25.071, -37.824, -1.771, 2.102, 6.931, -24.296, -0.04, -19.758, 4.503, -2.638, -0.947, 3.554, -10.426, 18.286, 11.946, 10.195, -10.78, -2.851, 14.742, 13.578, 5.715, 14.85, -2.243, 14.775, -13.341, -0.552, 3.272, 17.285, 5.205, -2.628, -12.984, 1.411, -10.444, -18.134, -6.799, -2.849, 6.421, -7.362, 0.403, -6.358, 40.759, 6.787, -5.229, 13.641, -0.205, 13.917, 1.414, -6.071, -2.626, -17.239, 3.212, -17.236, 2.634, -5.35, -10.373, 22.758, 4.719, -5.066, -6.826, -11.449, 0.344, -8.438, -3.488, -19.441, 15.674, 3.972, 8.022, 15.102, 11.207, -37.321, 12.576, 11.216, 12.966, -2.692, 5.409, -1.133, 4.144, 39.145, 1.516, 4.502, -3.704, -13.603, -2.587, -2.46, -4.563, -15.195, 1.629, 7.517, 2.397, -7.25, -9.563, -0.213, -5.572, 1.173, -3.397, 1.328, -6.5, -1.705, 0.02, -2.473, -7.878, -15.551, 0.839, 0.201, -10.081, -4.635, -24.648, -7.487, 1.995, -3.433, -3.097, -3.54, -2.669, 0.911, -7.326, -0.482, 0.332, -9.071, 2.188, -4.61, 3.084, -6.702, 2.259, -4.565, -9.117, -11.178, 15.113, 11.372, 12.01, -19.328, -5.054, -11.619, 7.171, 6.817, 4.571, 6.019, 5.329, -0.067, 19.787, -7.003, -0.016, 12.162, -0.85, -0.377, 11.844, 28.698, -1.039, -10.201, -14.782, 3.422, 22.946, 11.833, -5.781, -8.263, 9.303, -11.53, -3.749, 10.928, 14.616, 0.647, -5.757, -12.963, -13.447, -13.432, -12.441, -3.38, -1.78, -5.456, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.683, 4.341, 18.877, 21.085, 20.915, -3.312, 4.845, -0.07, -6.085, -2.663, 12.745, 14.682, 11.529, 12.079, 15.882, -0.928, -14.337, 0.977, 6.429, 18.769, -4.423, 3.127, 6.118, 8.36, 3.69, -3.163, -9.604, -8.199, -10.28, -14.943, -10.258, 7.457, 2.505, 11.28, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.955, 38.331, 31.844, -4.259, -5.953, 3.235, -11.431, -12.579, -0.642, -5.022, -0.26, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 41\n",
      "training loss: 9428.192989416237\n",
      "test loss: 2758.7955476002267\n",
      "Model: [513.24, 36.022, 7.603, 12.487, -3.65, -27.318, -18.828, 25.259, -26.07, -8.432, 32.198, 50.88, -1.434, -33.538, -7.353, -2.983, -14.798, -12.167, -0.738, 14.939, -13.984, 5.477, 16.956, 3.293, -7.994, -19.372, 1.368, -1.207, -1.243, -25.071, -37.823, -1.771, 2.102, 6.931, -24.296, -0.04, -19.758, 4.504, -2.639, -0.947, 3.554, -10.426, 18.286, 11.946, 10.194, -10.78, -2.851, 14.742, 13.578, 5.715, 14.85, -2.243, 14.775, -13.341, -0.552, 3.272, 17.285, 5.205, -2.628, -12.984, 1.411, -10.444, -18.134, -6.799, -2.85, 6.421, -7.361, 0.403, -6.358, 40.758, 6.787, -5.229, 13.641, -0.205, 13.917, 1.414, -6.071, -2.626, -17.239, 3.212, -17.236, 2.634, -5.35, -10.374, 22.758, 4.719, -5.066, -6.826, -11.449, 0.343, -8.437, -3.488, -19.44, 15.674, 3.971, 8.022, 15.102, 11.207, -37.32, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.145, 1.516, 4.502, -3.704, -13.603, -2.587, -2.46, -4.563, -15.195, 1.629, 7.517, 2.397, -7.25, -9.563, -0.213, -5.572, 1.173, -3.397, 1.328, -6.5, -1.705, 0.02, -2.473, -7.878, -15.552, 0.839, 0.201, -10.081, -4.635, -24.649, -7.487, 1.995, -3.433, -3.097, -3.54, -2.668, 0.911, -7.326, -0.481, 0.332, -9.071, 2.188, -4.61, 3.084, -6.702, 2.259, -4.565, -9.117, -11.178, 15.113, 11.372, 12.01, -19.328, -5.053, -11.618, 7.171, 6.817, 4.571, 6.019, 5.329, -0.067, 19.786, -7.004, -0.016, 12.162, -0.85, -0.377, 11.844, 28.698, -1.039, -10.201, -14.782, 3.422, 22.946, 11.833, -5.781, -8.262, 9.303, -11.53, -3.749, 10.928, 14.616, 0.647, -5.757, -12.963, -13.447, -13.432, -12.441, -3.38, -1.78, -5.456, -1.048, -10.321, -2.154, 1.504, 2.396, 0.594, 0.683, 4.341, 18.877, 21.085, 20.915, -3.312, 4.845, -0.07, -6.085, -2.663, 12.745, 14.682, 11.529, 12.079, 15.882, -0.928, -14.337, 0.977, 6.429, 18.769, -4.423, 3.127, 6.118, 8.36, 3.689, -3.163, -9.604, -8.199, -10.28, -14.943, -10.258, 7.457, 2.505, 11.28, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.955, 38.331, 31.844, -4.259, -5.953, 3.235, -11.431, -12.58, -0.642, -5.022, -0.26, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 42\n",
      "training loss: 9428.192845591668\n",
      "test loss: 2758.7751852259835\n",
      "Model: [513.241, 36.022, 7.603, 12.487, -3.65, -27.318, -18.827, 25.259, -26.07, -8.432, 32.198, 50.88, -1.434, -33.538, -7.354, -2.983, -14.797, -12.167, -0.738, 14.94, -13.984, 5.477, 16.955, 3.293, -7.994, -19.372, 1.368, -1.207, -1.243, -25.07, -37.822, -1.771, 2.102, 6.932, -24.295, -0.039, -19.758, 4.504, -2.639, -0.947, 3.554, -10.426, 18.286, 11.946, 10.194, -10.78, -2.85, 14.742, 13.578, 5.715, 14.85, -2.244, 14.774, -13.342, -0.552, 3.272, 17.285, 5.205, -2.628, -12.983, 1.411, -10.444, -18.134, -6.799, -2.85, 6.421, -7.361, 0.403, -6.358, 40.758, 6.787, -5.23, 13.641, -0.205, 13.916, 1.414, -6.071, -2.626, -17.238, 3.212, -17.237, 2.634, -5.35, -10.374, 22.758, 4.719, -5.066, -6.826, -11.449, 0.343, -8.437, -3.488, -19.439, 15.674, 3.971, 8.022, 15.102, 11.207, -37.318, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.145, 1.516, 4.501, -3.704, -13.604, -2.587, -2.46, -4.563, -15.195, 1.628, 7.517, 2.396, -7.251, -9.564, -0.213, -5.573, 1.173, -3.397, 1.328, -6.5, -1.705, 0.02, -2.473, -7.878, -15.552, 0.838, 0.201, -10.081, -4.635, -24.649, -7.487, 1.995, -3.433, -3.097, -3.54, -2.667, 0.911, -7.326, -0.48, 0.332, -9.071, 2.188, -4.61, 3.084, -6.701, 2.259, -4.565, -9.117, -11.178, 15.113, 11.372, 12.01, -19.328, -5.053, -11.618, 7.171, 6.816, 4.571, 6.018, 5.329, -0.067, 19.786, -7.004, -0.016, 12.162, -0.851, -0.377, 11.844, 28.698, -1.039, -10.201, -14.782, 3.422, 22.946, 11.833, -5.781, -8.262, 9.303, -11.531, -3.749, 10.928, 14.615, 0.647, -5.757, -12.963, -13.447, -13.432, -12.441, -3.38, -1.78, -5.456, -1.048, -10.321, -2.154, 1.504, 2.395, 0.594, 0.683, 4.341, 18.877, 21.085, 20.915, -3.312, 4.845, -0.071, -6.086, -2.663, 12.745, 14.682, 11.529, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.604, -8.199, -10.28, -14.943, -10.258, 7.457, 2.505, 11.28, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.955, 38.331, 31.844, -4.259, -5.953, 3.235, -11.431, -12.58, -0.642, -5.022, -0.26, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 43\n",
      "training loss: 9428.19271049459\n",
      "test loss: 2758.7554098618607\n",
      "Model: [513.241, 36.022, 7.602, 12.486, -3.651, -27.318, -18.826, 25.259, -26.07, -8.432, 32.198, 50.88, -1.434, -33.539, -7.354, -2.983, -14.797, -12.167, -0.738, 14.94, -13.984, 5.477, 16.955, 3.293, -7.994, -19.372, 1.368, -1.207, -1.243, -25.07, -37.82, -1.77, 2.103, 6.932, -24.295, -0.038, -19.758, 4.504, -2.639, -0.947, 3.555, -10.425, 18.286, 11.946, 10.194, -10.78, -2.85, 14.742, 13.578, 5.715, 14.85, -2.245, 14.774, -13.342, -0.552, 3.272, 17.284, 5.204, -2.628, -12.983, 1.411, -10.444, -18.134, -6.8, -2.85, 6.421, -7.361, 0.403, -6.358, 40.757, 6.786, -5.23, 13.64, -0.205, 13.916, 1.414, -6.071, -2.627, -17.237, 3.212, -17.237, 2.634, -5.35, -10.374, 22.757, 4.719, -5.066, -6.826, -11.449, 0.343, -8.436, -3.488, -19.438, 15.673, 3.971, 8.022, 15.102, 11.207, -37.317, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.144, 1.516, 4.501, -3.704, -13.604, -2.587, -2.46, -4.563, -15.195, 1.628, 7.517, 2.396, -7.251, -9.564, -0.213, -5.573, 1.173, -3.397, 1.327, -6.5, -1.705, 0.02, -2.473, -7.878, -15.552, 0.838, 0.201, -10.081, -4.635, -24.649, -7.487, 1.995, -3.433, -3.097, -3.54, -2.665, 0.911, -7.326, -0.479, 0.331, -9.072, 2.188, -4.611, 3.084, -6.701, 2.259, -4.565, -9.117, -11.179, 15.113, 11.372, 12.01, -19.328, -5.052, -11.618, 7.171, 6.816, 4.571, 6.018, 5.329, -0.068, 19.786, -7.004, -0.016, 12.162, -0.851, -0.377, 11.843, 28.698, -1.039, -10.201, -14.782, 3.422, 22.946, 11.833, -5.781, -8.261, 9.302, -11.531, -3.749, 10.928, 14.615, 0.647, -5.757, -12.963, -13.447, -13.432, -12.441, -3.38, -1.78, -5.456, -1.048, -10.321, -2.154, 1.504, 2.395, 0.594, 0.683, 4.341, 18.877, 21.085, 20.915, -3.312, 4.845, -0.071, -6.086, -2.663, 12.745, 14.682, 11.528, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.605, -8.199, -10.28, -14.943, -10.258, 7.457, 2.505, 11.28, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.955, 38.331, 31.844, -4.259, -5.953, 3.235, -11.431, -12.58, -0.642, -5.023, -0.26, 4.927, 8.822, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 44\n",
      "training loss: 9428.192583308304\n",
      "test loss: 2758.736184397963\n",
      "Model: [513.242, 36.022, 7.602, 12.486, -3.651, -27.317, -18.825, 25.259, -26.071, -8.433, 32.198, 50.88, -1.434, -33.539, -7.354, -2.983, -14.797, -12.167, -0.738, 14.94, -13.984, 5.477, 16.954, 3.293, -7.994, -19.371, 1.368, -1.208, -1.243, -25.07, -37.819, -1.77, 2.103, 6.933, -24.295, -0.037, -19.758, 4.504, -2.639, -0.947, 3.555, -10.425, 18.285, 11.946, 10.194, -10.78, -2.85, 14.742, 13.578, 5.715, 14.85, -2.245, 14.774, -13.342, -0.552, 3.272, 17.284, 5.204, -2.628, -12.983, 1.41, -10.444, -18.134, -6.8, -2.85, 6.421, -7.36, 0.403, -6.358, 40.757, 6.786, -5.23, 13.64, -0.205, 13.916, 1.414, -6.072, -2.627, -17.236, 3.212, -17.237, 2.634, -5.351, -10.374, 22.757, 4.719, -5.066, -6.826, -11.449, 0.343, -8.436, -3.488, -19.437, 15.673, 3.971, 8.022, 15.101, 11.207, -37.316, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.144, 1.516, 4.501, -3.704, -13.604, -2.587, -2.46, -4.563, -15.195, 1.628, 7.517, 2.396, -7.251, -9.564, -0.213, -5.573, 1.173, -3.397, 1.327, -6.5, -1.705, 0.02, -2.473, -7.878, -15.552, 0.838, 0.201, -10.081, -4.635, -24.649, -7.487, 1.995, -3.433, -3.097, -3.54, -2.664, 0.911, -7.326, -0.478, 0.331, -9.072, 2.188, -4.611, 3.084, -6.7, 2.259, -4.565, -9.117, -11.179, 15.113, 11.372, 12.01, -19.327, -5.052, -11.617, 7.171, 6.816, 4.571, 6.018, 5.329, -0.068, 19.786, -7.004, -0.016, 12.162, -0.851, -0.377, 11.843, 28.698, -1.039, -10.201, -14.782, 3.422, 22.946, 11.833, -5.781, -8.261, 9.302, -11.531, -3.749, 10.928, 14.615, 0.647, -5.757, -12.964, -13.447, -13.432, -12.441, -3.38, -1.78, -5.456, -1.049, -10.321, -2.154, 1.504, 2.395, 0.594, 0.683, 4.341, 18.877, 21.084, 20.915, -3.312, 4.845, -0.071, -6.086, -2.663, 12.745, 14.682, 11.528, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.605, -8.199, -10.28, -14.943, -10.258, 7.457, 2.504, 11.28, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.955, 38.331, 31.844, -4.259, -5.953, 3.235, -11.431, -12.58, -0.642, -5.023, -0.26, 4.927, 8.821, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 45\n",
      "training loss: 9428.192463318705\n",
      "test loss: 2758.7174753294394\n",
      "Model: [513.242, 36.022, 7.602, 12.486, -3.651, -27.317, -18.825, 25.259, -26.071, -8.433, 32.198, 50.88, -1.434, -33.539, -7.354, -2.983, -14.797, -12.167, -0.738, 14.941, -13.984, 5.477, 16.953, 3.292, -7.994, -19.371, 1.368, -1.208, -1.243, -25.07, -37.818, -1.77, 2.103, 6.933, -24.295, -0.037, -19.758, 4.504, -2.639, -0.947, 3.555, -10.424, 18.285, 11.946, 10.193, -10.78, -2.85, 14.742, 13.578, 5.715, 14.85, -2.246, 14.773, -13.342, -0.552, 3.272, 17.284, 5.204, -2.628, -12.982, 1.41, -10.444, -18.135, -6.8, -2.85, 6.421, -7.36, 0.403, -6.358, 40.756, 6.786, -5.23, 13.64, -0.205, 13.916, 1.414, -6.072, -2.627, -17.236, 3.212, -17.237, 2.634, -5.351, -10.375, 22.757, 4.719, -5.066, -6.826, -11.45, 0.343, -8.435, -3.488, -19.436, 15.673, 3.971, 8.022, 15.101, 11.207, -37.314, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.144, 1.516, 4.501, -3.704, -13.604, -2.587, -2.46, -4.563, -15.195, 1.628, 7.517, 2.396, -7.251, -9.564, -0.214, -5.573, 1.173, -3.397, 1.327, -6.5, -1.705, 0.02, -2.473, -7.878, -15.552, 0.838, 0.201, -10.081, -4.635, -24.649, -7.487, 1.995, -3.433, -3.097, -3.54, -2.663, 0.911, -7.326, -0.478, 0.331, -9.072, 2.188, -4.611, 3.084, -6.7, 2.259, -4.565, -9.117, -11.179, 15.113, 11.372, 12.01, -19.327, -5.051, -11.617, 7.171, 6.816, 4.571, 6.018, 5.329, -0.068, 19.786, -7.004, -0.016, 12.162, -0.851, -0.377, 11.843, 28.697, -1.039, -10.201, -14.782, 3.422, 22.946, 11.833, -5.781, -8.26, 9.302, -11.531, -3.749, 10.928, 14.615, 0.647, -5.757, -12.964, -13.447, -13.432, -12.441, -3.38, -1.78, -5.456, -1.049, -10.321, -2.154, 1.503, 2.395, 0.594, 0.683, 4.341, 18.877, 21.084, 20.915, -3.312, 4.845, -0.071, -6.086, -2.663, 12.745, 14.682, 11.528, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.605, -8.199, -10.28, -14.943, -10.258, 7.457, 2.504, 11.28, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.014, 36.079, 39.382, 37.319, 38.955, 38.331, 31.844, -4.259, -5.953, 3.235, -11.431, -12.58, -0.642, -5.023, -0.26, 4.927, 8.821, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 46\n",
      "training loss: 9428.192349898296\n",
      "test loss: 2758.699252290754\n",
      "Model: [513.243, 36.021, 7.602, 12.486, -3.651, -27.317, -18.824, 25.259, -26.071, -8.433, 32.198, 50.879, -1.434, -33.539, -7.354, -2.983, -14.797, -12.166, -0.738, 14.941, -13.984, 5.476, 16.953, 3.292, -7.994, -19.371, 1.368, -1.208, -1.243, -25.07, -37.817, -1.77, 2.103, 6.933, -24.295, -0.036, -19.759, 4.505, -2.639, -0.947, 3.556, -10.424, 18.285, 11.946, 10.193, -10.78, -2.849, 14.742, 13.578, 5.715, 14.85, -2.246, 14.773, -13.342, -0.552, 3.272, 17.284, 5.203, -2.628, -12.982, 1.41, -10.444, -18.135, -6.8, -2.85, 6.421, -7.36, 0.403, -6.358, 40.756, 6.786, -5.231, 13.64, -0.205, 13.916, 1.414, -6.072, -2.627, -17.235, 3.212, -17.237, 2.633, -5.351, -10.375, 22.757, 4.719, -5.066, -6.826, -11.45, 0.343, -8.435, -3.488, -19.435, 15.673, 3.971, 8.022, 15.101, 11.207, -37.313, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.144, 1.516, 4.501, -3.704, -13.604, -2.587, -2.46, -4.563, -15.195, 1.628, 7.517, 2.396, -7.251, -9.564, -0.214, -5.573, 1.173, -3.398, 1.327, -6.5, -1.706, 0.02, -2.473, -7.878, -15.552, 0.838, 0.201, -10.081, -4.636, -24.649, -7.488, 1.995, -3.433, -3.097, -3.541, -2.662, 0.911, -7.327, -0.477, 0.331, -9.072, 2.188, -4.611, 3.084, -6.7, 2.259, -4.565, -9.117, -11.179, 15.113, 11.372, 12.01, -19.327, -5.051, -11.617, 7.171, 6.816, 4.571, 6.018, 5.329, -0.068, 19.786, -7.004, -0.016, 12.162, -0.851, -0.377, 11.843, 28.697, -1.039, -10.201, -14.782, 3.422, 22.946, 11.833, -5.781, -8.26, 9.302, -11.531, -3.749, 10.928, 14.615, 0.647, -5.758, -12.964, -13.447, -13.432, -12.441, -3.38, -1.78, -5.456, -1.049, -10.321, -2.154, 1.503, 2.395, 0.594, 0.683, 4.341, 18.877, 21.084, 20.914, -3.312, 4.845, -0.071, -6.086, -2.663, 12.745, 14.682, 11.528, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.605, -8.199, -10.28, -14.943, -10.258, 7.457, 2.504, 11.28, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.015, 36.079, 39.382, 37.319, 38.955, 38.33, 31.844, -4.259, -5.953, 3.235, -11.432, -12.58, -0.642, -5.023, -0.26, 4.927, 8.821, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 47\n",
      "training loss: 9428.192242493129\n",
      "test loss: 2758.6814876639414\n",
      "Model: [513.243, 36.021, 7.602, 12.486, -3.652, -27.317, -18.823, 25.259, -26.072, -8.433, 32.199, 50.879, -1.434, -33.539, -7.354, -2.984, -14.797, -12.166, -0.738, 14.942, -13.984, 5.476, 16.952, 3.292, -7.994, -19.371, 1.368, -1.208, -1.243, -25.069, -37.816, -1.769, 2.103, 6.934, -24.294, -0.035, -19.759, 4.505, -2.639, -0.947, 3.556, -10.424, 18.285, 11.945, 10.193, -10.78, -2.849, 14.741, 13.577, 5.715, 14.85, -2.247, 14.772, -13.342, -0.552, 3.272, 17.284, 5.203, -2.628, -12.982, 1.41, -10.444, -18.135, -6.8, -2.85, 6.42, -7.359, 0.403, -6.358, 40.756, 6.786, -5.231, 13.64, -0.205, 13.915, 1.414, -6.072, -2.627, -17.235, 3.212, -17.237, 2.633, -5.351, -10.375, 22.757, 4.718, -5.066, -6.826, -11.45, 0.343, -8.434, -3.488, -19.434, 15.673, 3.971, 8.022, 15.101, 11.207, -37.312, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.144, 1.516, 4.501, -3.704, -13.604, -2.588, -2.46, -4.563, -15.195, 1.628, 7.517, 2.396, -7.251, -9.564, -0.214, -5.573, 1.173, -3.398, 1.327, -6.5, -1.706, 0.02, -2.473, -7.878, -15.552, 0.838, 0.201, -10.081, -4.636, -24.649, -7.488, 1.995, -3.433, -3.097, -3.541, -2.661, 0.911, -7.327, -0.476, 0.331, -9.072, 2.188, -4.611, 3.084, -6.699, 2.259, -4.565, -9.117, -11.179, 15.113, 11.372, 12.01, -19.327, -5.05, -11.616, 7.171, 6.816, 4.571, 6.018, 5.329, -0.068, 19.786, -7.004, -0.016, 12.162, -0.851, -0.377, 11.843, 28.697, -1.039, -10.202, -14.782, 3.421, 22.946, 11.833, -5.781, -8.259, 9.302, -11.531, -3.749, 10.928, 14.615, 0.647, -5.758, -12.964, -13.447, -13.433, -12.441, -3.38, -1.78, -5.456, -1.049, -10.321, -2.154, 1.503, 2.395, 0.594, 0.683, 4.341, 18.876, 21.084, 20.914, -3.312, 4.845, -0.071, -6.086, -2.663, 12.745, 14.682, 11.528, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.605, -8.199, -10.28, -14.943, -10.258, 7.457, 2.504, 11.279, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.015, 36.079, 39.382, 37.319, 38.955, 38.33, 31.844, -4.259, -5.953, 3.235, -11.432, -12.58, -0.642, -5.023, -0.26, 4.927, 8.821, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 48\n",
      "training loss: 9428.19214061208\n",
      "test loss: 2758.664156247104\n",
      "Model: [513.244, 36.021, 7.602, 12.486, -3.652, -27.317, -18.823, 25.259, -26.072, -8.433, 32.199, 50.879, -1.434, -33.54, -7.354, -2.984, -14.796, -12.166, -0.738, 14.942, -13.983, 5.476, 16.952, 3.292, -7.994, -19.371, 1.368, -1.208, -1.243, -25.069, -37.815, -1.769, 2.104, 6.934, -24.294, -0.035, -19.759, 4.505, -2.639, -0.946, 3.556, -10.423, 18.285, 11.945, 10.193, -10.78, -2.849, 14.741, 13.577, 5.715, 14.849, -2.248, 14.772, -13.342, -0.552, 3.272, 17.284, 5.203, -2.628, -12.981, 1.41, -10.444, -18.135, -6.8, -2.85, 6.42, -7.359, 0.403, -6.358, 40.755, 6.786, -5.231, 13.639, -0.206, 13.915, 1.414, -6.072, -2.627, -17.234, 3.212, -17.237, 2.633, -5.351, -10.375, 22.757, 4.718, -5.066, -6.826, -11.45, 0.343, -8.434, -3.488, -19.433, 15.673, 3.971, 8.022, 15.101, 11.207, -37.31, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.144, 1.516, 4.501, -3.704, -13.604, -2.588, -2.46, -4.563, -15.195, 1.628, 7.517, 2.396, -7.251, -9.565, -0.214, -5.573, 1.173, -3.398, 1.327, -6.5, -1.706, 0.02, -2.473, -7.878, -15.552, 0.838, 0.2, -10.081, -4.636, -24.649, -7.488, 1.995, -3.433, -3.097, -3.541, -2.66, 0.911, -7.327, -0.476, 0.331, -9.072, 2.188, -4.611, 3.084, -6.699, 2.259, -4.565, -9.117, -11.179, 15.113, 11.371, 12.01, -19.327, -5.05, -11.616, 7.171, 6.816, 4.571, 6.018, 5.329, -0.068, 19.786, -7.004, -0.016, 12.161, -0.851, -0.378, 11.843, 28.697, -1.039, -10.202, -14.782, 3.421, 22.946, 11.833, -5.781, -8.259, 9.302, -11.531, -3.749, 10.928, 14.615, 0.647, -5.758, -12.964, -13.447, -13.433, -12.441, -3.381, -1.78, -5.456, -1.049, -10.321, -2.154, 1.503, 2.395, 0.594, 0.683, 4.341, 18.876, 21.084, 20.914, -3.312, 4.845, -0.071, -6.086, -2.663, 12.744, 14.682, 11.528, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.605, -8.199, -10.28, -14.944, -10.258, 7.457, 2.504, 11.279, -1.429, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.015, 36.079, 39.382, 37.319, 38.955, 38.33, 31.844, -4.259, -5.953, 3.235, -11.432, -12.58, -0.642, -5.023, -0.26, 4.927, 8.821, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 49\n",
      "training loss: 9428.19204381798\n",
      "test loss: 2758.6472349722403\n",
      "Model: [513.244, 36.021, 7.602, 12.486, -3.652, -27.317, -18.822, 25.259, -26.072, -8.433, 32.199, 50.879, -1.434, -33.54, -7.355, -2.984, -14.796, -12.166, -0.738, 14.942, -13.983, 5.476, 16.951, 3.292, -7.993, -19.37, 1.368, -1.209, -1.243, -25.069, -37.814, -1.769, 2.104, 6.934, -24.294, -0.034, -19.759, 4.505, -2.639, -0.946, 3.557, -10.423, 18.285, 11.945, 10.193, -10.78, -2.848, 14.741, 13.577, 5.715, 14.849, -2.248, 14.772, -13.343, -0.552, 3.272, 17.284, 5.202, -2.628, -12.981, 1.41, -10.445, -18.136, -6.8, -2.85, 6.42, -7.359, 0.402, -6.358, 40.755, 6.786, -5.231, 13.639, -0.206, 13.915, 1.414, -6.072, -2.627, -17.234, 3.212, -17.237, 2.633, -5.351, -10.376, 22.756, 4.718, -5.066, -6.826, -11.45, 0.343, -8.434, -3.488, -19.433, 15.673, 3.971, 8.021, 15.101, 11.207, -37.309, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.144, 1.516, 4.501, -3.704, -13.604, -2.588, -2.46, -4.563, -15.195, 1.628, 7.517, 2.396, -7.251, -9.565, -0.214, -5.573, 1.173, -3.398, 1.327, -6.5, -1.706, 0.02, -2.473, -7.879, -15.552, 0.838, 0.2, -10.081, -4.636, -24.649, -7.488, 1.995, -3.434, -3.098, -3.541, -2.66, 0.911, -7.327, -0.475, 0.331, -9.072, 2.188, -4.611, 3.084, -6.698, 2.259, -4.565, -9.117, -11.179, 15.113, 11.371, 12.01, -19.327, -5.049, -11.616, 7.171, 6.816, 4.571, 6.018, 5.329, -0.068, 19.786, -7.004, -0.016, 12.161, -0.851, -0.378, 11.843, 28.697, -1.039, -10.202, -14.782, 3.421, 22.945, 11.833, -5.781, -8.258, 9.302, -11.531, -3.749, 10.928, 14.615, 0.647, -5.758, -12.964, -13.447, -13.433, -12.441, -3.381, -1.78, -5.456, -1.049, -10.321, -2.154, 1.503, 2.395, 0.594, 0.683, 4.341, 18.876, 21.084, 20.914, -3.312, 4.845, -0.071, -6.086, -2.663, 12.744, 14.682, 11.528, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.605, -8.199, -10.28, -14.944, -10.259, 7.457, 2.504, 11.279, -1.43, -6.346, -13.972, 5.721, -15.1, -14.607, -6.918, -2.015, 36.079, 39.381, 37.319, 38.955, 38.33, 31.844, -4.259, -5.953, 3.235, -11.432, -12.58, -0.642, -5.023, -0.26, 4.927, 8.821, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "----------\n",
      "STEP: 50\n",
      "training loss: 9428.191951720224\n",
      "test loss: 2758.6307026637614\n",
      "Model: [513.244, 36.021, 7.602, 12.486, -3.652, -27.317, -18.822, 25.259, -26.073, -8.434, 32.199, 50.879, -1.435, -33.54, -7.355, -2.984, -14.796, -12.165, -0.739, 14.943, -13.983, 5.476, 16.951, 3.291, -7.993, -19.37, 1.368, -1.209, -1.243, -25.069, -37.813, -1.769, 2.104, 6.935, -24.294, -0.034, -19.759, 4.505, -2.639, -0.946, 3.557, -10.423, 18.285, 11.945, 10.193, -10.78, -2.848, 14.741, 13.577, 5.715, 14.849, -2.249, 14.771, -13.343, -0.552, 3.272, 17.284, 5.202, -2.628, -12.981, 1.41, -10.445, -18.136, -6.8, -2.85, 6.42, -7.358, 0.402, -6.358, 40.754, 6.786, -5.231, 13.639, -0.206, 13.915, 1.414, -6.072, -2.627, -17.233, 3.211, -17.237, 2.633, -5.351, -10.376, 22.756, 4.718, -5.066, -6.826, -11.45, 0.342, -8.433, -3.488, -19.432, 15.673, 3.971, 8.021, 15.101, 11.207, -37.308, 12.576, 11.216, 12.965, -2.692, 5.409, -1.133, 4.144, 39.144, 1.516, 4.5, -3.704, -13.604, -2.588, -2.46, -4.563, -15.196, 1.628, 7.517, 2.396, -7.251, -9.565, -0.214, -5.573, 1.173, -3.398, 1.327, -6.5, -1.706, 0.02, -2.473, -7.879, -15.552, 0.838, 0.2, -10.082, -4.636, -24.65, -7.488, 1.995, -3.434, -3.098, -3.541, -2.659, 0.911, -7.327, -0.474, 0.33, -9.072, 2.188, -4.611, 3.084, -6.698, 2.259, -4.565, -9.117, -11.179, 15.113, 11.371, 12.01, -19.327, -5.049, -11.616, 7.171, 6.816, 4.571, 6.018, 5.329, -0.068, 19.786, -7.004, -0.016, 12.161, -0.851, -0.378, 11.843, 28.697, -1.039, -10.202, -14.782, 3.421, 22.945, 11.832, -5.781, -8.258, 9.302, -11.531, -3.749, 10.928, 14.615, 0.647, -5.758, -12.964, -13.448, -13.433, -12.441, -3.381, -1.781, -5.456, -1.049, -10.321, -2.154, 1.503, 2.395, 0.593, 0.683, 4.341, 18.876, 21.084, 20.914, -3.312, 4.845, -0.071, -6.086, -2.663, 12.744, 14.682, 11.528, 12.079, 15.882, -0.928, -14.337, 0.976, 6.429, 18.768, -4.423, 3.127, 6.118, 8.36, 3.689, -3.164, -9.605, -8.199, -10.28, -14.944, -10.259, 7.457, 2.504, 11.279, -1.43, -6.346, -13.973, 5.721, -15.1, -14.607, -6.918, -2.015, 36.079, 39.381, 37.319, 38.955, 38.33, 31.844, -4.259, -5.953, 3.235, -11.432, -12.58, -0.642, -5.023, -0.26, 4.927, 8.821, -0.309, 3.056, 5.967, -6.143, 10.941, 3.6, 5.646, -3.571]\n",
      "\n",
      "... trained 50 iterations in 86.63243961334229 seconds\n"
     ]
    }
   ],
   "source": [
    "!python algorithmImplementation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ImportError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ImportError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/anaconda3/lib/python3.5/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "Uploading pyspark file to GCS\n",
      "featengcluster - RUNNING\n",
      "Submitted job ID 387359e2-6961-4429-b516-d455f969d980\n",
      "Waiting for job to finish...\n",
      "Job finished.\n",
      "Downloading output file\n",
      "Received job output b\"Ivy Default Cache set to: /root/.ivy2/cache\\nThe jars for the packages stored in: /root/.ivy2/jars\\n:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\\ncom.databricks#spark-xml_2.11 added as a dependency\\ngraphframes#graphframes added as a dependency\\ncom.databricks#spark-avro_2.11 added as a dependency\\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\\n\\tconfs: [default]\\n\\tfound com.databricks#spark-xml_2.11;0.4.1 in central\\n\\tfound graphframes#graphframes;0.5.0-spark2.1-s_2.11 in spark-packages\\n\\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\\n\\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\\n\\tfound org.scala-lang#scala-reflect;2.11.0 in central\\n\\tfound org.slf4j#slf4j-api;1.7.7 in central\\n\\tfound com.databricks#spark-avro_2.11;4.0.0 in central\\n\\tfound org.apache.avro#avro;1.7.6 in central\\n\\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\\n\\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\\n\\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\\n\\tfound org.xerial.snappy#snappy-java;1.0.5 in central\\n\\tfound org.apache.commons#commons-compress;1.4.1 in central\\n\\tfound org.tukaani#xz;1.0 in central\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.4.1/spark-xml_2.11-0.4.1.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-xml_2.11;0.4.1!spark-xml_2.11.jar (32ms)\\ndownloading http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar ...\\n\\t[SUCCESSFUL ] graphframes#graphframes;0.5.0-spark2.1-s_2.11!graphframes.jar (225ms)\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-avro_2.11;4.0.0!spark-avro_2.11.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2!scala-logging-api_2.11.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2!scala-logging-slf4j_2.11.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.0/scala-reflect-2.11.0.jar ...\\n\\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.0!scala-reflect.jar (54ms)\\ndownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar ...\\n\\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.7!slf4j-api.jar (13ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.6/avro-1.7.6.jar ...\\n\\t[SUCCESSFUL ] org.apache.avro#avro;1.7.6!avro.jar(bundle) (13ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (14ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (15ms)\\ndownloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\\n\\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (12ms)\\ndownloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar ...\\n\\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.0.5!snappy-java.jar(bundle) (15ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\\n\\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (14ms)\\ndownloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\\n\\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (12ms)\\n:: resolution report :: resolve 4008ms :: artifacts dl 463ms\\n\\t:: modules in use:\\n\\tcom.databricks#spark-avro_2.11;4.0.0 from central in [default]\\n\\tcom.databricks#spark-xml_2.11;0.4.1 from central in [default]\\n\\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\\n\\tgraphframes#graphframes;0.5.0-spark2.1-s_2.11 from spark-packages in [default]\\n\\torg.apache.avro#avro;1.7.6 from central in [default]\\n\\torg.apache.commons#commons-compress;1.4.1 from central in [default]\\n\\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\\n\\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\\n\\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\\n\\torg.slf4j#slf4j-api;1.7.7 from central in [default]\\n\\torg.tukaani#xz;1.0 from central in [default]\\n\\torg.xerial.snappy#snappy-java;1.0.5 from central in [default]\\n\\t:: evicted modules:\\n\\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\torg.slf4j#slf4j-api;1.6.4 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\t---------------------------------------------------------------------\\n\\t|                  |            modules            ||   artifacts   |\\n\\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\\n\\t---------------------------------------------------------------------\\n\\t|      default     |   16  |   14  |   14  |   2   ||   14  |   14  |\\n\\t---------------------------------------------------------------------\\n:: retrieving :: org.apache.spark#spark-submit-parent\\n\\tconfs: [default]\\n\\t14 artifacts copied, 0 already retrieved (7998kB/25ms)\\n18/12/08 01:49:37 INFO org.spark_project.jetty.util.log: Logging initialized @7368ms\\n18/12/08 01:49:37 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT\\n18/12/08 01:49:37 INFO org.spark_project.jetty.server.Server: Started @7471ms\\n18/12/08 01:49:37 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@4d0f9577{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n18/12/08 01:49:37 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.10-hadoop2\\n18/12/08 01:49:38 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at featengcluster-m/10.128.0.3:8032\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.databricks_spark-xml_2.11-0.4.1.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar added multiple times to distributed cache.\\n18/12/08 01:49:40 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:/root/.ivy2/jars/org.tukaani_xz-1.0.jar added multiple times to distributed cache.\\n18/12/08 01:49:41 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1544233642029_0001\\n18/12/08 01:49:58 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\\n\\r[Stage 1:>                                                          (0 + 2) / 2]\\r[Stage 1:=============================>                             (1 + 1) / 2]\\r[Stage 2:>                                                        (0 + 6) / 200]\\r[Stage 2:=======>                                                (26 + 6) / 200]\\r[Stage 2:===============>                                        (57 + 6) / 200]\\r[Stage 2:====================>                                   (74 + 7) / 200]\\r[Stage 2:===========================>                            (99 + 4) / 200]\\r[Stage 2:===============================>                       (113 + 4) / 200]\\r[Stage 2:===================================>                   (130 + 4) / 200]\\r[Stage 2:=========================================>             (152 + 4) / 200]\\r[Stage 2:==============================================>        (168 + 4) / 200]\\r[Stage 2:====================================================>  (190 + 4) / 200]\\r                                                                                \\r\\r[Stage 4:==============================>                        (111 + 4) / 200]\\r[Stage 4:========================================>              (146 + 4) / 200]\\r[Stage 4:==================================================>    (184 + 4) / 200]\\r                                                                                \\r\\r[Stage 6:=====================>                                  (76 + 4) / 200]\\r[Stage 6:==============================>                        (111 + 4) / 200]\\r[Stage 6:=========================================>             (152 + 4) / 200]\\r[Stage 6:====================================================>  (192 + 4) / 200]\\r                                                                                \\r\\r[Stage 8:============================>                          (105 + 4) / 200]\\r[Stage 8:=======================================>               (142 + 4) / 200]\\r[Stage 8:====================================================>  (191 + 4) / 200]\\r                                                                                \\r\\r[Stage 14:============================================>         (164 + 4) / 200]\\r                                                                                \\r\\r[Stage 20:===========================================>          (162 + 4) / 200]\\r                                                                                \\r\\r[Stage 26:============================================>         (164 + 4) / 200]\\r                                                                                \\r\\r[Stage 53:=============================>                            (1 + 1) / 2]\\r                                                                                \\r\\r[Stage 54:>                                                         (0 + 2) / 2]\\r[Stage 54:=============================>                            (1 + 1) / 2]\\r                                                                                \\r\\r[Stage 55:>                                                         (0 + 2) / 2]\\r[Stage 55:=============================>                            (1 + 1) / 2]\\r                                                                                \\r\\r[Stage 56:>                                                         (0 + 2) / 2]\\r[Stage 56:=============================>                            (1 + 1) / 2]\\r                                                                                \\r[(['1', '31', '17', '12', '2826', '351', '30', '13', '138', '1', '4', '0', '12', 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '751', '4', '4', '7788', '45', '3', '2', '75', '1', '3', '0', '4', 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['0', '1', '1', '22', '1910', '200', '5', '25', '149', '0', '1', '0', '29', 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '2', '29', '12', '17435', '1365', '1', '14', '191', '1', '1', '0', '14', 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0), (['1', '184', '4', '4', '2715', '31', '0', '4', '4', '1', '0', '0', '4', 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 0)]\\n18/12/08 01:51:46 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@4d0f9577{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n\"\n",
      "Tearing down cluster\n"
     ]
    }
   ],
   "source": [
    "!python submit_job_to_cluster.py --project_id=w261-222623 --zone=us-central1-b --cluster_name=featengcluster --gcs_bucket=w261_final_project --key_file=$HOME/w261.json --create_new_cluster --pyspark_file=featureEngineering.py --instance_type=n1-standard-4 --worker_nodes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
